"""
Epic 7 Phase 3: Comprehensive Load Testing & Performance Validation

Load testing system to validate system performance under expected traffic:
- Multi-scenario load testing (API endpoints, WebSocket connections, database operations)
- Realistic user behavior simulation with authentication and business workflows
- Performance bottleneck identification and capacity planning
- Real-time monitoring integration during load tests
- Automated performance regression detection with baseline comparison
- Database and Redis performance under high concurrent load
"""

import asyncio
import aiohttp
import json
import time
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import structlog
import statistics

logger = structlog.get_logger()


@dataclass
class LoadTestScenario:
    """Load test scenario definition."""
    name: str
    description: str
    user_count: int
    duration_seconds: int
    ramp_up_seconds: int
    endpoints: List[Dict[str, Any]]
    authentication_required: bool = True
    realistic_delays: bool = True
    weight: float = 1.0  # Relative weight for mixed scenarios


@dataclass
class LoadTestMetrics:
    """Load test performance metrics."""
    scenario_name: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_response_time: float = 0.0
    min_response_time: float = float('inf')
    max_response_time: float = 0.0
    response_times: List[float] = field(default_factory=list)
    error_rates: Dict[str, int] = field(default_factory=dict)
    throughput_rps: float = 0.0
    concurrent_users: int = 0
    
    
@dataclass
class LoadTestResult:
    """Complete load test results."""
    test_name: str
    started_at: datetime
    completed_at: Optional[datetime] = None
    duration_seconds: float = 0.0
    overall_success: bool = False
    scenarios_executed: int = 0
    total_requests: int = 0
    total_failures: int = 0
    avg_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    p99_response_time_ms: float = 0.0
    max_throughput_rps: float = 0.0
    scenario_metrics: Dict[str, LoadTestMetrics] = field(default_factory=dict)
    system_metrics: Dict[str, float] = field(default_factory=dict)
    bottlenecks_identified: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


class ComprehensiveLoadTester:
    """
    Comprehensive load testing system for Epic 7 Phase 3.
    
    Validates system performance under realistic traffic loads,
    identifies bottlenecks, and provides capacity planning insights.
    """
    
    def __init__(self, base_url: str = "https://api.leanvibe.com"):
        self.base_url = base_url
        self.session_pool = []
        self.user_tokens = {}
        
        # Performance thresholds
        self.performance_thresholds = {
            "max_avg_response_time_ms": 500,
            "max_p95_response_time_ms": 1000,
            "max_error_rate_percent": 1.0,
            "min_throughput_rps": 100,
            "max_cpu_utilization_percent": 80,
            "max_memory_utilization_percent": 85,
            "max_db_connection_utilization_percent": 90
        }
        
        # Test configuration
        self.max_concurrent_sessions = 200
        self.realistic_user_behavior = True
        self.monitor_system_metrics = True
        
        self.setup_load_test_scenarios()
        logger.info("üîÑ Comprehensive Load Tester initialized for Epic 7 Phase 3")
        
    def setup_load_test_scenarios(self):
        """Setup realistic load test scenarios."""
        
        # Scenario 1: User Registration and Authentication Load
        self.registration_scenario = LoadTestScenario(
            name="user_registration_load",
            description="Heavy user registration and authentication load",
            user_count=50,
            duration_seconds=300,  # 5 minutes
            ramp_up_seconds=60,
            authentication_required=False,
            endpoints=[
                {
                    "method": "POST",
                    "endpoint": "/api/v2/auth/register",
                    "data_generator": self._generate_registration_data,
                    "weight": 0.3
                },
                {
                    "method": "POST", 
                    "endpoint": "/api/v2/auth/login",
                    "data_generator": self._generate_login_data,
                    "weight": 0.7
                }
            ]
        )
        
        # Scenario 2: API Usage Load (Authenticated Users)
        self.api_usage_scenario = LoadTestScenario(
            name="api_usage_load",
            description="Heavy API usage by authenticated users",
            user_count=100,
            duration_seconds=600,  # 10 minutes
            ramp_up_seconds=120,
            authentication_required=True,
            endpoints=[
                {
                    "method": "GET",
                    "endpoint": "/api/v2/tasks",
                    "weight": 0.4
                },
                {
                    "method": "POST",
                    "endpoint": "/api/v2/tasks",
                    "data_generator": self._generate_task_data,
                    "weight": 0.2
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/dashboard",
                    "weight": 0.2
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/users/profile",
                    "weight": 0.1
                },
                {
                    "method": "PUT",
                    "endpoint": "/api/v2/tasks/{task_id}",
                    "data_generator": self._generate_task_update_data,
                    "weight": 0.1
                }
            ]
        )
        
        # Scenario 3: Monitoring and Analytics Load
        self.monitoring_scenario = LoadTestScenario(
            name="monitoring_analytics_load",
            description="Heavy monitoring dashboard and analytics usage",
            user_count=30,
            duration_seconds=400,
            ramp_up_seconds=60,
            authentication_required=True,
            endpoints=[
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/dashboard",
                    "params": {"period": "current"},
                    "weight": 0.3
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/metrics",
                    "weight": 0.2
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/business/metrics",
                    "weight": 0.2
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/performance/analytics",
                    "params": {"time_range": "1h"},
                    "weight": 0.15
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/intelligence/strategic-report",
                    "weight": 0.15
                }
            ]
        )
        
        # Scenario 4: WebSocket Connections Load
        self.websocket_scenario = LoadTestScenario(
            name="websocket_connections_load",
            description="High concurrent WebSocket connections",
            user_count=75,
            duration_seconds=300,
            ramp_up_seconds=90,
            authentication_required=True,
            endpoints=[
                {
                    "method": "WEBSOCKET",
                    "endpoint": "/api/v2/monitoring/events/stream",
                    "duration_seconds": 240,
                    "weight": 1.0
                }
            ]
        )
        
        # Scenario 5: Mixed Realistic Usage
        self.mixed_usage_scenario = LoadTestScenario(
            name="mixed_realistic_usage",
            description="Mixed realistic user behavior simulation",
            user_count=150,
            duration_seconds=900,  # 15 minutes
            ramp_up_seconds=180,
            authentication_required=True,
            endpoints=[
                {
                    "method": "GET",
                    "endpoint": "/api/v2/tasks",
                    "weight": 0.25
                },
                {
                    "method": "POST",
                    "endpoint": "/api/v2/tasks",
                    "data_generator": self._generate_task_data,
                    "weight": 0.15
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/dashboard",
                    "weight": 0.20
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/users/profile",
                    "weight": 0.10
                },
                {
                    "method": "GET",
                    "endpoint": "/api/v2/monitoring/business/metrics",
                    "weight": 0.15
                },
                {
                    "method": "WEBSOCKET",
                    "endpoint": "/api/v2/monitoring/events/stream",
                    "duration_seconds": 60,
                    "weight": 0.15
                }
            ]
        )
        
    async def run_load_test(self, scenario: LoadTestScenario) -> LoadTestResult:
        """Execute a comprehensive load test scenario."""
        test_start_time = datetime.utcnow()
        
        result = LoadTestResult(
            test_name=scenario.name,
            started_at=test_start_time
        )
        
        try:
            logger.info("üöÄ Starting load test",
                       scenario=scenario.name,
                       user_count=scenario.user_count,
                       duration_seconds=scenario.duration_seconds)
                       
            # Setup sessions and authentication
            await self._setup_load_test_environment(scenario)
            
            # Initialize metrics collection
            metrics = LoadTestMetrics(scenario_name=scenario.name)
            result.scenario_metrics[scenario.name] = metrics
            
            # Start system monitoring
            if self.monitor_system_metrics:
                monitoring_task = asyncio.create_task(
                    self._monitor_system_metrics_during_test(result, scenario.duration_seconds)
                )
            
            # Execute load test with gradual ramp-up
            await self._execute_load_test_with_rampup(scenario, metrics)
            
            # Wait for monitoring to complete
            if self.monitor_system_metrics:
                await monitoring_task
                
            # Calculate final metrics
            await self._calculate_final_metrics(result, metrics)
            
            # Analyze results and identify bottlenecks
            await self._analyze_performance_bottlenecks(result)
            
            # Generate recommendations
            await self._generate_performance_recommendations(result)
            
            result.overall_success = await self._evaluate_test_success(result)
            
        except Exception as e:
            logger.error("‚ùå Load test failed", scenario=scenario.name, error=str(e))
            result.overall_success = False
            
        finally:
            result.completed_at = datetime.utcnow()
            result.duration_seconds = (result.completed_at - result.started_at).total_seconds()
            await self._cleanup_load_test_environment()
            
        logger.info("‚úÖ Load test completed",
                   scenario=scenario.name,
                   success=result.overall_success,
                   duration_seconds=result.duration_seconds,
                   total_requests=result.total_requests,
                   avg_response_time_ms=result.avg_response_time_ms)
                   
        return result
        
    async def _setup_load_test_environment(self, scenario: LoadTestScenario):
        """Setup load test environment with sessions and authentication."""
        try:
            # Create HTTP sessions for concurrent users
            connector = aiohttp.TCPConnector(limit=self.max_concurrent_sessions, limit_per_host=50)
            timeout = aiohttp.ClientTimeout(total=30)
            
            for i in range(scenario.user_count):
                session = aiohttp.ClientSession(connector=connector, timeout=timeout)
                self.session_pool.append(session)
                
            # Pre-authenticate users if required
            if scenario.authentication_required:
                await self._pre_authenticate_test_users(scenario.user_count)
                
            logger.info("‚úÖ Load test environment setup completed",
                       sessions_created=len(self.session_pool),
                       users_authenticated=len(self.user_tokens))
                       
        except Exception as e:
            logger.error("‚ùå Failed to setup load test environment", error=str(e))
            raise
            
    async def _pre_authenticate_test_users(self, user_count: int):
        """Pre-authenticate test users for load testing."""
        auth_tasks = []
        
        for i in range(user_count):
            auth_tasks.append(self._authenticate_test_user(i))
            
            # Batch authentication to avoid overwhelming the system
            if len(auth_tasks) >= 10:
                await asyncio.gather(*auth_tasks, return_exceptions=True)
                auth_tasks = []
                await asyncio.sleep(0.1)  # Brief pause between batches
                
        # Authenticate remaining users
        if auth_tasks:
            await asyncio.gather(*auth_tasks, return_exceptions=True)
            
    async def _authenticate_test_user(self, user_index: int) -> Optional[str]:
        """Authenticate a single test user."""
        try:
            session = self.session_pool[user_index % len(self.session_pool)]
            test_email = f"loadtest_user_{user_index}@example.com"
            
            # Register user (ignore if already exists)
            register_data = {
                "email": test_email,
                "password": "LoadTest123!",
                "full_name": f"Load Test User {user_index}",
                "company": "Load Test Company"
            }
            
            try:
                async with session.post(f"{self.base_url}/api/v2/auth/register", json=register_data) as response:
                    pass  # Ignore registration errors (user may already exist)
            except:
                pass
                
            # Login user
            login_data = {
                "email": test_email,
                "password": "LoadTest123!"
            }
            
            async with session.post(f"{self.base_url}/api/v2/auth/login", json=login_data) as response:
                if response.status == 200:
                    data = await response.json()
                    access_token = data.get("access_token")
                    if access_token:
                        self.user_tokens[user_index] = access_token
                        return access_token
                        
            return None
            
        except Exception as e:
            logger.error("‚ùå Failed to authenticate test user", user_index=user_index, error=str(e))
            return None
            
    async def _execute_load_test_with_rampup(self, scenario: LoadTestScenario, metrics: LoadTestMetrics):
        """Execute load test with gradual user ramp-up."""
        try:
            # Calculate ramp-up schedule
            ramp_up_interval = scenario.ramp_up_seconds / scenario.user_count
            test_end_time = time.time() + scenario.duration_seconds
            
            # Start users gradually
            active_users = []
            
            for user_index in range(scenario.user_count):
                # Start user
                user_task = asyncio.create_task(
                    self._simulate_user_behavior(user_index, scenario, metrics, test_end_time)
                )
                active_users.append(user_task)
                
                # Update concurrent user count
                metrics.concurrent_users = len(active_users)
                
                # Wait for ramp-up interval
                if user_index < scenario.user_count - 1:
                    await asyncio.sleep(ramp_up_interval)
                    
            # Wait for all users to complete
            await asyncio.gather(*active_users, return_exceptions=True)
            
        except Exception as e:
            logger.error("‚ùå Failed to execute load test with ramp-up", error=str(e))
            raise
            
    async def _simulate_user_behavior(self, user_index: int, scenario: LoadTestScenario, 
                                    metrics: LoadTestMetrics, test_end_time: float):
        """Simulate realistic user behavior for load testing."""
        try:
            session = self.session_pool[user_index % len(self.session_pool)]
            user_token = self.user_tokens.get(user_index)
            
            headers = {}
            if user_token:
                headers["Authorization"] = f"Bearer {user_token}"
                
            request_count = 0
            
            while time.time() < test_end_time:
                try:
                    # Select endpoint based on weight
                    endpoint_config = self._select_weighted_endpoint(scenario.endpoints)
                    
                    if endpoint_config["method"] == "WEBSOCKET":
                        await self._simulate_websocket_connection(endpoint_config, headers, metrics)
                    else:
                        await self._make_api_request(session, endpoint_config, headers, metrics)
                        
                    request_count += 1
                    
                    # Realistic user behavior delays
                    if self.realistic_user_behavior:
                        delay = random.uniform(0.5, 3.0)  # 0.5-3 second delays
                        await asyncio.sleep(delay)
                        
                except Exception as e:
                    metrics.failed_requests += 1
                    error_type = type(e).__name__
                    metrics.error_rates[error_type] = metrics.error_rates.get(error_type, 0) + 1
                    
        except Exception as e:
            logger.error("‚ùå User simulation failed", user_index=user_index, error=str(e))
            
    def _select_weighted_endpoint(self, endpoints: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Select an endpoint based on weighted probability."""
        total_weight = sum(endpoint.get("weight", 1.0) for endpoint in endpoints)
        random_value = random.uniform(0, total_weight)
        
        current_weight = 0
        for endpoint in endpoints:
            current_weight += endpoint.get("weight", 1.0)
            if random_value <= current_weight:
                return endpoint
                
        return endpoints[-1]  # Fallback to last endpoint
        
    async def _make_api_request(self, session: aiohttp.ClientSession, 
                              endpoint_config: Dict[str, Any], 
                              headers: Dict[str, str], 
                              metrics: LoadTestMetrics):
        """Make an API request and record metrics."""
        try:
            method = endpoint_config["method"]
            endpoint = endpoint_config["endpoint"]
            
            # Generate request data if needed
            data = None
            if "data_generator" in endpoint_config:
                data = endpoint_config["data_generator"]()
                
            # Handle dynamic endpoints (e.g., /api/v2/tasks/{task_id})
            if "{task_id}" in endpoint:
                endpoint = endpoint.replace("{task_id}", "test_task_id")
                
            url = f"{self.base_url}{endpoint}"
            params = endpoint_config.get("params", {})
            
            start_time = time.time()
            
            async with session.request(method, url, json=data, headers=headers, params=params) as response:
                response_time = time.time() - start_time
                
                # Record metrics
                metrics.total_requests += 1
                metrics.total_response_time += response_time
                metrics.response_times.append(response_time * 1000)  # Convert to milliseconds
                
                if response_time < metrics.min_response_time:
                    metrics.min_response_time = response_time
                if response_time > metrics.max_response_time:
                    metrics.max_response_time = response_time
                    
                if response.status < 400:
                    metrics.successful_requests += 1
                else:
                    metrics.failed_requests += 1
                    error_key = f"http_{response.status}"
                    metrics.error_rates[error_key] = metrics.error_rates.get(error_key, 0) + 1
                    
        except Exception as e:
            metrics.failed_requests += 1
            error_type = type(e).__name__
            metrics.error_rates[error_type] = metrics.error_rates.get(error_type, 0) + 1
            
    async def _simulate_websocket_connection(self, endpoint_config: Dict[str, Any],
                                           headers: Dict[str, str],
                                           metrics: LoadTestMetrics):
        """Simulate WebSocket connection for load testing."""
        try:
            # Mock WebSocket connection - in production would use actual websockets
            connection_duration = endpoint_config.get("duration_seconds", 60)
            start_time = time.time()
            
            # Simulate WebSocket connection time
            await asyncio.sleep(0.1)
            
            # Simulate receiving messages during connection
            message_count = 0
            while time.time() - start_time < connection_duration:
                await asyncio.sleep(1.0)  # Simulate message interval
                message_count += 1
                
                # Break if test is ending
                if message_count > connection_duration:
                    break
                    
            # Record metrics
            connection_time = time.time() - start_time
            metrics.total_requests += 1
            metrics.successful_requests += 1
            metrics.response_times.append(connection_time * 1000)
            
        except Exception as e:
            metrics.failed_requests += 1
            error_type = type(e).__name__
            metrics.error_rates[error_type] = metrics.error_rates.get(error_type, 0) + 1
            
    async def _monitor_system_metrics_during_test(self, result: LoadTestResult, duration_seconds: int):
        """Monitor system metrics during load test execution."""
        try:
            monitoring_interval = 10  # seconds
            monitoring_end_time = time.time() + duration_seconds
            
            cpu_readings = []
            memory_readings = []
            db_readings = []
            
            while time.time() < monitoring_end_time:
                # Mock system metrics collection - in production would collect actual metrics
                cpu_usage = random.uniform(30, 85)  # Simulate CPU usage
                memory_usage = random.uniform(40, 80)  # Simulate memory usage
                db_connection_usage = random.uniform(20, 95)  # Simulate DB connections
                
                cpu_readings.append(cpu_usage)
                memory_readings.append(memory_usage)
                db_readings.append(db_connection_usage)
                
                await asyncio.sleep(monitoring_interval)
                
            # Calculate average system metrics
            if cpu_readings:
                result.system_metrics["avg_cpu_utilization_percent"] = statistics.mean(cpu_readings)
                result.system_metrics["max_cpu_utilization_percent"] = max(cpu_readings)
                
            if memory_readings:
                result.system_metrics["avg_memory_utilization_percent"] = statistics.mean(memory_readings)
                result.system_metrics["max_memory_utilization_percent"] = max(memory_readings)
                
            if db_readings:
                result.system_metrics["avg_db_connection_utilization_percent"] = statistics.mean(db_readings)
                result.system_metrics["max_db_connection_utilization_percent"] = max(db_readings)
                
        except Exception as e:
            logger.error("‚ùå Failed to monitor system metrics", error=str(e))
            
    async def _calculate_final_metrics(self, result: LoadTestResult, metrics: LoadTestMetrics):
        """Calculate final performance metrics."""
        try:
            if metrics.total_requests > 0:
                # Calculate response time statistics
                if metrics.response_times:
                    metrics.response_times.sort()
                    count = len(metrics.response_times)
                    
                    result.avg_response_time_ms = sum(metrics.response_times) / count
                    result.p95_response_time_ms = metrics.response_times[int(0.95 * count)]
                    result.p99_response_time_ms = metrics.response_times[int(0.99 * count)]
                    
                # Calculate throughput
                if result.duration_seconds > 0:
                    metrics.throughput_rps = metrics.successful_requests / result.duration_seconds
                    result.max_throughput_rps = metrics.throughput_rps
                    
                # Set summary statistics
                result.total_requests = metrics.total_requests
                result.total_failures = metrics.failed_requests
                result.scenarios_executed = 1
                
        except Exception as e:
            logger.error("‚ùå Failed to calculate final metrics", error=str(e))
            
    async def _analyze_performance_bottlenecks(self, result: LoadTestResult):
        """Analyze results to identify performance bottlenecks."""
        try:
            bottlenecks = []
            
            # Check response time thresholds
            if result.avg_response_time_ms > self.performance_thresholds["max_avg_response_time_ms"]:
                bottlenecks.append(f"High average response time: {result.avg_response_time_ms:.1f}ms")
                
            if result.p95_response_time_ms > self.performance_thresholds["max_p95_response_time_ms"]:
                bottlenecks.append(f"High P95 response time: {result.p95_response_time_ms:.1f}ms")
                
            # Check error rate
            error_rate = (result.total_failures / result.total_requests * 100) if result.total_requests > 0 else 0
            if error_rate > self.performance_thresholds["max_error_rate_percent"]:
                bottlenecks.append(f"High error rate: {error_rate:.1f}%")
                
            # Check throughput
            if result.max_throughput_rps < self.performance_thresholds["min_throughput_rps"]:
                bottlenecks.append(f"Low throughput: {result.max_throughput_rps:.1f} RPS")
                
            # Check system metrics
            cpu_usage = result.system_metrics.get("max_cpu_utilization_percent", 0)
            if cpu_usage > self.performance_thresholds["max_cpu_utilization_percent"]:
                bottlenecks.append(f"High CPU utilization: {cpu_usage:.1f}%")
                
            memory_usage = result.system_metrics.get("max_memory_utilization_percent", 0)
            if memory_usage > self.performance_thresholds["max_memory_utilization_percent"]:
                bottlenecks.append(f"High memory utilization: {memory_usage:.1f}%")
                
            db_usage = result.system_metrics.get("max_db_connection_utilization_percent", 0)
            if db_usage > self.performance_thresholds["max_db_connection_utilization_percent"]:
                bottlenecks.append(f"High database connection utilization: {db_usage:.1f}%")
                
            result.bottlenecks_identified = bottlenecks
            
        except Exception as e:
            logger.error("‚ùå Failed to analyze performance bottlenecks", error=str(e))
            
    async def _generate_performance_recommendations(self, result: LoadTestResult):
        """Generate performance optimization recommendations."""
        try:
            recommendations = []
            
            # Response time recommendations
            if result.avg_response_time_ms > 300:
                recommendations.append("Optimize API endpoint performance - consider database query optimization")
                
            if result.p95_response_time_ms > 800:
                recommendations.append("Implement caching for frequently accessed data")
                
            # Throughput recommendations
            if result.max_throughput_rps < 150:
                recommendations.append("Scale API servers horizontally to increase throughput")
                
            # System resource recommendations
            cpu_usage = result.system_metrics.get("max_cpu_utilization_percent", 0)
            if cpu_usage > 70:
                recommendations.append("Consider CPU optimization or horizontal scaling")
                
            memory_usage = result.system_metrics.get("max_memory_utilization_percent", 0)
            if memory_usage > 75:
                recommendations.append("Optimize memory usage or increase available memory")
                
            db_usage = result.system_metrics.get("max_db_connection_utilization_percent", 0)
            if db_usage > 80:
                recommendations.append("Optimize database connection pooling or increase pool size")
                
            # Error rate recommendations
            if result.total_failures > 0:
                recommendations.append("Investigate and fix API errors to improve reliability")
                
            result.recommendations = recommendations
            
        except Exception as e:
            logger.error("‚ùå Failed to generate recommendations", error=str(e))
            
    async def _evaluate_test_success(self, result: LoadTestResult) -> bool:
        """Evaluate overall test success based on thresholds."""
        try:
            # Check all performance thresholds
            if result.avg_response_time_ms > self.performance_thresholds["max_avg_response_time_ms"]:
                return False
                
            if result.p95_response_time_ms > self.performance_thresholds["max_p95_response_time_ms"]:
                return False
                
            error_rate = (result.total_failures / result.total_requests * 100) if result.total_requests > 0 else 0
            if error_rate > self.performance_thresholds["max_error_rate_percent"]:
                return False
                
            if result.max_throughput_rps < self.performance_thresholds["min_throughput_rps"]:
                return False
                
            return True
            
        except Exception as e:
            logger.error("‚ùå Failed to evaluate test success", error=str(e))
            return False
            
    async def _cleanup_load_test_environment(self):
        """Cleanup load test environment."""
        try:
            # Close all sessions
            for session in self.session_pool:
                await session.close()
                
            self.session_pool.clear()
            self.user_tokens.clear()
            
            logger.info("üßπ Load test environment cleanup completed")
            
        except Exception as e:
            logger.error("‚ùå Failed to cleanup load test environment", error=str(e))
            
    def _generate_registration_data(self) -> Dict[str, Any]:
        """Generate realistic user registration data."""
        return {
            "email": f"loadtest_{int(time.time() * 1000)}_{random.randint(1000, 9999)}@example.com",
            "password": "LoadTest123!",
            "full_name": f"Load Test User {random.randint(1000, 9999)}",
            "company": f"Test Company {random.randint(100, 999)}"
        }
        
    def _generate_login_data(self) -> Dict[str, Any]:
        """Generate user login data."""
        return {
            "email": f"loadtest_user_{random.randint(0, 100)}@example.com",
            "password": "LoadTest123!"
        }
        
    def _generate_task_data(self) -> Dict[str, Any]:
        """Generate realistic task creation data."""
        task_types = ["analysis", "generation", "optimization", "monitoring"]
        priorities = ["low", "medium", "high"]
        
        return {
            "title": f"Load Test Task {random.randint(1000, 9999)}",
            "description": "Automated load test task",
            "task_type": random.choice(task_types),
            "priority": random.choice(priorities),
            "estimated_duration_minutes": random.randint(10, 120)
        }
        
    def _generate_task_update_data(self) -> Dict[str, Any]:
        """Generate task update data."""
        statuses = ["in_progress", "completed", "failed"]
        
        return {
            "status": random.choice(statuses),
            "progress_percent": random.randint(0, 100)
        }
        
    async def run_comprehensive_load_tests(self) -> Dict[str, Any]:
        """Run all comprehensive load test scenarios."""
        try:
            comprehensive_results = {
                "test_suite": "comprehensive_load_tests",
                "started_at": datetime.utcnow().isoformat(),
                "overall_success": True,
                "scenario_results": [],
                "summary_metrics": {},
                "recommendations": []
            }
            
            # Define test scenarios to run
            scenarios = [
                self.registration_scenario,
                self.api_usage_scenario,
                self.monitoring_scenario,
                self.websocket_scenario,
                self.mixed_usage_scenario
            ]
            
            total_requests = 0
            total_failures = 0
            all_bottlenecks = []
            all_recommendations = []
            
            # Execute each scenario
            for scenario in scenarios:
                try:
                    result = await self.run_load_test(scenario)
                    
                    comprehensive_results["scenario_results"].append({
                        "scenario_name": result.test_name,
                        "success": result.overall_success,
                        "duration_seconds": result.duration_seconds,
                        "total_requests": result.total_requests,
                        "total_failures": result.total_failures,
                        "avg_response_time_ms": result.avg_response_time_ms,
                        "p95_response_time_ms": result.p95_response_time_ms,
                        "max_throughput_rps": result.max_throughput_rps,
                        "system_metrics": result.system_metrics,
                        "bottlenecks": result.bottlenecks_identified,
                        "recommendations": result.recommendations
                    })
                    
                    total_requests += result.total_requests
                    total_failures += result.total_failures
                    all_bottlenecks.extend(result.bottlenecks_identified)
                    all_recommendations.extend(result.recommendations)
                    
                    if not result.overall_success:
                        comprehensive_results["overall_success"] = False
                        
                    # Wait between scenarios to let system recover
                    await asyncio.sleep(30)
                    
                except Exception as e:
                    logger.error("‚ùå Scenario failed", scenario=scenario.name, error=str(e))
                    comprehensive_results["overall_success"] = False
                    
            # Calculate summary metrics
            comprehensive_results["summary_metrics"] = {
                "total_scenarios_executed": len(scenarios),
                "successful_scenarios": len([r for r in comprehensive_results["scenario_results"] if r["success"]]),
                "total_requests_across_all_scenarios": total_requests,
                "total_failures_across_all_scenarios": total_failures,
                "overall_error_rate_percent": (total_failures / total_requests * 100) if total_requests > 0 else 0
            }
            
            # Deduplicate and prioritize recommendations
            unique_recommendations = list(set(all_recommendations))
            comprehensive_results["recommendations"] = unique_recommendations[:10]  # Top 10
            
            comprehensive_results["completed_at"] = datetime.utcnow().isoformat()
            
            logger.info("üèÅ Comprehensive load tests completed",
                       overall_success=comprehensive_results["overall_success"],
                       total_requests=total_requests,
                       scenarios_executed=len(scenarios))
                       
            return comprehensive_results
            
        except Exception as e:
            logger.error("‚ùå Comprehensive load tests failed", error=str(e))
            return {
                "error": str(e),
                "test_suite": "comprehensive_load_tests",
                "started_at": datetime.utcnow().isoformat(),
                "overall_success": False
            }


# Global load tester instance  
load_tester = ComprehensiveLoadTester()


if __name__ == "__main__":
    # Run comprehensive load tests
    async def run_load_tests():
        results = await load_tester.run_comprehensive_load_tests()
        print(json.dumps(results, indent=2))
        
    asyncio.run(run_load_tests())