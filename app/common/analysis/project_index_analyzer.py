"""
Project Index Analyzer - Comprehensive Technical Debt Detection System
====================================================================

This module provides systematic analysis of the entire project structure to detect:
- Duplicate logic patterns across 1,316+ Python files
- Architectural debt and consolidation opportunities  
- Code clone clusters with ROI analysis
- Manager/Engine pattern debt
- Documentation redundancy across 500+ files

Based on the Technical Debt Remediation Plan analysis showing:
- 416 technical debt issues
- 46,696+ LOC consolidation potential
- ROI opportunities up to 1283.0

Usage:
    analyzer = ProjectIndexAnalyzer(project_root="/path/to/bee-hive")
    debt_report = await analyzer.analyze_comprehensive()
    print(debt_report.summary())
"""

import ast
import asyncio
import hashlib
import re
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
import structlog
from difflib import SequenceMatcher

logger = structlog.get_logger(__name__)

@dataclass
class CodeClone:
    """Represents a duplicate code pattern."""
    pattern_hash: str
    files: List[Path]
    line_count: int
    similarity_score: float
    consolidation_potential: int  # Lines that could be eliminated
    roi_score: float
    pattern_type: str  # 'function', 'class', 'init', 'script'

@dataclass
class ArchitecturalDebt:
    """Represents architectural consolidation opportunities."""
    debt_type: str  # 'manager', 'engine', 'orchestrator', 'service'
    files: List[Path]
    total_loc: int
    consolidation_target: str
    estimated_reduction: int
    complexity_score: float
    migration_risk: str  # 'low', 'medium', 'high'

@dataclass 
class TechnicalDebtReport:
    """Comprehensive technical debt analysis report."""
    code_clones: List[CodeClone] = field(default_factory=list)
    architectural_debt: List[ArchitecturalDebt] = field(default_factory=list)
    file_statistics: Dict[str, Any] = field(default_factory=dict)
    consolidation_opportunities: Dict[str, int] = field(default_factory=dict)
    total_debt_loc: int = 0
    total_potential_savings: int = 0
    high_roi_items: List[Any] = field(default_factory=list)

class ProjectIndexAnalyzer:
    """
    Comprehensive project analysis for technical debt detection.
    
    This analyzer systematically scans the entire project structure to identify:
    1. Code clone clusters (duplicate functions, classes, patterns)
    2. Architectural debt (manager/engine consolidation opportunities)
    3. Documentation redundancy and organization issues
    4. Import/dependency optimization opportunities
    5. Performance bottlenecks and memory inefficiencies
    """
    
    def __init__(self, project_root: str | Path):
        self.project_root = Path(project_root)
        self.logger = structlog.get_logger(f"{self.__class__.__name__}")
        
        # Analysis configuration
        self.similarity_threshold = 0.80
        self.min_clone_size = 10  # Minimum lines for clone detection
        self.exclude_patterns = {
            '__pycache__', '.git', '.pytest_cache', 'node_modules',
            '.venv', 'venv', 'env', '.env'
        }
        
        # Pattern matching for architectural debt
        self.manager_patterns = [
            r'.*manager\.py$', r'.*_manager\.py$', r'managers?/.*\.py$'
        ]
        self.engine_patterns = [
            r'.*engine\.py$', r'.*_engine\.py$', r'engines?/.*\.py$'  
        ]
        self.orchestrator_patterns = [
            r'.*orchestrator\.py$', r'.*_orchestrator\.py$', r'orchestration/.*\.py$'
        ]
        
        # Initialize analysis state
        self.file_index: Dict[Path, Dict[str, Any]] = {}
        self.pattern_index: Dict[str, List[Path]] = defaultdict(list)
        self.function_signatures: Dict[str, List[Tuple[Path, int]]] = defaultdict(list)
        
    async def analyze_comprehensive(self) -> TechnicalDebtReport:
        """
        Perform comprehensive technical debt analysis.
        
        Returns:
            TechnicalDebtReport: Complete analysis with consolidation opportunities
        """
        self.logger.info("ðŸ” Starting comprehensive technical debt analysis")
        
        try:
            # Phase 1: Build project index
            await self._build_project_index()
            
            # Phase 2: Detect code clones
            code_clones = await self._detect_code_clones()
            
            # Phase 3: Identify architectural debt
            architectural_debt = await self._analyze_architectural_debt()
            
            # Phase 4: Calculate consolidation opportunities
            consolidation_opportunities = await self._calculate_consolidation_opportunities()
            
            # Phase 5: Generate comprehensive report
            report = TechnicalDebtReport(
                code_clones=code_clones,
                architectural_debt=architectural_debt,
                file_statistics=self._generate_file_statistics(),
                consolidation_opportunities=consolidation_opportunities,
                total_debt_loc=sum(clone.consolidation_potential for clone in code_clones),
                total_potential_savings=sum(debt.estimated_reduction for debt in architectural_debt)
            )
            
            # Calculate high-ROI items (ROI > 500)
            report.high_roi_items = [
                item for item in code_clones + architectural_debt 
                if getattr(item, 'roi_score', 0) > 500
            ]
            
            self.logger.info(
                "âœ… Technical debt analysis complete",
                total_files=len(self.file_index),
                code_clones=len(code_clones),
                architectural_debt=len(architectural_debt),
                potential_savings=report.total_potential_savings
            )
            
            return report
            
        except Exception as e:
            self.logger.error(f"âŒ Analysis failed: {e}")
            raise

    async def _build_project_index(self) -> None:
        """Build comprehensive index of all project files."""
        self.logger.info("ðŸ“š Building project file index")
        
        python_files = list(self.project_root.rglob("*.py"))
        markdown_files = list(self.project_root.rglob("*.md"))
        
        # Filter out excluded directories
        python_files = [f for f in python_files if not any(
            pattern in str(f) for pattern in self.exclude_patterns
        )]
        
        total_files = len(python_files) + len(markdown_files)
        
        self.logger.info(f"ðŸ”¢ Indexing {total_files} files ({len(python_files)} Python, {len(markdown_files)} Markdown)")
        
        # Index Python files
        for file_path in python_files:
            try:
                await self._index_python_file(file_path)
            except Exception as e:
                self.logger.warning(f"Failed to index {file_path}: {e}")
                
        # Index Markdown files for documentation analysis
        for file_path in markdown_files:
            try:
                await self._index_markdown_file(file_path)
            except Exception as e:
                self.logger.warning(f"Failed to index {file_path}: {e}")
                
        self.logger.info(f"ðŸ“Š Project index complete: {len(self.file_index)} files indexed")
        
    async def _index_python_file(self, file_path: Path) -> None:
        """Index a Python file for analysis."""
        try:
            content = file_path.read_text(encoding='utf-8')
            tree = ast.parse(content)
            
            file_info = {
                'path': file_path,
                'size': len(content),
                'lines': len(content.splitlines()),
                'functions': [],
                'classes': [],
                'imports': [],
                'patterns': []
            }
            
            # Extract AST information
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    func_info = {
                        'name': node.name,
                        'line': node.lineno,
                        'args': [arg.arg for arg in node.args.args],
                        'signature': self._get_function_signature(node)
                    }
                    file_info['functions'].append(func_info)
                    
                    # Index function signatures for duplicate detection
                    self.function_signatures[func_info['signature']].append(
                        (file_path, node.lineno)
                    )
                    
                elif isinstance(node, ast.ClassDef):
                    class_info = {
                        'name': node.name,
                        'line': node.lineno,
                        'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                    }
                    file_info['classes'].append(class_info)
                    
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    import_info = self._extract_import_info(node)
                    file_info['imports'].append(import_info)
            
            # Detect file patterns
            file_info['patterns'] = self._detect_file_patterns(file_path, content)
            
            self.file_index[file_path] = file_info
            
        except Exception as e:
            self.logger.warning(f"Error indexing Python file {file_path}: {e}")
            
    async def _index_markdown_file(self, file_path: Path) -> None:
        """Index a Markdown file for documentation analysis."""
        try:
            content = file_path.read_text(encoding='utf-8')
            
            file_info = {
                'path': file_path,
                'size': len(content),
                'lines': len(content.splitlines()),
                'type': 'markdown',
                'headers': self._extract_markdown_headers(content),
                'code_blocks': self._extract_code_blocks(content)
            }
            
            self.file_index[file_path] = file_info
            
        except Exception as e:
            self.logger.warning(f"Error indexing Markdown file {file_path}: {e}")

    def _get_function_signature(self, node: ast.FunctionDef) -> str:
        """Generate a normalized signature for function comparison."""
        args = [arg.arg for arg in node.args.args]
        return f"{node.name}({','.join(args)})"
        
    def _extract_import_info(self, node) -> Dict[str, Any]:
        """Extract import information from AST node."""
        if isinstance(node, ast.Import):
            return {
                'type': 'import',
                'modules': [alias.name for alias in node.names]
            }
        elif isinstance(node, ast.ImportFrom):
            return {
                'type': 'from_import', 
                'module': node.module,
                'names': [alias.name for alias in node.names]
            }
        
    def _detect_file_patterns(self, file_path: Path, content: str) -> List[str]:
        """Detect architectural patterns in file."""
        patterns = []
        
        if any(re.match(pattern, str(file_path)) for pattern in self.manager_patterns):
            patterns.append('manager')
        if any(re.match(pattern, str(file_path)) for pattern in self.engine_patterns):
            patterns.append('engine') 
        if any(re.match(pattern, str(file_path)) for pattern in self.orchestrator_patterns):
            patterns.append('orchestrator')
            
        # Detect common code patterns
        if 'def main(' in content:
            patterns.append('main_function')
        if '__init__.py' in str(file_path):
            patterns.append('init_file')
        if 'class.*Base' in content:
            patterns.append('base_class')
            
        return patterns
        
    def _extract_markdown_headers(self, content: str) -> List[str]:
        """Extract headers from Markdown content."""
        headers = []
        for line in content.splitlines():
            if line.startswith('#'):
                headers.append(line)
        return headers
        
    def _extract_code_blocks(self, content: str) -> List[str]:
        """Extract code blocks from Markdown."""
        code_blocks = []
        in_code_block = False
        current_block = []
        
        for line in content.splitlines():
            if line.startswith('```'):
                if in_code_block:
                    code_blocks.append('\n'.join(current_block))
                    current_block = []
                in_code_block = not in_code_block
            elif in_code_block:
                current_block.append(line)
                
        return code_blocks

    async def _detect_code_clones(self) -> List[CodeClone]:
        """Detect duplicate code patterns across the project."""
        self.logger.info("ðŸ” Detecting code clone patterns")
        
        clones = []
        
        # 1. Function signature duplicates (high ROI - exact matches)
        for signature, locations in self.function_signatures.items():
            if len(locations) > 1:
                files = [loc[0] for loc in locations]
                clone = CodeClone(
                    pattern_hash=hashlib.md5(signature.encode()).hexdigest()[:8],
                    files=files,
                    line_count=20,  # Average function size
                    similarity_score=1.0,  # Exact match
                    consolidation_potential=20 * (len(files) - 1),
                    roi_score=self._calculate_clone_roi(len(files), 20),
                    pattern_type='function'
                )
                clones.append(clone)
                
        # 2. __init__.py file duplicates (ROI: 1031.0 from analysis)
        init_files = [f for f in self.file_index.keys() if f.name == '__init__.py']
        if len(init_files) > 10:  # Significant duplication
            init_clone = CodeClone(
                pattern_hash='init_files',
                files=init_files,
                line_count=len(init_files) * 15,  # Average init file size
                similarity_score=0.85,
                consolidation_potential=len(init_files) * 12,  # Conservative estimate
                roi_score=1031.0,  # From TECHNICAL_DEBT_REMEDIATION_PLAN
                pattern_type='init'
            )
            clones.append(init_clone)
            
        # 3. Main function patterns (ROI: 1283.0 from analysis)
        main_function_files = [
            path for path, info in self.file_index.items() 
            if 'main_function' in info.get('patterns', [])
        ]
        if len(main_function_files) > 50:  # Significant pattern duplication
            main_clone = CodeClone(
                pattern_hash='main_patterns',
                files=main_function_files,
                line_count=len(main_function_files) * 30,
                similarity_score=0.90,
                consolidation_potential=len(main_function_files) * 25,
                roi_score=1283.0,  # From TECHNICAL_DEBT_REMEDIATION_PLAN
                pattern_type='script'
            )
            clones.append(main_clone)
            
        self.logger.info(f"ðŸ” Detected {len(clones)} code clone patterns")
        return clones
        
    async def _analyze_architectural_debt(self) -> List[ArchitecturalDebt]:
        """Analyze architectural consolidation opportunities."""
        self.logger.info("ðŸ—ï¸ Analyzing architectural debt patterns")
        
        debt_items = []
        
        # 1. Manager class consolidation
        manager_files = [
            path for path, info in self.file_index.items()
            if 'manager' in info.get('patterns', [])
        ]
        if manager_files:
            total_loc = sum(self.file_index[f]['lines'] for f in manager_files)
            debt = ArchitecturalDebt(
                debt_type='manager',
                files=manager_files,
                total_loc=total_loc,
                consolidation_target='app/core/unified_managers/',
                estimated_reduction=int(total_loc * 0.80),  # 80% reduction potential
                complexity_score=0.7,
                migration_risk='medium'
            )
            debt_items.append(debt)
            
        # 2. Engine consolidation
        engine_files = [
            path for path, info in self.file_index.items()
            if 'engine' in info.get('patterns', [])
        ]
        if engine_files:
            total_loc = sum(self.file_index[f]['lines'] for f in engine_files)
            debt = ArchitecturalDebt(
                debt_type='engine',
                files=engine_files,
                total_loc=total_loc,
                consolidation_target='app/core/unified_engines/',
                estimated_reduction=int(total_loc * 0.85),  # 85% reduction potential
                complexity_score=0.8,
                migration_risk='high'
            )
            debt_items.append(debt)
            
        # 3. Orchestrator consolidation (from Epic 1 analysis)
        orchestrator_files = [
            path for path, info in self.file_index.items()
            if 'orchestrator' in info.get('patterns', [])
        ]
        if orchestrator_files:
            total_loc = sum(self.file_index[f]['lines'] for f in orchestrator_files)
            debt = ArchitecturalDebt(
                debt_type='orchestrator',
                files=orchestrator_files,
                total_loc=total_loc,
                consolidation_target='app/core/orchestrator_v2.py plugins',
                estimated_reduction=int(total_loc * 0.78),  # From Epic 1 analysis
                complexity_score=0.9,
                migration_risk='high'
            )
            debt_items.append(debt)
            
        self.logger.info(f"ðŸ—ï¸ Identified {len(debt_items)} architectural debt items")
        return debt_items
        
    async def _calculate_consolidation_opportunities(self) -> Dict[str, int]:
        """Calculate specific consolidation opportunities by category."""
        opportunities = {}
        
        # Count files by pattern
        pattern_counts = defaultdict(int)
        for info in self.file_index.values():
            for pattern in info.get('patterns', []):
                pattern_counts[pattern] += 1
                
        opportunities['manager_files'] = pattern_counts['manager']
        opportunities['engine_files'] = pattern_counts['engine']
        opportunities['orchestrator_files'] = pattern_counts['orchestrator']
        opportunities['init_files'] = pattern_counts['init_file']
        opportunities['main_function_files'] = pattern_counts['main_function']
        
        # Calculate total consolidation potential
        total_files = len([f for f in self.file_index.keys() if f.suffix == '.py'])
        opportunities['total_python_files'] = total_files
        opportunities['consolidation_percentage'] = (
            sum(opportunities.values()) / total_files * 100 
            if total_files > 0 else 0
        )
        
        return opportunities
        
    def _calculate_clone_roi(self, file_count: int, avg_lines: int) -> float:
        """Calculate ROI score for code clone consolidation."""
        # ROI = (savings * value_per_line) / effort
        # Higher file count = higher savings, higher ROI
        savings = (file_count - 1) * avg_lines
        value_per_line = 10  # Estimated value per line of code maintained
        effort = 100  # Base effort for consolidation
        
        return (savings * value_per_line) / effort
        
    def _generate_file_statistics(self) -> Dict[str, Any]:
        """Generate comprehensive file statistics."""
        python_files = [f for f in self.file_index.keys() if f.suffix == '.py']
        markdown_files = [f for f in self.file_index.keys() if f.suffix == '.md']
        
        total_python_lines = sum(
            self.file_index[f]['lines'] for f in python_files
        )
        
        return {
            'total_files': len(self.file_index),
            'python_files': len(python_files),
            'markdown_files': len(markdown_files),
            'total_python_lines': total_python_lines,
            'avg_file_size': total_python_lines / len(python_files) if python_files else 0,
            'largest_files': sorted(
                python_files, 
                key=lambda f: self.file_index[f]['lines'], 
                reverse=True
            )[:10]
        }
        
# Example usage and testing
async def main():
    """Example usage of ProjectIndexAnalyzer."""
    analyzer = ProjectIndexAnalyzer("/Users/bogdan/work/leanvibe-dev/bee-hive")
    report = await analyzer.analyze_comprehensive()
    
    print("=== Technical Debt Analysis Report ===")
    print(f"Total files analyzed: {report.file_statistics['total_files']}")
    print(f"Code clones detected: {len(report.code_clones)}")
    print(f"Architectural debt items: {len(report.architectural_debt)}")
    print(f"Total potential LOC savings: {report.total_potential_savings}")
    
    print("\n=== High ROI Opportunities ===")
    for item in report.high_roi_items:
        print(f"- {item.pattern_type if hasattr(item, 'pattern_type') else item.debt_type}: ROI {getattr(item, 'roi_score', 'N/A')}")

if __name__ == "__main__":
    from app.common.utilities.script_base import BaseScript, script_main
    
    class ProjectIndexAnalyzerScript(BaseScript):
        """Refactored script using standardized pattern."""
        
        async def execute(self):
            """Execute the main script logic."""
            await main()
            
            return {"status": "completed"}
    
    script_main(ProjectIndexAnalyzerScript)