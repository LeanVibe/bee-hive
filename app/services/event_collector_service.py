"""
Standalone Event Collector Service for LeanVibe Agent Hive 2.0 - VS 6.1

High-performance event collector service consuming Redis event streams:
- Dedicated FastAPI service for event collection and processing
- Redis Streams consumer groups for scalability and reliability
- Event enrichment with semantic context and metadata
- Batch processing for efficient database operations
- <5ms processing overhead with >1000 events/second throughput
- Integration with monitoring infrastructure
"""

import asyncio
import time
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Callable
from contextlib import asynccontextmanager
import msgpack
import structlog

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import redis.asyncio as redis

from ..core.database import get_session
from ..core.redis import get_redis
from ..core.embedding_service import get_embedding_service
from ..models.observability import AgentEvent, EventType
from ..schemas.observability import (
    BaseObservabilityEvent, 
    HookEventResponse,
    EventAnalyticsRequest,
    EventAnalyticsResponse
)
from sqlalchemy import select, func, and_
from sqlalchemy.ext.asyncio import AsyncSession

logger = structlog.get_logger()


class EventCollectorStats(BaseModel):
    """Statistics for event collector performance."""
    events_processed: int = 0
    events_failed: int = 0
    processing_rate_per_second: float = 0.0
    avg_processing_time_ms: float = 0.0
    p95_processing_time_ms: float = 0.0
    last_processed_time: Optional[datetime] = None
    consumer_group_lag: int = 0
    memory_usage_mb: float = 0.0


class EventEnricher:
    """
    Event enrichment service for adding semantic context and metadata.
    
    Enriches events with:
    - Semantic embeddings for context-aware analysis
    - Session context and agent persona data
    - Performance metrics and system information
    """
    
    def __init__(self):
        self.embedding_service = None
        self._session_cache: Dict[str, Dict[str, Any]] = {}
        self._agent_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_ttl = 300  # 5 minutes
        
    async def _ensure_embedding_service(self):
        """Ensure embedding service is initialized."""
        if self.embedding_service is None:
            self.embedding_service = await get_embedding_service()
    
    async def enrich_event(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enrich event with semantic context and metadata.
        
        Args:
            event_data: Raw event data from Redis stream
            
        Returns:
            Enriched event data with additional context
        """
        try:
            enriched = event_data.copy()
            
            # Add system metadata
            enriched['enrichment_timestamp'] = datetime.utcnow().isoformat()
            enriched['collector_version'] = "1.0.0"
            
            # Enrich with semantic embeddings for text content
            await self._add_semantic_embeddings(enriched)
            
            # Enrich with session context
            await self._add_session_context(enriched)
            
            # Enrich with agent context
            await self._add_agent_context(enriched)
            
            # Add performance context
            self._add_performance_context(enriched)
            
            return enriched
            
        except Exception as e:
            logger.error("‚ùå Event enrichment failed", error=str(e), exc_info=True)
            return event_data
    
    async def _add_semantic_embeddings(self, event_data: Dict[str, Any]) -> None:
        """Add semantic embeddings for textual content."""
        try:
            await self._ensure_embedding_service()
            
            # Extract text content for embedding
            text_fields = []
            
            # Check for query text in semantic events
            if 'query_text' in event_data:
                text_fields.append(event_data['query_text'])
            
            # Check for error messages
            if 'error' in event_data:
                text_fields.append(event_data['error'])
            
            # Check for failure descriptions
            if 'failure_description' in event_data:
                text_fields.append(event_data['failure_description'])
            
            # Generate embeddings for text content
            if text_fields and self.embedding_service:
                combined_text = ' '.join(text_fields)
                embedding = await self.embedding_service.generate_embedding(combined_text)
                event_data['semantic_embedding'] = embedding
                
        except Exception as e:
            logger.debug("Failed to add semantic embeddings", error=str(e))
    
    async def _add_session_context(self, event_data: Dict[str, Any]) -> None:
        """Add session context information."""
        session_id = event_data.get('session_id')
        if not session_id:
            return
        
        try:
            # Check cache first
            if session_id in self._session_cache:
                cache_entry = self._session_cache[session_id]
                if time.time() - cache_entry['timestamp'] < self._cache_ttl:
                    event_data['session_context'] = cache_entry['data']
                    return
            
            # Fetch session context from database
            async with get_session() as db_session:
                # This would query actual session data - simplified for now
                session_context = {
                    "session_start_time": "2024-01-01T00:00:00Z",
                    "session_duration_minutes": 30,
                    "workflow_count": 5,
                    "agent_count": 3
                }
                
                # Cache the result
                self._session_cache[session_id] = {
                    'data': session_context,
                    'timestamp': time.time()
                }
                
                event_data['session_context'] = session_context
                
        except Exception as e:
            logger.debug("Failed to add session context", error=str(e))
    
    async def _add_agent_context(self, event_data: Dict[str, Any]) -> None:
        """Add agent context information."""
        agent_id = event_data.get('agent_id')
        if not agent_id:
            return
        
        try:
            # Check cache first
            if agent_id in self._agent_cache:
                cache_entry = self._agent_cache[agent_id]
                if time.time() - cache_entry['timestamp'] < self._cache_ttl:
                    event_data['agent_context'] = cache_entry['data']
                    return
            
            # Fetch agent context from database
            async with get_session() as db_session:
                # This would query actual agent data - simplified for now
                agent_context = {
                    "agent_type": "claude",
                    "capabilities": ["text_processing", "code_generation"],
                    "current_workload": 0.7,
                    "performance_rating": 0.95
                }
                
                # Cache the result
                self._agent_cache[agent_id] = {
                    'data': agent_context,
                    'timestamp': time.time()
                }
                
                event_data['agent_context'] = agent_context
                
        except Exception as e:
            logger.debug("Failed to add agent context", error=str(e))
    
    def _add_performance_context(self, event_data: Dict[str, Any]) -> None:
        """Add performance context information."""
        try:
            # Add system performance indicators
            event_data['system_performance'] = {
                "cpu_load": 0.3,  # Would get from system metrics
                "memory_usage": 0.6,
                "network_latency_ms": 2.5,
                "disk_io_rate": 0.1
            }
            
        except Exception as e:
            logger.debug("Failed to add performance context", error=str(e))


class EventCollectorService:
    """
    High-performance event collector service for multi-agent observability.
    
    Consumes events from Redis Streams, enriches them with context,
    and persists to database with batch processing for optimal performance.
    """
    
    def __init__(
        self,
        redis_client: Optional[redis.Redis] = None,
        stream_name: str = "observability_events",
        consumer_group: str = "event_collectors",
        consumer_name: str = "collector_1",
        batch_size: int = 50,
        batch_timeout_ms: int = 1000,
        max_processing_time_ms: int = 5000
    ):
        """Initialize event collector service."""
        self.redis_client = redis_client
        self.stream_name = stream_name
        self.consumer_group = consumer_group
        self.consumer_name = consumer_name
        self.batch_size = batch_size
        self.batch_timeout_ms = batch_timeout_ms
        self.max_processing_time_ms = max_processing_time_ms
        
        # Event enricher
        self.enricher = EventEnricher()
        
        # Statistics tracking
        self.stats = EventCollectorStats()
        self._processing_times: List[float] = []
        self._max_processing_samples = 1000
        
        # Control flags
        self._is_running = False
        self._stop_event = asyncio.Event()
        
        logger.info(
            "üè≠ EventCollectorService initialized",
            stream_name=stream_name,
            consumer_group=consumer_group,
            batch_size=batch_size
        )
    
    async def _ensure_redis_client(self) -> None:
        """Ensure Redis client is initialized."""
        if self.redis_client is None:
            self.redis_client = await get_redis()
    
    async def start(self) -> None:
        """Start the event collector service."""
        if self._is_running:
            logger.warning("Event collector service is already running")
            return
        
        await self._ensure_redis_client()
        
        # Create consumer group
        try:
            await self.redis_client.xgroup_create(
                self.stream_name,
                self.consumer_group,
                id='0',
                mkstream=True
            )
            logger.info(f"‚úÖ Created consumer group '{self.consumer_group}'")
        except Exception as e:
            if "BUSYGROUP" in str(e):
                logger.info(f"‚ÑπÔ∏è Consumer group '{self.consumer_group}' already exists")
            else:
                logger.error("‚ùå Failed to create consumer group", error=str(e))
                raise
        
        self._is_running = True
        self._stop_event.clear()
        
        logger.info("üöÄ Event collector service started")
        
        # Start background processing
        asyncio.create_task(self._process_events_loop())
    
    async def stop(self) -> None:
        """Stop the event collector service gracefully."""
        if not self._is_running:
            logger.warning("Event collector service is not running")
            return
        
        logger.info("üõë Stopping event collector service...")
        self._stop_event.set()
        
        # Wait for graceful shutdown
        timeout = 10  # seconds
        for _ in range(timeout * 10):
            if not self._is_running:
                break
            await asyncio.sleep(0.1)
        
        if self._is_running:
            logger.warning("‚ö†Ô∏è Event collector service did not stop gracefully")
        else:
            logger.info("‚úÖ Event collector service stopped")
    
    async def _process_events_loop(self) -> None:
        """Main event processing loop."""
        try:
            while self._is_running and not self._stop_event.is_set():
                await self._process_event_batch()
                
                # Small delay to prevent busy waiting
                await asyncio.sleep(0.01)
                
        except Exception as e:
            logger.error("‚ùå Event processing loop failed", error=str(e), exc_info=True)
        finally:
            self._is_running = False
            logger.info("üõë Event processing loop stopped")
    
    async def _process_event_batch(self) -> None:
        """Process a batch of events from Redis Stream."""
        start_time = time.time()
        
        try:
            # Read messages from stream
            messages = await self.redis_client.xreadgroup(
                self.consumer_group,
                self.consumer_name,
                {self.stream_name: ">"},
                count=self.batch_size,
                block=self.batch_timeout_ms
            )
            
            if not messages:
                return
            
            # Process messages
            events_to_persist = []
            message_ids = []
            
            for stream, msgs in messages:
                for msg_id, fields in msgs:
                    try:
                        # Deserialize event data
                        event_data = await self._deserialize_event(fields)
                        
                        # Enrich event
                        enriched_event = await self.enricher.enrich_event(event_data)
                        
                        # Create database model
                        agent_event = await self._create_agent_event(enriched_event)
                        if agent_event:
                            events_to_persist.append(agent_event)
                            message_ids.append(msg_id.decode() if isinstance(msg_id, bytes) else str(msg_id))
                        
                    except Exception as e:
                        logger.error(
                            "‚ùå Failed to process event message",
                            error=str(e),
                            msg_id=str(msg_id),
                            exc_info=True
                        )
            
            # Persist batch to database
            if events_to_persist:
                await self._persist_event_batch(events_to_persist)
                
                # Acknowledge processed messages
                for msg_id in message_ids:
                    try:
                        await self.redis_client.xack(
                            self.stream_name,
                            self.consumer_group,
                            msg_id
                        )
                    except Exception as e:
                        logger.error(
                            "‚ùå Failed to acknowledge message",
                            error=str(e),
                            msg_id=msg_id
                        )
                
                # Update statistics
                processing_time_ms = (time.time() - start_time) * 1000
                self._update_statistics(len(events_to_persist), processing_time_ms)
                
                logger.debug(
                    "üì¶ Processed event batch",
                    batch_size=len(events_to_persist),
                    processing_time_ms=round(processing_time_ms, 2),
                    acknowledged=len(message_ids)
                )
            
        except Exception as e:
            logger.error("‚ùå Failed to process event batch", error=str(e), exc_info=True)
            self.stats.events_failed += 1
    
    async def _deserialize_event(self, fields: Dict[bytes, bytes]) -> Dict[str, Any]:
        """Deserialize event data from Redis stream fields."""
        try:
            # Convert bytes to strings
            decoded_fields = {
                k.decode() if isinstance(k, bytes) else k: 
                v.decode() if isinstance(v, bytes) else v 
                for k, v in fields.items()
            }
            
            # Extract and deserialize the main event data
            event_data_bytes = decoded_fields.get('event_data')
            if event_data_bytes:
                # Deserialize MessagePack data
                if isinstance(event_data_bytes, str):
                    event_data_bytes = event_data_bytes.encode()
                
                event_data = msgpack.unpackb(event_data_bytes, raw=False)
                
                # Add stream metadata
                event_data.update({
                    'stream_timestamp': decoded_fields.get('timestamp'),
                    'stream_agent_id': decoded_fields.get('agent_id'),
                    'stream_session_id': decoded_fields.get('session_id'),
                    'stream_workflow_id': decoded_fields.get('workflow_id')
                })
                
                return event_data
            else:
                # Fall back to using decoded fields directly
                return decoded_fields
                
        except Exception as e:
            logger.error("‚ùå Event deserialization failed", error=str(e), exc_info=True)
            raise
    
    async def _create_agent_event(self, event_data: Dict[str, Any]) -> Optional[AgentEvent]:
        """Create AgentEvent database model from event data."""
        try:
            # Extract required fields
            event_type_str = event_data.get('event_type', 'unknown')
            
            # Map string event type to EventType enum
            try:
                event_type = EventType(event_type_str)
            except ValueError:
                # Handle unknown event types
                event_type = EventType.NOTIFICATION
                logger.warning(f"Unknown event type: {event_type_str}")
            
            # Extract identifiers
            session_id = event_data.get('session_id') or event_data.get('stream_session_id')
            agent_id = event_data.get('agent_id') or event_data.get('stream_agent_id')
            
            # Convert string UUIDs to UUID objects
            if session_id and isinstance(session_id, str) and session_id != '':
                try:
                    session_id = uuid.UUID(session_id)
                except ValueError:
                    session_id = None
            
            if agent_id and isinstance(agent_id, str) and agent_id != '':
                try:
                    agent_id = uuid.UUID(agent_id)
                except ValueError:
                    agent_id = None
            
            # Extract performance metrics
            latency_ms = None
            performance_metrics = event_data.get('performance_metrics', {})
            if performance_metrics:
                latency_ms = performance_metrics.get('execution_time_ms')
            
            # Create AgentEvent
            agent_event = AgentEvent(
                session_id=session_id,
                agent_id=agent_id,
                event_type=event_type,
                payload=event_data,
                latency_ms=int(latency_ms) if latency_ms else None
            )
            
            return agent_event
            
        except Exception as e:
            logger.error("‚ùå Failed to create AgentEvent", error=str(e), exc_info=True)
            return None
    
    async def _persist_event_batch(self, events: List[AgentEvent]) -> None:
        """Persist batch of events to database efficiently."""
        try:
            async with get_session() as db_session:
                # Add all events to session
                for event in events:
                    db_session.add(event)
                
                # Commit batch
                await db_session.commit()
                
                logger.debug(
                    "üíæ Event batch persisted to database",
                    batch_size=len(events)
                )
                
        except Exception as e:
            logger.error(
                "‚ùå Failed to persist event batch",
                error=str(e),
                batch_size=len(events),
                exc_info=True
            )
            raise
    
    def _update_statistics(self, events_processed: int, processing_time_ms: float) -> None:
        """Update service statistics."""
        self.stats.events_processed += events_processed
        self.stats.last_processed_time = datetime.utcnow()
        
        # Track processing times
        self._processing_times.append(processing_time_ms)
        if len(self._processing_times) > self._max_processing_samples:
            self._processing_times.pop(0)
        
        # Calculate performance metrics
        if self._processing_times:
            self.stats.avg_processing_time_ms = sum(self._processing_times) / len(self._processing_times)
            sorted_times = sorted(self._processing_times)
            p95_index = int(0.95 * len(sorted_times))
            self.stats.p95_processing_time_ms = sorted_times[p95_index] if sorted_times else 0
        
        # Calculate processing rate
        if self.stats.last_processed_time:
            time_window_seconds = 60  # 1 minute window
            recent_events = sum(1 for _ in self._processing_times[-int(time_window_seconds):])
            self.stats.processing_rate_per_second = recent_events / min(time_window_seconds, len(self._processing_times))
    
    async def get_statistics(self) -> EventCollectorStats:
        """Get current service statistics."""
        # Update consumer group lag
        try:
            await self._ensure_redis_client()
            groups_info = await self.redis_client.xinfo_groups(self.stream_name)
            
            for group in groups_info:
                if group[b'name'].decode() == self.consumer_group:
                    self.stats.consumer_group_lag = group[b'pending']
                    break
                    
        except Exception as e:
            logger.debug("Failed to get consumer group lag", error=str(e))
        
        return self.stats
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform health check of the event collector service."""
        try:
            # Check Redis connectivity
            await self._ensure_redis_client()
            await self.redis_client.ping()
            redis_healthy = True
        except:
            redis_healthy = False
        
        # Check database connectivity
        try:
            async with get_session() as db_session:
                await db_session.execute("SELECT 1")
            database_healthy = True
        except:
            database_healthy = False
        
        # Determine overall health
        if redis_healthy and database_healthy and self._is_running:
            status = "healthy"
        elif redis_healthy and database_healthy:
            status = "degraded"  # Not running but components healthy
        else:
            status = "unhealthy"
        
        # Check performance targets
        performance_within_target = (
            self.stats.p95_processing_time_ms <= 5.0 and
            self.stats.processing_rate_per_second >= 200
        )
        
        return {
            "status": status,
            "is_running": self._is_running,
            "redis_healthy": redis_healthy,
            "database_healthy": database_healthy,
            "performance_within_target": performance_within_target,
            "statistics": self.stats.dict()
        }


# Global event collector service instance
_event_collector: Optional[EventCollectorService] = None


def get_event_collector() -> Optional[EventCollectorService]:
    """Get the global event collector service."""
    return _event_collector


async def initialize_event_collector(
    redis_client: Optional[redis.Redis] = None,
    **kwargs
) -> EventCollectorService:
    """Initialize and start the global event collector service."""
    global _event_collector
    
    _event_collector = EventCollectorService(redis_client=redis_client, **kwargs)
    await _event_collector.start()
    
    logger.info("‚úÖ Global event collector service initialized and started")
    return _event_collector


async def shutdown_event_collector() -> None:
    """Shutdown the global event collector service."""
    global _event_collector
    
    if _event_collector:
        await _event_collector.stop()
        _event_collector = None
        logger.info("‚úÖ Event collector service shutdown complete")


# FastAPI app for event collector service
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan context manager."""
    # Startup
    await initialize_event_collector()
    
    yield
    
    # Shutdown
    await shutdown_event_collector()


# Create FastAPI app
event_collector_app = FastAPI(
    title="LeanVibe Event Collector Service",
    description="High-performance event collector for multi-agent observability",
    version="1.0.0",
    lifespan=lifespan
)


@event_collector_app.get("/health")
async def health_check():
    """Health check endpoint."""
    collector = get_event_collector()
    if not collector:
        raise HTTPException(status_code=503, detail="Event collector not initialized")
    
    health_status = await collector.health_check()
    
    if health_status["status"] == "unhealthy":
        raise HTTPException(status_code=503, detail="Event collector unhealthy")
    
    return health_status


@event_collector_app.get("/statistics")
async def get_statistics():
    """Get event collector statistics."""
    collector = get_event_collector()
    if not collector:
        raise HTTPException(status_code=503, detail="Event collector not initialized")
    
    return await collector.get_statistics()


@event_collector_app.post("/analytics", response_model=EventAnalyticsResponse)
async def get_event_analytics(request: EventAnalyticsRequest):
    """Get event analytics for a time range."""
    try:
        async with get_session() as db_session:
            # Calculate time range
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(hours=request.time_range_hours)
            
            # Base query
            query = select(AgentEvent).where(
                AgentEvent.created_at >= start_time,
                AgentEvent.created_at <= end_time
            )
            
            # Apply filters
            if request.session_id:
                query = query.where(AgentEvent.session_id == request.session_id)
            if request.agent_id:
                query = query.where(AgentEvent.agent_id == request.agent_id)
            if request.event_types:
                query = query.where(AgentEvent.event_type.in_(request.event_types))
            
            # Execute query
            result = await db_session.execute(query)
            events = result.scalars().all()
            
            # Generate analytics
            summary = {
                "total_events": len(events),
                "time_range_hours": request.time_range_hours,
                "unique_sessions": len(set(e.session_id for e in events if e.session_id)),
                "unique_agents": len(set(e.agent_id for e in events if e.agent_id))
            }
            
            # Event distribution
            event_distribution = {}
            for event in events:
                event_type = event.event_type.value
                event_distribution[event_type] = event_distribution.get(event_type, 0) + 1
            
            # Performance trends (simplified)
            performance_trends = {
                "avg_latency_ms": sum(e.latency_ms for e in events if e.latency_ms) / max(1, len([e for e in events if e.latency_ms])),
                "events_per_hour": len(events) / request.time_range_hours
            }
            
            # Error patterns
            error_events = [e for e in events if not e.payload.get('success', True)]
            error_patterns = {
                "total_errors": len(error_events),
                "error_rate": len(error_events) / max(1, len(events)),
                "common_errors": {}
            }
            
            # Generate recommendations
            recommendations = []
            if error_patterns["error_rate"] > 0.1:
                recommendations.append("High error rate detected - investigate failing operations")
            if performance_trends["avg_latency_ms"] > 1000:
                recommendations.append("High latency detected - consider performance optimization")
            
            return EventAnalyticsResponse(
                summary=summary,
                event_distribution=event_distribution,
                performance_trends=performance_trends,
                error_patterns=error_patterns,
                agent_activity=None,  # Could be enhanced
                recommendations=recommendations,
                generated_at=datetime.utcnow()
            )
            
    except Exception as e:
        logger.error("‚ùå Failed to generate event analytics", error=str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analytics generation failed: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app.services.event_collector_service:event_collector_app",
        host="0.0.0.0",
        port=8001,
        reload=False,
        log_level="info"
    )