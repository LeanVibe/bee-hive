"""
Operations Specialist Agent - Operational Excellence Framework
Epic G: Production Readiness - Phase 3

Enterprise-grade operational excellence with automated backup and disaster recovery,
log aggregation and analysis, resource optimization, capacity planning, and
maintenance procedures for the LeanVibe Agent Hive 2.0 platform.
"""

import asyncio
import json
import gzip
import os
import shutil
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import tempfile
import tarfile
import time

import structlog
import aiofiles
import boto3
import schedule
from elasticsearch import AsyncElasticsearch
import psutil

logger = structlog.get_logger(__name__)

class BackupType(Enum):
    """Backup type classifications."""
    FULL = "full"
    INCREMENTAL = "incremental"
    DIFFERENTIAL = "differential"
    TRANSACTION_LOG = "transaction_log"

class BackupStatus(Enum):
    """Backup operation status."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    EXPIRED = "expired"

class MaintenanceType(Enum):
    """Maintenance operation types."""
    SCHEDULED_UPDATE = "scheduled_update"
    SECURITY_PATCH = "security_patch"
    SCALING_EVENT = "scaling_event"
    CONFIGURATION_CHANGE = "configuration_change"
    DISASTER_RECOVERY_TEST = "disaster_recovery_test"

@dataclass
class BackupConfiguration:
    """Backup configuration settings."""
    backup_type: BackupType
    schedule_cron: str
    retention_days: int
    compression_enabled: bool = True
    encryption_enabled: bool = True
    storage_backend: str = "s3"  # s3, gcs, azure, local
    storage_config: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.storage_config is None:
            self.storage_config = {}

@dataclass
class BackupRecord:
    """Backup operation record."""
    backup_id: str
    backup_type: BackupType
    status: BackupStatus
    start_time: datetime
    end_time: Optional[datetime]
    file_path: str
    file_size_bytes: int
    compressed_size_bytes: Optional[int]
    encryption_key_id: Optional[str]
    metadata: Dict[str, Any]
    error_message: Optional[str] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class ResourceMetrics:
    """System resource metrics."""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    disk_usage_percent: float
    network_io_bytes: Tuple[int, int]  # (sent, received)
    disk_io_bytes: Tuple[int, int]  # (read, write)
    active_connections: int
    load_average: Tuple[float, float, float]  # 1min, 5min, 15min

@dataclass
class CapacityPrediction:
    """Capacity planning prediction."""
    component: str
    metric_name: str
    current_value: float
    predicted_value: float
    prediction_date: datetime
    confidence_score: float
    recommendation: str
    action_required: bool

class AutomatedBackupSystem:
    """Automated backup and disaster recovery system."""
    
    def __init__(self, config: BackupConfiguration):
        self.config = config
        self.backup_history: List[BackupRecord] = []
        self.is_running = False
        self.backup_tasks = []
        
    async def initialize(self):
        """Initialize backup system."""
        logger.info("ðŸ—„ï¸ Initializing Automated Backup System")
        
        # Validate storage backend configuration
        await self._validate_storage_config()
        
        # Schedule backup jobs
        await self._schedule_backup_jobs()
        
        logger.info("âœ… Automated Backup System initialized")
    
    async def start(self):
        """Start backup scheduler."""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info("Starting backup scheduler")
        
        # Start backup scheduler task
        scheduler_task = asyncio.create_task(self._run_backup_scheduler())
        self.backup_tasks.append(scheduler_task)
        
        # Start backup retention cleanup task
        cleanup_task = asyncio.create_task(self._run_retention_cleanup())
        self.backup_tasks.append(cleanup_task)
        
        logger.info("âœ… Backup scheduler started")
    
    async def stop(self):
        """Stop backup scheduler."""
        if not self.is_running:
            return
        
        self.is_running = False
        logger.info("Stopping backup scheduler")
        
        # Cancel all backup tasks
        for task in self.backup_tasks:
            task.cancel()
        
        await asyncio.gather(*self.backup_tasks, return_exceptions=True)
        self.backup_tasks.clear()
        
        logger.info("âœ… Backup scheduler stopped")
    
    async def create_backup(self, backup_type: BackupType = None) -> BackupRecord:
        """Create a backup manually or on schedule."""
        backup_type = backup_type or self.config.backup_type
        backup_id = f"backup-{int(datetime.utcnow().timestamp())}"
        
        logger.info(f"Creating {backup_type.value} backup: {backup_id}")
        
        start_time = datetime.utcnow()
        backup_record = BackupRecord(
            backup_id=backup_id,
            backup_type=backup_type,
            status=BackupStatus.IN_PROGRESS,
            start_time=start_time,
            end_time=None,
            file_path="",
            file_size_bytes=0,
            compressed_size_bytes=None,
            encryption_key_id=None,
            metadata={'created_by': 'automated_system'}
        )
        
        try:
            # Create backup based on type
            if backup_type == BackupType.FULL:
                backup_path = await self._create_full_backup(backup_id)
            elif backup_type == BackupType.INCREMENTAL:
                backup_path = await self._create_incremental_backup(backup_id)
            else:
                raise ValueError(f"Unsupported backup type: {backup_type}")
            
            # Get file size
            file_size = os.path.getsize(backup_path)
            
            # Compress if enabled
            compressed_path = backup_path
            compressed_size = None
            if self.config.compression_enabled:
                compressed_path = await self._compress_backup(backup_path)
                compressed_size = os.path.getsize(compressed_path)
                os.remove(backup_path)  # Remove uncompressed version
            
            # Encrypt if enabled
            final_path = compressed_path
            encryption_key_id = None
            if self.config.encryption_enabled:
                final_path, encryption_key_id = await self._encrypt_backup(compressed_path)
                if final_path != compressed_path:
                    os.remove(compressed_path)  # Remove unencrypted version
            
            # Upload to storage backend
            storage_path = await self._upload_to_storage(final_path, backup_id)
            
            # Clean up local file
            os.remove(final_path)
            
            # Update backup record
            backup_record.status = BackupStatus.SUCCESS
            backup_record.end_time = datetime.utcnow()
            backup_record.file_path = storage_path
            backup_record.file_size_bytes = file_size
            backup_record.compressed_size_bytes = compressed_size
            backup_record.encryption_key_id = encryption_key_id
            
            backup_duration = (backup_record.end_time - start_time).total_seconds()
            logger.info(
                f"Backup completed successfully",
                backup_id=backup_id,
                duration_seconds=backup_duration,
                file_size_mb=round(file_size / 1024 / 1024, 2),
                compression_ratio=round(compressed_size / file_size, 2) if compressed_size else 1.0
            )
            
        except Exception as e:
            backup_record.status = BackupStatus.FAILED
            backup_record.end_time = datetime.utcnow()
            backup_record.error_message = str(e)
            
            logger.error(f"Backup failed: {e}", backup_id=backup_id)
        
        # Add to history
        self.backup_history.append(backup_record)
        
        return backup_record
    
    async def restore_backup(self, backup_id: str, restore_path: str = None) -> bool:
        """Restore from backup."""
        logger.info(f"Starting restore from backup: {backup_id}")
        
        # Find backup record
        backup_record = None
        for record in self.backup_history:
            if record.backup_id == backup_id:
                backup_record = record
                break
        
        if not backup_record:
            logger.error(f"Backup not found: {backup_id}")
            return False
        
        if backup_record.status != BackupStatus.SUCCESS:
            logger.error(f"Cannot restore from failed backup: {backup_id}")
            return False
        
        try:
            # Download from storage
            local_path = await self._download_from_storage(backup_record.file_path, backup_id)
            
            # Decrypt if needed
            if backup_record.encryption_key_id:
                decrypted_path = await self._decrypt_backup(local_path, backup_record.encryption_key_id)
                os.remove(local_path)
                local_path = decrypted_path
            
            # Decompress if needed
            if backup_record.compressed_size_bytes:
                decompressed_path = await self._decompress_backup(local_path)
                os.remove(local_path)
                local_path = decompressed_path
            
            # Restore data
            success = await self._restore_data(local_path, restore_path)
            
            # Clean up
            os.remove(local_path)
            
            logger.info(f"Restore completed: {'success' if success else 'failed'}")
            return success
            
        except Exception as e:
            logger.error(f"Restore failed: {e}", backup_id=backup_id)
            return False
    
    async def _create_full_backup(self, backup_id: str) -> str:
        """Create full system backup."""
        backup_path = f"/tmp/{backup_id}_full.tar"
        
        # This would create a full backup of the database, Redis, and application data
        # For now, creating a sample backup
        with tarfile.open(backup_path, 'w') as tar:
            # Add configuration files
            if os.path.exists('/app'):
                tar.add('/app', arcname='app', recursive=False)
        
        return backup_path
    
    async def _create_incremental_backup(self, backup_id: str) -> str:
        """Create incremental backup."""
        backup_path = f"/tmp/{backup_id}_incremental.tar"
        
        # This would create an incremental backup
        # For now, creating a minimal backup
        with tarfile.open(backup_path, 'w') as tar:
            tar.addfile(tarfile.TarInfo("incremental_marker"), None)
        
        return backup_path
    
    async def _compress_backup(self, file_path: str) -> str:
        """Compress backup file."""
        compressed_path = f"{file_path}.gz"
        
        with open(file_path, 'rb') as f_in:
            with gzip.open(compressed_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        return compressed_path
    
    async def _encrypt_backup(self, file_path: str) -> Tuple[str, str]:
        """Encrypt backup file."""
        encrypted_path = f"{file_path}.enc"
        encryption_key_id = "backup-key-001"  # Would use proper key management
        
        # This would use proper encryption (AES-256, etc.)
        # For now, just rename the file
        shutil.move(file_path, encrypted_path)
        
        return encrypted_path, encryption_key_id
    
    async def _upload_to_storage(self, file_path: str, backup_id: str) -> str:
        """Upload backup to storage backend."""
        if self.config.storage_backend == "s3":
            return await self._upload_to_s3(file_path, backup_id)
        elif self.config.storage_backend == "local":
            return await self._upload_to_local(file_path, backup_id)
        else:
            raise ValueError(f"Unsupported storage backend: {self.config.storage_backend}")
    
    async def _upload_to_s3(self, file_path: str, backup_id: str) -> str:
        """Upload to AWS S3."""
        s3_client = boto3.client('s3')
        bucket = self.config.storage_config.get('bucket', 'leanvibe-backups')
        key = f"backups/{datetime.utcnow().strftime('%Y/%m/%d')}/{backup_id}"
        
        s3_client.upload_file(file_path, bucket, key)
        return f"s3://{bucket}/{key}"
    
    async def _upload_to_local(self, file_path: str, backup_id: str) -> str:
        """Upload to local storage."""
        storage_dir = self.config.storage_config.get('directory', '/backups')
        os.makedirs(storage_dir, exist_ok=True)
        
        destination = os.path.join(storage_dir, f"{backup_id}.backup")
        shutil.move(file_path, destination)
        
        return destination
    
    async def _validate_storage_config(self):
        """Validate storage configuration."""
        if self.config.storage_backend == "s3":
            required_keys = ['bucket']
            for key in required_keys:
                if key not in self.config.storage_config:
                    raise ValueError(f"Missing S3 config: {key}")
    
    async def _schedule_backup_jobs(self):
        """Schedule backup jobs based on configuration."""
        # This would integrate with a proper scheduler like Celery or APScheduler
        logger.info(f"Scheduling backup jobs: {self.config.schedule_cron}")
    
    async def _run_backup_scheduler(self):
        """Run backup scheduler loop."""
        while self.is_running:
            try:
                # Check if it's time for a backup (simplified)
                await asyncio.sleep(3600)  # Check every hour
                
                # Create backup if scheduled
                current_hour = datetime.utcnow().hour
                if current_hour == 2:  # 2 AM backup
                    await self.create_backup()
                
            except Exception as e:
                logger.error("Backup scheduler error", error=str(e))
                await asyncio.sleep(600)  # Wait 10 minutes on error
    
    async def _run_retention_cleanup(self):
        """Clean up old backups based on retention policy."""
        while self.is_running:
            try:
                cutoff_date = datetime.utcnow() - timedelta(days=self.config.retention_days)
                
                expired_backups = [
                    record for record in self.backup_history
                    if record.start_time < cutoff_date and record.status == BackupStatus.SUCCESS
                ]
                
                for backup in expired_backups:
                    await self._delete_backup(backup)
                    backup.status = BackupStatus.EXPIRED
                
                if expired_backups:
                    logger.info(f"Cleaned up {len(expired_backups)} expired backups")
                
                await asyncio.sleep(86400)  # Run daily
                
            except Exception as e:
                logger.error("Retention cleanup error", error=str(e))
                await asyncio.sleep(3600)
    
    async def _delete_backup(self, backup_record: BackupRecord):
        """Delete backup from storage."""
        # Implementation would depend on storage backend
        logger.info(f"Deleting expired backup: {backup_record.backup_id}")

class LogAggregationSystem:
    """Advanced log aggregation and analysis system."""
    
    def __init__(self):
        self.elasticsearch_client = None
        self.log_processors = []
        self.is_running = False
        
    async def initialize(self):
        """Initialize log aggregation system."""
        logger.info("ðŸ“Š Initializing Log Aggregation System")
        
        # Initialize Elasticsearch client
        try:
            self.elasticsearch_client = AsyncElasticsearch([
                {'host': 'elasticsearch', 'port': 9200}
            ])
            
            # Test connection
            health = await self.elasticsearch_client.cluster.health()
            logger.info(f"Elasticsearch connection established: {health['status']}")
            
        except Exception as e:
            logger.warning(f"Elasticsearch not available: {e}")
            self.elasticsearch_client = None
        
        logger.info("âœ… Log Aggregation System initialized")
    
    async def start(self):
        """Start log aggregation services."""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info("Starting log aggregation services")
        
        # Start log processing tasks
        processor_task = asyncio.create_task(self._process_logs())
        self.log_processors.append(processor_task)
        
        # Start log analytics task
        analytics_task = asyncio.create_task(self._analyze_logs())
        self.log_processors.append(analytics_task)
        
        logger.info("âœ… Log aggregation services started")
    
    async def stop(self):
        """Stop log aggregation services."""
        if not self.is_running:
            return
        
        self.is_running = False
        logger.info("Stopping log aggregation services")
        
        # Cancel processor tasks
        for task in self.log_processors:
            task.cancel()
        
        await asyncio.gather(*self.log_processors, return_exceptions=True)
        self.log_processors.clear()
        
        # Close Elasticsearch client
        if self.elasticsearch_client:
            await self.elasticsearch_client.close()
        
        logger.info("âœ… Log aggregation services stopped")
    
    async def ingest_log_entry(self, log_entry: Dict[str, Any]):
        """Ingest a log entry into the system."""
        if not self.elasticsearch_client:
            return
        
        # Add metadata
        log_entry['@timestamp'] = datetime.utcnow().isoformat()
        log_entry['@version'] = '1'
        log_entry['host'] = os.uname().nodename
        
        # Determine index based on log level
        index = f"leanvibe-logs-{datetime.utcnow().strftime('%Y.%m.%d')}"
        
        try:
            await self.elasticsearch_client.index(
                index=index,
                body=log_entry
            )
        except Exception as e:
            logger.error(f"Failed to ingest log entry: {e}")
    
    async def search_logs(
        self,
        query: str,
        time_range: timedelta = timedelta(hours=24),
        size: int = 100
    ) -> List[Dict[str, Any]]:
        """Search logs using Elasticsearch."""
        if not self.elasticsearch_client:
            return []
        
        end_time = datetime.utcnow()
        start_time = end_time - time_range
        
        search_body = {
            'query': {
                'bool': {
                    'must': [
                        {'query_string': {'query': query}},
                        {
                            'range': {
                                '@timestamp': {
                                    'gte': start_time.isoformat(),
                                    'lte': end_time.isoformat()
                                }
                            }
                        }
                    ]
                }
            },
            'sort': [{'@timestamp': {'order': 'desc'}}],
            'size': size
        }
        
        try:
            response = await self.elasticsearch_client.search(
                index='leanvibe-logs-*',
                body=search_body
            )
            
            return [hit['_source'] for hit in response['hits']['hits']]
            
        except Exception as e:
            logger.error(f"Log search failed: {e}")
            return []
    
    async def analyze_error_patterns(self, time_range: timedelta = timedelta(hours=24)) -> Dict[str, Any]:
        """Analyze error patterns in logs."""
        if not self.elasticsearch_client:
            return {}
        
        end_time = datetime.utcnow()
        start_time = end_time - time_range
        
        # Aggregation query to find error patterns
        agg_body = {
            'query': {
                'bool': {
                    'must': [
                        {'term': {'level': 'ERROR'}},
                        {
                            'range': {
                                '@timestamp': {
                                    'gte': start_time.isoformat(),
                                    'lte': end_time.isoformat()
                                }
                            }
                        }
                    ]
                }
            },
            'aggs': {
                'error_messages': {
                    'terms': {
                        'field': 'message.keyword',
                        'size': 10
                    }
                },
                'error_components': {
                    'terms': {
                        'field': 'logger_name.keyword',
                        'size': 10
                    }
                }
            },
            'size': 0
        }
        
        try:
            response = await self.elasticsearch_client.search(
                index='leanvibe-logs-*',
                body=agg_body
            )
            
            return {
                'total_errors': response['hits']['total']['value'],
                'error_messages': response['aggregations']['error_messages']['buckets'],
                'error_components': response['aggregations']['error_components']['buckets'],
                'analysis_period': {
                    'start': start_time.isoformat(),
                    'end': end_time.isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"Error pattern analysis failed: {e}")
            return {}
    
    async def _process_logs(self):
        """Process incoming logs."""
        while self.is_running:
            try:
                # This would process logs from various sources
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error("Log processing error", error=str(e))
                await asyncio.sleep(30)
    
    async def _analyze_logs(self):
        """Analyze logs for patterns and anomalies."""
        while self.is_running:
            try:
                # Run periodic log analysis
                error_analysis = await self.analyze_error_patterns()
                
                if error_analysis.get('total_errors', 0) > 100:  # Threshold
                    logger.warning(
                        "High error rate detected",
                        total_errors=error_analysis['total_errors'],
                        time_period="24h"
                    )
                
                await asyncio.sleep(3600)  # Run every hour
                
            except Exception as e:
                logger.error("Log analysis error", error=str(e))
                await asyncio.sleep(1800)

class ResourceOptimizationSystem:
    """Resource optimization and capacity planning system."""
    
    def __init__(self):
        self.metrics_history: List[ResourceMetrics] = []
        self.optimization_recommendations = []
        self.is_running = False
        self.monitoring_tasks = []
        
    async def initialize(self):
        """Initialize resource optimization system."""
        logger.info("ðŸŽ¯ Initializing Resource Optimization System")
        
        # Load historical metrics if available
        await self._load_metrics_history()
        
        logger.info("âœ… Resource Optimization System initialized")
    
    async def start(self):
        """Start resource monitoring and optimization."""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info("Starting resource optimization services")
        
        # Start resource monitoring
        monitoring_task = asyncio.create_task(self._monitor_resources())
        self.monitoring_tasks.append(monitoring_task)
        
        # Start optimization analysis
        optimization_task = asyncio.create_task(self._analyze_optimization_opportunities())
        self.monitoring_tasks.append(optimization_task)
        
        # Start capacity planning
        capacity_task = asyncio.create_task(self._perform_capacity_planning())
        self.monitoring_tasks.append(capacity_task)
        
        logger.info("âœ… Resource optimization services started")
    
    async def stop(self):
        """Stop resource optimization services."""
        if not self.is_running:
            return
        
        self.is_running = False
        logger.info("Stopping resource optimization services")
        
        # Cancel monitoring tasks
        for task in self.monitoring_tasks:
            task.cancel()
        
        await asyncio.gather(*self.monitoring_tasks, return_exceptions=True)
        self.monitoring_tasks.clear()
        
        # Save metrics history
        await self._save_metrics_history()
        
        logger.info("âœ… Resource optimization services stopped")
    
    async def collect_resource_metrics(self) -> ResourceMetrics:
        """Collect current resource metrics."""
        try:
            # Get CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Get memory usage
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # Get disk usage
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100
            
            # Get network I/O
            net_io = psutil.net_io_counters()
            network_io = (net_io.bytes_sent, net_io.bytes_recv)
            
            # Get disk I/O
            disk_io = psutil.disk_io_counters()
            disk_io_bytes = (disk_io.read_bytes, disk_io.write_bytes) if disk_io else (0, 0)
            
            # Get connection count (simplified)
            connections = len(psutil.net_connections())
            
            # Get load average
            load_avg = os.getloadavg()
            
            metrics = ResourceMetrics(
                timestamp=datetime.utcnow(),
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                disk_usage_percent=disk_percent,
                network_io_bytes=network_io,
                disk_io_bytes=disk_io_bytes,
                active_connections=connections,
                load_average=load_avg
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"Failed to collect resource metrics: {e}")
            return None
    
    async def analyze_resource_trends(self, hours: int = 24) -> Dict[str, Any]:
        """Analyze resource usage trends."""
        if len(self.metrics_history) < 10:
            return {}
        
        # Get metrics from the last N hours
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.metrics_history
            if m.timestamp > cutoff_time
        ]
        
        if not recent_metrics:
            return {}
        
        # Calculate trends
        cpu_values = [m.cpu_percent for m in recent_metrics]
        memory_values = [m.memory_percent for m in recent_metrics]
        disk_values = [m.disk_usage_percent for m in recent_metrics]
        
        analysis = {
            'period_hours': hours,
            'sample_count': len(recent_metrics),
            'cpu': {
                'avg': sum(cpu_values) / len(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values),
                'trend': 'stable'  # Would calculate actual trend
            },
            'memory': {
                'avg': sum(memory_values) / len(memory_values),
                'max': max(memory_values),
                'min': min(memory_values),
                'trend': 'stable'
            },
            'disk': {
                'avg': sum(disk_values) / len(disk_values),
                'max': max(disk_values),
                'min': min(disk_values),
                'trend': 'stable'
            }
        }
        
        return analysis
    
    async def predict_capacity_needs(self, days_ahead: int = 30) -> List[CapacityPrediction]:
        """Predict future capacity needs."""
        if len(self.metrics_history) < 100:  # Need enough data for prediction
            return []
        
        predictions = []
        
        # Simple linear prediction (would use more sophisticated ML in production)
        recent_metrics = self.metrics_history[-100:]  # Last 100 metrics
        
        # CPU prediction
        cpu_values = [m.cpu_percent for m in recent_metrics]
        cpu_avg = sum(cpu_values) / len(cpu_values)
        cpu_trend = (cpu_values[-10:] - cpu_values[:10]) / len(cpu_values) * days_ahead
        cpu_predicted = cpu_avg + cpu_trend
        
        predictions.append(CapacityPrediction(
            component="system",
            metric_name="cpu_percent",
            current_value=cpu_avg,
            predicted_value=cpu_predicted,
            prediction_date=datetime.utcnow() + timedelta(days=days_ahead),
            confidence_score=0.7,
            recommendation="Monitor CPU usage trends" if cpu_predicted < 80 else "Consider CPU upgrade",
            action_required=cpu_predicted > 80
        ))
        
        # Memory prediction
        memory_values = [m.memory_percent for m in recent_metrics]
        memory_avg = sum(memory_values) / len(memory_values)
        memory_trend = (memory_values[-10:] - memory_values[:10]) / len(memory_values) * days_ahead
        memory_predicted = memory_avg + memory_trend
        
        predictions.append(CapacityPrediction(
            component="system",
            metric_name="memory_percent",
            current_value=memory_avg,
            predicted_value=memory_predicted,
            prediction_date=datetime.utcnow() + timedelta(days=days_ahead),
            confidence_score=0.8,
            recommendation="Monitor memory usage" if memory_predicted < 85 else "Consider memory upgrade",
            action_required=memory_predicted > 85
        ))
        
        return predictions
    
    async def _monitor_resources(self):
        """Continuously monitor system resources."""
        while self.is_running:
            try:
                metrics = await self.collect_resource_metrics()
                if metrics:
                    self.metrics_history.append(metrics)
                    
                    # Keep only last 24 hours of metrics
                    cutoff_time = datetime.utcnow() - timedelta(hours=24)
                    self.metrics_history = [
                        m for m in self.metrics_history
                        if m.timestamp > cutoff_time
                    ]
                
                await asyncio.sleep(60)  # Collect every minute
                
            except Exception as e:
                logger.error("Resource monitoring error", error=str(e))
                await asyncio.sleep(300)  # Wait 5 minutes on error
    
    async def _analyze_optimization_opportunities(self):
        """Analyze system for optimization opportunities."""
        while self.is_running:
            try:
                if len(self.metrics_history) >= 60:  # Need at least 1 hour of data
                    analysis = await self.analyze_resource_trends(hours=6)
                    
                    recommendations = []
                    
                    # CPU optimization
                    if analysis.get('cpu', {}).get('avg', 0) > 80:
                        recommendations.append({
                            'component': 'cpu',
                            'severity': 'high',
                            'recommendation': 'CPU usage is consistently high. Consider scaling up or optimizing CPU-intensive processes.'
                        })
                    
                    # Memory optimization
                    if analysis.get('memory', {}).get('avg', 0) > 85:
                        recommendations.append({
                            'component': 'memory',
                            'severity': 'high',
                            'recommendation': 'Memory usage is high. Consider increasing memory allocation or optimizing memory usage.'
                        })
                    
                    self.optimization_recommendations = recommendations
                    
                    if recommendations:
                        logger.info(f"Generated {len(recommendations)} optimization recommendations")
                
                await asyncio.sleep(3600)  # Analyze every hour
                
            except Exception as e:
                logger.error("Optimization analysis error", error=str(e))
                await asyncio.sleep(1800)
    
    async def _perform_capacity_planning(self):
        """Perform capacity planning analysis."""
        while self.is_running:
            try:
                predictions = await self.predict_capacity_needs(days_ahead=30)
                
                for prediction in predictions:
                    if prediction.action_required:
                        logger.warning(
                            f"Capacity action required for {prediction.component}.{prediction.metric_name}",
                            current_value=prediction.current_value,
                            predicted_value=prediction.predicted_value,
                            recommendation=prediction.recommendation
                        )
                
                await asyncio.sleep(86400)  # Run daily
                
            except Exception as e:
                logger.error("Capacity planning error", error=str(e))
                await asyncio.sleep(3600)
    
    async def _load_metrics_history(self):
        """Load historical metrics from storage."""
        # Would load from persistent storage
        pass
    
    async def _save_metrics_history(self):
        """Save metrics history to storage."""
        # Would save to persistent storage
        pass

class OperationalExcellenceOrchestrator:
    """Main orchestrator for operational excellence framework."""
    
    def __init__(self):
        self.backup_system = None
        self.log_aggregation = LogAggregationSystem()
        self.resource_optimization = ResourceOptimizationSystem()
        
        self.is_running = False
        
    async def initialize(self, backup_config: BackupConfiguration = None):
        """Initialize operational excellence systems."""
        logger.info("ðŸš€ Initializing Operational Excellence Framework")
        
        # Initialize backup system with configuration
        if backup_config:
            self.backup_system = AutomatedBackupSystem(backup_config)
            await self.backup_system.initialize()
        
        # Initialize other systems
        await self.log_aggregation.initialize()
        await self.resource_optimization.initialize()
        
        logger.info("âœ… Operational Excellence Framework initialized")
    
    async def start(self):
        """Start all operational excellence services."""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info("Starting operational excellence services")
        
        # Start backup system
        if self.backup_system:
            await self.backup_system.start()
        
        # Start log aggregation
        await self.log_aggregation.start()
        
        # Start resource optimization
        await self.resource_optimization.start()
        
        logger.info("âœ… All operational excellence services started")
    
    async def stop(self):
        """Stop all operational excellence services."""
        if not self.is_running:
            return
        
        self.is_running = False
        logger.info("Stopping operational excellence services")
        
        # Stop all systems
        if self.backup_system:
            await self.backup_system.stop()
        
        await self.log_aggregation.stop()
        await self.resource_optimization.stop()
        
        logger.info("âœ… All operational excellence services stopped")
    
    async def get_operational_status(self) -> Dict[str, Any]:
        """Get comprehensive operational status."""
        status = {
            'timestamp': datetime.utcnow().isoformat(),
            'backup_system': None,
            'log_aggregation': {
                'status': 'running' if self.log_aggregation.is_running else 'stopped',
                'elasticsearch_available': self.log_aggregation.elasticsearch_client is not None
            },
            'resource_optimization': {
                'status': 'running' if self.resource_optimization.is_running else 'stopped',
                'metrics_count': len(self.resource_optimization.metrics_history),
                'recommendations_count': len(self.resource_optimization.optimization_recommendations)
            },
            'overall_health': 'healthy'
        }
        
        if self.backup_system:
            recent_backups = [
                b for b in self.backup_system.backup_history
                if b.start_time > datetime.utcnow() - timedelta(days=7)
            ]
            
            status['backup_system'] = {
                'status': 'running' if self.backup_system.is_running else 'stopped',
                'recent_backups': len(recent_backups),
                'last_successful_backup': None
            }
            
            successful_backups = [b for b in recent_backups if b.status == BackupStatus.SUCCESS]
            if successful_backups:
                status['backup_system']['last_successful_backup'] = max(
                    successful_backups, key=lambda x: x.start_time
                ).start_time.isoformat()
        
        return status

# Global operational excellence instance
_operational_excellence: Optional[OperationalExcellenceOrchestrator] = None

async def get_operational_excellence() -> OperationalExcellenceOrchestrator:
    """Get the global operational excellence orchestrator."""
    global _operational_excellence
    
    if _operational_excellence is None:
        _operational_excellence = OperationalExcellenceOrchestrator()
    
    return _operational_excellence

async def start_operational_excellence(backup_config: BackupConfiguration = None):
    """Start the operational excellence framework."""
    ops = await get_operational_excellence()
    await ops.initialize(backup_config)
    await ops.start()

async def stop_operational_excellence():
    """Stop the operational excellence framework."""
    global _operational_excellence
    
    if _operational_excellence:
        await _operational_excellence.stop()
        _operational_excellence = None