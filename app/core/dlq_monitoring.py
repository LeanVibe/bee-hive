"""
DLQ Monitoring System for LeanVibe Agent Hive 2.0 - VS 4.3

Comprehensive observability integration for Dead Letter Queue operations.
Provides real-time monitoring, alerting, and performance tracking for DLQ systems.

Features:
- Real-time DLQ size and performance monitoring
- Intelligent alerting with threshold management
- Integration with observability hooks and metrics
- Performance degradation detection
- Automated alert escalation and notification
- Comprehensive dashboard metrics
"""

import asyncio
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Set, Union
from dataclasses import dataclass, field
from enum import Enum
import structlog

import redis.asyncio as redis
from redis.asyncio import Redis

from .config import settings
from .observability_hooks import get_observability_hooks
from .error_handling_integration import get_error_handling_integration

logger = structlog.get_logger()


class AlertSeverity(str, Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class AlertType(str, Enum):
    """Types of alerts that can be generated."""
    DLQ_SIZE_THRESHOLD = "dlq_size_threshold"
    HIGH_FAILURE_RATE = "high_failure_rate"
    PROCESSING_TIMEOUT = "processing_timeout"
    POISON_MESSAGE_SPIKE = "poison_message_spike"
    SYSTEM_DEGRADATION = "system_degradation"
    RECOVERY_FAILURE = "recovery_failure"
    CIRCUIT_BREAKER_OPEN = "circuit_breaker_open"
    RETRY_EXHAUSTION = "retry_exhaustion"


@dataclass
class AlertThreshold:
    """Threshold configuration for alerts."""
    
    alert_type: AlertType
    threshold_value: Union[int, float]
    severity: AlertSeverity
    evaluation_window_minutes: int = 5
    cooldown_minutes: int = 15  # Minimum time between alerts of same type
    
    # Advanced threshold settings
    trigger_count: int = 1  # How many times threshold must be exceeded
    percentage_threshold: bool = False  # Whether threshold is a percentage
    comparison_operator: str = "greater_than"  # greater_than, less_than, equals
    
    def evaluate(self, current_value: Union[int, float], historical_values: List[float] = None) -> bool:
        """Evaluate if threshold is exceeded."""
        
        if self.comparison_operator == "greater_than":
            return current_value > self.threshold_value
        elif self.comparison_operator == "less_than":
            return current_value < self.threshold_value
        elif self.comparison_operator == "equals":
            return current_value == self.threshold_value
        else:
            return False


@dataclass
class Alert:
    """Alert generated by the monitoring system."""
    
    alert_id: str
    alert_type: AlertType
    severity: AlertSeverity
    title: str
    message: str
    current_value: Union[int, float]
    threshold_value: Union[int, float]
    
    # Metadata
    created_at: datetime = field(default_factory=datetime.utcnow)
    resolved_at: Optional[datetime] = None
    acknowledged_at: Optional[datetime] = None
    acknowledged_by: Optional[str] = None
    
    # Context
    affected_components: List[str] = field(default_factory=list)
    related_metrics: Dict[str, Any] = field(default_factory=dict)
    remediation_suggestions: List[str] = field(default_factory=list)
    
    @property
    def is_active(self) -> bool:
        """Check if alert is still active."""
        return self.resolved_at is None
    
    @property
    def duration_minutes(self) -> float:
        """Get alert duration in minutes."""
        end_time = self.resolved_at or datetime.utcnow()
        return (end_time - self.created_at).total_seconds() / 60
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert alert to dictionary for serialization."""
        return {
            "alert_id": self.alert_id,
            "alert_type": self.alert_type.value,
            "severity": self.severity.value,
            "title": self.title,
            "message": self.message,
            "current_value": self.current_value,
            "threshold_value": self.threshold_value,
            "created_at": self.created_at.isoformat(),
            "resolved_at": self.resolved_at.isoformat() if self.resolved_at else None,
            "acknowledged_at": self.acknowledged_at.isoformat() if self.acknowledged_at else None,
            "acknowledged_by": self.acknowledged_by,
            "is_active": self.is_active,
            "duration_minutes": self.duration_minutes,
            "affected_components": self.affected_components,
            "related_metrics": self.related_metrics,
            "remediation_suggestions": self.remediation_suggestions
        }


@dataclass
class MonitoringMetrics:
    """Metrics tracked by the monitoring system."""
    
    # DLQ metrics
    dlq_size: int = 0
    retry_queue_size: int = 0
    processing_queue_size: int = 0
    
    # Performance metrics
    messages_processed_per_minute: float = 0.0
    average_processing_time_ms: float = 0.0
    success_rate: float = 0.0
    
    # Poison detection metrics
    poison_messages_detected_per_minute: float = 0.0
    poison_detection_accuracy: float = 0.0
    
    # Alert metrics
    active_alerts_count: int = 0
    total_alerts_generated: int = 0
    alerts_resolved_per_hour: float = 0.0
    
    # System health metrics
    circuit_breakers_open: int = 0
    system_degradation_score: float = 0.0  # 0.0 = healthy, 1.0 = severely degraded
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary."""
        return {
            "dlq_metrics": {
                "dlq_size": self.dlq_size,
                "retry_queue_size": self.retry_queue_size,
                "processing_queue_size": self.processing_queue_size
            },
            "performance_metrics": {
                "messages_processed_per_minute": self.messages_processed_per_minute,
                "average_processing_time_ms": self.average_processing_time_ms,
                "success_rate": self.success_rate
            },
            "poison_detection_metrics": {
                "poison_messages_detected_per_minute": self.poison_messages_detected_per_minute,
                "poison_detection_accuracy": self.poison_detection_accuracy
            },
            "alert_metrics": {
                "active_alerts_count": self.active_alerts_count,
                "total_alerts_generated": self.total_alerts_generated,
                "alerts_resolved_per_hour": self.alerts_resolved_per_hour
            },
            "system_health_metrics": {
                "circuit_breakers_open": self.circuit_breakers_open,
                "system_degradation_score": self.system_degradation_score
            }
        }


class DLQMonitor:
    """
    Comprehensive DLQ monitoring system with intelligent alerting.
    
    Features:
    - Real-time metrics collection and analysis
    - Configurable alert thresholds and escalation
    - Integration with observability and error handling systems
    - Performance trend analysis and degradation detection
    - Automated remediation suggestions
    - Dashboard-ready metrics and visualizations
    """
    
    def __init__(
        self,
        redis_client: Redis,
        monitoring_interval_seconds: int = 30,
        enable_alerting: bool = True,
        enable_trend_analysis: bool = True
    ):
        """Initialize DLQ monitoring system."""
        self.redis = redis_client
        self.monitoring_interval_seconds = monitoring_interval_seconds
        self.enable_alerting = enable_alerting
        self.enable_trend_analysis = enable_trend_analysis
        
        # External integrations
        self.observability_hooks = get_observability_hooks()
        self.error_integration = get_error_handling_integration()
        
        # Monitoring state
        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None
        self._alert_processor_task: Optional[asyncio.Task] = None
        
        # Metrics and alerts
        self.current_metrics = MonitoringMetrics()
        self._metrics_history: List[Tuple[datetime, MonitoringMetrics]] = []
        self._active_alerts: Dict[str, Alert] = {}
        self._alert_history: List[Alert] = []
        self._last_alert_times: Dict[AlertType, datetime] = {}
        
        # Alert thresholds
        self.alert_thresholds = self._initialize_alert_thresholds()
        
        # Performance tracking
        self._monitoring_metrics = {
            "monitoring_cycles_completed": 0,
            "alerts_generated": 0,
            "alerts_resolved": 0,
            "average_monitoring_time_ms": 0.0,
            "last_monitoring_time": None
        }
        
        logger.info(
            "ðŸ“Š DLQ Monitor initialized",
            monitoring_interval_seconds=monitoring_interval_seconds,
            enable_alerting=enable_alerting,
            enable_trend_analysis=enable_trend_analysis,
            alert_thresholds_count=len(self.alert_thresholds)
        )
    
    def _initialize_alert_thresholds(self) -> List[AlertThreshold]:
        """Initialize default alert thresholds."""
        return [
            # DLQ size thresholds
            AlertThreshold(
                alert_type=AlertType.DLQ_SIZE_THRESHOLD,
                threshold_value=1000,
                severity=AlertSeverity.WARNING,
                evaluation_window_minutes=5,
                cooldown_minutes=10
            ),
            AlertThreshold(
                alert_type=AlertType.DLQ_SIZE_THRESHOLD,
                threshold_value=5000,
                severity=AlertSeverity.ERROR,
                evaluation_window_minutes=3,
                cooldown_minutes=15
            ),
            AlertThreshold(
                alert_type=AlertType.DLQ_SIZE_THRESHOLD,
                threshold_value=10000,
                severity=AlertSeverity.CRITICAL,
                evaluation_window_minutes=1,
                cooldown_minutes=30
            ),
            
            # Failure rate thresholds
            AlertThreshold(
                alert_type=AlertType.HIGH_FAILURE_RATE,
                threshold_value=0.2,  # 20% failure rate
                severity=AlertSeverity.WARNING,
                evaluation_window_minutes=10,
                percentage_threshold=True
            ),
            AlertThreshold(
                alert_type=AlertType.HIGH_FAILURE_RATE,
                threshold_value=0.5,  # 50% failure rate
                severity=AlertSeverity.ERROR,
                evaluation_window_minutes=5,
                percentage_threshold=True
            ),
            
            # Processing timeout thresholds
            AlertThreshold(
                alert_type=AlertType.PROCESSING_TIMEOUT,
                threshold_value=5000,  # 5 seconds
                severity=AlertSeverity.WARNING,
                evaluation_window_minutes=5
            ),
            AlertThreshold(
                alert_type=AlertType.PROCESSING_TIMEOUT,
                threshold_value=10000,  # 10 seconds
                severity=AlertSeverity.ERROR,
                evaluation_window_minutes=3
            ),
            
            # Poison message spike threshold
            AlertThreshold(
                alert_type=AlertType.POISON_MESSAGE_SPIKE,
                threshold_value=50,  # 50 poison messages per minute
                severity=AlertSeverity.WARNING,
                evaluation_window_minutes=1
            ),
            
            # System degradation threshold
            AlertThreshold(
                alert_type=AlertType.SYSTEM_DEGRADATION,
                threshold_value=0.7,  # 70% degradation score
                severity=AlertSeverity.ERROR,
                evaluation_window_minutes=5
            )
        ]
    
    async def start(self) -> None:
        """Start the DLQ monitoring system."""
        if self._running:
            logger.warning("DLQ Monitor already running")
            return
        
        self._running = True
        
        # Start main monitoring loop
        self._monitor_task = asyncio.create_task(self._monitoring_loop())
        
        # Start alert processor if alerting is enabled
        if self.enable_alerting:
            self._alert_processor_task = asyncio.create_task(self._alert_processing_loop())
        
        logger.info("âœ… DLQ Monitor started")
    
    async def stop(self) -> None:
        """Stop the DLQ monitoring system."""
        if not self._running:
            return
        
        self._running = False
        
        # Cancel tasks
        tasks = [self._monitor_task, self._alert_processor_task]
        for task in tasks:
            if task and not task.done():
                task.cancel()
        
        # Wait for tasks to complete
        active_tasks = [t for t in tasks if t and not t.done()]
        if active_tasks:
            await asyncio.gather(*active_tasks, return_exceptions=True)
        
        logger.info("ðŸ›‘ DLQ Monitor stopped")
    
    async def _monitoring_loop(self) -> None:
        """Main monitoring loop for collecting metrics and detecting issues."""
        while self._running:
            try:
                start_time = time.time()
                
                # Collect current metrics
                await self._collect_metrics()
                
                # Store metrics history for trend analysis
                if self.enable_trend_analysis:
                    await self._store_metrics_history()
                
                # Evaluate alert conditions
                if self.enable_alerting:
                    await self._evaluate_alert_conditions()
                
                # Emit metrics to observability system
                await self._emit_observability_metrics()
                
                # Update monitoring performance metrics
                monitoring_time = (time.time() - start_time) * 1000
                await self._update_monitoring_metrics(monitoring_time)
                
                # Wait for next monitoring cycle
                await asyncio.sleep(self.monitoring_interval_seconds)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                await asyncio.sleep(self.monitoring_interval_seconds)
    
    async def _alert_processing_loop(self) -> None:
        """Process and manage alerts."""
        while self._running:
            try:
                # Check for alert resolutions
                await self._check_alert_resolutions()
                
                # Process alert escalations
                await self._process_alert_escalations()
                
                # Clean up old resolved alerts
                await self._cleanup_old_alerts()
                
                # Wait before next alert processing cycle
                await asyncio.sleep(60)  # Process alerts every minute
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in alert processing loop: {e}")
                await asyncio.sleep(60)
    
    async def _collect_metrics(self) -> None:
        """Collect current DLQ metrics from all sources."""
        try:
            # Reset current metrics
            self.current_metrics = MonitoringMetrics()
            
            # Collect DLQ size metrics
            await self._collect_dlq_size_metrics()
            
            # Collect performance metrics
            await self._collect_performance_metrics()
            
            # Collect poison detection metrics
            await self._collect_poison_detection_metrics()
            
            # Collect alert metrics
            await self._collect_alert_metrics()
            
            # Collect system health metrics
            await self._collect_system_health_metrics()
            
            logger.debug(
                "ðŸ“Š Metrics collected",
                dlq_size=self.current_metrics.dlq_size,
                success_rate=self.current_metrics.success_rate,
                active_alerts=self.current_metrics.active_alerts_count
            )
            
        except Exception as e:
            logger.error(f"Error collecting metrics: {e}")
    
    async def _collect_dlq_size_metrics(self) -> None:
        """Collect DLQ size-related metrics."""
        try:
            # Get DLQ stream sizes
            dlq_streams = await self.redis.keys("*:dlq")
            total_dlq_size = 0
            
            for stream in dlq_streams:
                stream_size = await self.redis.xlen(stream)
                total_dlq_size += stream_size
            
            self.current_metrics.dlq_size = total_dlq_size
            
            # Get retry queue sizes
            retry_queues = await self.redis.keys("dlq:retry_queue:*")
            total_retry_size = 0
            
            for queue in retry_queues:
                queue_size = await self.redis.zcard(queue)
                total_retry_size += queue_size
            
            self.current_metrics.retry_queue_size = total_retry_size
            
            # Get processing queue sizes
            processing_queues = await self.redis.keys("dlq:processing_queue:*")
            total_processing_size = 0
            
            for queue in processing_queues:
                queue_size = await self.redis.llen(queue)
                total_processing_size += queue_size
            
            self.current_metrics.processing_queue_size = total_processing_size
            
        except Exception as e:
            logger.error(f"Error collecting DLQ size metrics: {e}")
    
    async def _collect_performance_metrics(self) -> None:
        """Collect performance-related metrics."""
        try:
            # Get processing metrics from Redis (would be stored by DLQ components)
            metrics_key = "dlq:performance_metrics"
            
            # Get messages processed per minute
            processed_count = await self.redis.get(f"{metrics_key}:processed_count")
            if processed_count:
                self.current_metrics.messages_processed_per_minute = float(processed_count)
            
            # Get average processing time
            avg_time = await self.redis.get(f"{metrics_key}:avg_processing_time_ms")
            if avg_time:
                self.current_metrics.average_processing_time_ms = float(avg_time)
            
            # Calculate success rate from success/failure counts
            success_count = await self.redis.get(f"{metrics_key}:success_count") or "0"
            failure_count = await self.redis.get(f"{metrics_key}:failure_count") or "0"
            
            total_attempts = int(success_count) + int(failure_count)
            if total_attempts > 0:
                self.current_metrics.success_rate = int(success_count) / total_attempts
            
        except Exception as e:
            logger.error(f"Error collecting performance metrics: {e}")
    
    async def _collect_poison_detection_metrics(self) -> None:
        """Collect poison detection metrics."""
        try:
            # Get poison detection metrics from Redis
            poison_key = "dlq:poison_metrics"
            
            # Get poison messages detected per minute
            poison_count = await self.redis.get(f"{poison_key}:detected_count")
            if poison_count:
                self.current_metrics.poison_messages_detected_per_minute = float(poison_count)
            
            # Get detection accuracy
            accuracy = await self.redis.get(f"{poison_key}:accuracy")
            if accuracy:
                self.current_metrics.poison_detection_accuracy = float(accuracy)
                
        except Exception as e:
            logger.error(f"Error collecting poison detection metrics: {e}")
    
    async def _collect_alert_metrics(self) -> None:
        """Collect alert-related metrics."""
        try:
            self.current_metrics.active_alerts_count = len(self._active_alerts)
            self.current_metrics.total_alerts_generated = len(self._alert_history)
            
            # Calculate alerts resolved per hour
            one_hour_ago = datetime.utcnow() - timedelta(hours=1)
            resolved_in_hour = sum(
                1 for alert in self._alert_history 
                if alert.resolved_at and alert.resolved_at > one_hour_ago
            )
            self.current_metrics.alerts_resolved_per_hour = float(resolved_in_hour)
            
        except Exception as e:
            logger.error(f"Error collecting alert metrics: {e}")
    
    async def _collect_system_health_metrics(self) -> None:
        """Collect system health metrics."""
        try:
            # Get circuit breaker states from Redis
            cb_states = await self.redis.keys("circuit_breaker:*:state")
            open_cb_count = 0
            
            for cb_key in cb_states:
                state = await self.redis.get(cb_key)
                if state == "open":
                    open_cb_count += 1
            
            self.current_metrics.circuit_breakers_open = open_cb_count
            
            # Calculate system degradation score based on multiple factors
            degradation_factors = []
            
            # Factor 1: DLQ size relative to threshold
            if self.current_metrics.dlq_size > 0:
                dlq_factor = min(1.0, self.current_metrics.dlq_size / 10000)
                degradation_factors.append(dlq_factor * 0.3)  # 30% weight
            
            # Factor 2: Success rate (inverted)
            success_factor = 1.0 - self.current_metrics.success_rate
            degradation_factors.append(success_factor * 0.4)  # 40% weight
            
            # Factor 3: Circuit breakers
            cb_factor = min(1.0, open_cb_count / 10)  # Assume 10 CBs is max expected
            degradation_factors.append(cb_factor * 0.2)  # 20% weight
            
            # Factor 4: Processing time
            if self.current_metrics.average_processing_time_ms > 0:
                time_factor = min(1.0, self.current_metrics.average_processing_time_ms / 10000)  # 10s max
                degradation_factors.append(time_factor * 0.1)  # 10% weight
            
            self.current_metrics.system_degradation_score = sum(degradation_factors)
            
        except Exception as e:
            logger.error(f"Error collecting system health metrics: {e}")
    
    async def _store_metrics_history(self) -> None:
        """Store metrics history for trend analysis."""
        try:
            # Add current metrics to history
            self._metrics_history.append((datetime.utcnow(), self.current_metrics))
            
            # Keep only last 24 hours of history (assuming 30-second intervals)
            max_history_entries = 24 * 60 * 2  # 2880 entries for 24 hours
            if len(self._metrics_history) > max_history_entries:
                self._metrics_history = self._metrics_history[-max_history_entries:]
            
        except Exception as e:
            logger.error(f"Error storing metrics history: {e}")
    
    async def _evaluate_alert_conditions(self) -> None:
        """Evaluate alert conditions based on current metrics."""
        try:
            for threshold in self.alert_thresholds:
                # Check cooldown period
                if not self._is_alert_cooldown_expired(threshold.alert_type, threshold.cooldown_minutes):
                    continue
                
                # Get current value for evaluation
                current_value = self._get_metric_value_for_alert_type(threshold.alert_type)
                if current_value is None:
                    continue
                
                # Evaluate threshold
                if threshold.evaluate(current_value):
                    await self._generate_alert(threshold, current_value)
            
        except Exception as e:
            logger.error(f"Error evaluating alert conditions: {e}")
    
    def _is_alert_cooldown_expired(self, alert_type: AlertType, cooldown_minutes: int) -> bool:
        """Check if alert cooldown period has expired."""
        if alert_type not in self._last_alert_times:
            return True
        
        last_alert_time = self._last_alert_times[alert_type]
        cooldown_expired = datetime.utcnow() - last_alert_time > timedelta(minutes=cooldown_minutes)
        
        return cooldown_expired
    
    def _get_metric_value_for_alert_type(self, alert_type: AlertType) -> Optional[Union[int, float]]:
        """Get current metric value for specific alert type."""
        
        if alert_type == AlertType.DLQ_SIZE_THRESHOLD:
            return self.current_metrics.dlq_size
        elif alert_type == AlertType.HIGH_FAILURE_RATE:
            return 1.0 - self.current_metrics.success_rate
        elif alert_type == AlertType.PROCESSING_TIMEOUT:
            return self.current_metrics.average_processing_time_ms
        elif alert_type == AlertType.POISON_MESSAGE_SPIKE:
            return self.current_metrics.poison_messages_detected_per_minute
        elif alert_type == AlertType.SYSTEM_DEGRADATION:
            return self.current_metrics.system_degradation_score
        else:
            return None
    
    async def _generate_alert(self, threshold: AlertThreshold, current_value: Union[int, float]) -> None:
        """Generate a new alert."""
        try:
            alert_id = f"alert_{threshold.alert_type.value}_{int(time.time())}"
            
            # Create alert
            alert = Alert(
                alert_id=alert_id,
                alert_type=threshold.alert_type,
                severity=threshold.severity,
                title=self._get_alert_title(threshold.alert_type, threshold.severity),
                message=self._get_alert_message(threshold.alert_type, current_value, threshold.threshold_value),
                current_value=current_value,
                threshold_value=threshold.threshold_value,
                affected_components=self._get_affected_components(threshold.alert_type),
                related_metrics=self.current_metrics.to_dict(),
                remediation_suggestions=self._get_remediation_suggestions(threshold.alert_type)
            )
            
            # Store alert
            self._active_alerts[alert_id] = alert
            self._alert_history.append(alert)
            self._last_alert_times[threshold.alert_type] = datetime.utcnow()
            
            # Emit alert to observability system
            await self._emit_alert_to_observability(alert)
            
            # Update metrics
            self._monitoring_metrics["alerts_generated"] += 1
            
            logger.warning(
                f"ðŸš¨ Alert generated",
                alert_id=alert_id,
                alert_type=threshold.alert_type.value,
                severity=threshold.severity.value,
                current_value=current_value,
                threshold_value=threshold.threshold_value
            )
            
        except Exception as e:
            logger.error(f"Error generating alert: {e}")
    
    def _get_alert_title(self, alert_type: AlertType, severity: AlertSeverity) -> str:
        """Get alert title based on type and severity."""
        
        titles = {
            AlertType.DLQ_SIZE_THRESHOLD: "DLQ Size Threshold Exceeded",
            AlertType.HIGH_FAILURE_RATE: "High Message Failure Rate",
            AlertType.PROCESSING_TIMEOUT: "Message Processing Timeout",
            AlertType.POISON_MESSAGE_SPIKE: "Poison Message Spike Detected",
            AlertType.SYSTEM_DEGRADATION: "System Performance Degradation",
            AlertType.RECOVERY_FAILURE: "Message Recovery Failure",
            AlertType.CIRCUIT_BREAKER_OPEN: "Circuit Breaker Opened",
            AlertType.RETRY_EXHAUSTION: "Retry Attempts Exhausted"
        }
        
        base_title = titles.get(alert_type, "DLQ System Alert")
        return f"[{severity.value.upper()}] {base_title}"
    
    def _get_alert_message(
        self,
        alert_type: AlertType,
        current_value: Union[int, float],
        threshold_value: Union[int, float]
    ) -> str:
        """Get alert message based on type and values."""
        
        if alert_type == AlertType.DLQ_SIZE_THRESHOLD:
            return f"DLQ size ({current_value}) has exceeded threshold ({threshold_value}). Immediate attention required."
        elif alert_type == AlertType.HIGH_FAILURE_RATE:
            return f"Message failure rate ({current_value:.2%}) exceeds acceptable threshold ({threshold_value:.2%})."
        elif alert_type == AlertType.PROCESSING_TIMEOUT:
            return f"Average processing time ({current_value:.0f}ms) exceeds timeout threshold ({threshold_value:.0f}ms)."
        elif alert_type == AlertType.POISON_MESSAGE_SPIKE:
            return f"Poison message detection rate ({current_value:.1f}/min) indicates potential attack or system issue."
        elif alert_type == AlertType.SYSTEM_DEGRADATION:
            return f"System degradation score ({current_value:.2f}) indicates performance issues requiring investigation."
        else:
            return f"Alert condition detected: current value ({current_value}) exceeds threshold ({threshold_value})."
    
    def _get_affected_components(self, alert_type: AlertType) -> List[str]:
        """Get list of affected components for alert type."""
        
        component_map = {
            AlertType.DLQ_SIZE_THRESHOLD: ["dlq_manager", "message_processing"],
            AlertType.HIGH_FAILURE_RATE: ["message_handlers", "dlq_retry_scheduler"],
            AlertType.PROCESSING_TIMEOUT: ["message_processors", "redis_streams"],
            AlertType.POISON_MESSAGE_SPIKE: ["poison_detector", "message_validation"],
            AlertType.SYSTEM_DEGRADATION: ["dlq_system", "redis_infrastructure"],
            AlertType.RECOVERY_FAILURE: ["dlq_retry_scheduler", "message_replay"],
            AlertType.CIRCUIT_BREAKER_OPEN: ["circuit_breakers", "external_services"],
            AlertType.RETRY_EXHAUSTION: ["retry_policies", "dlq_manager"]
        }
        
        return component_map.get(alert_type, ["dlq_system"])
    
    def _get_remediation_suggestions(self, alert_type: AlertType) -> List[str]:
        """Get remediation suggestions for alert type."""
        
        suggestion_map = {
            AlertType.DLQ_SIZE_THRESHOLD: [
                "Review and replay recoverable messages",
                "Investigate root cause of message failures",
                "Consider increasing DLQ processing capacity",
                "Check for poison messages requiring quarantine"
            ],
            AlertType.HIGH_FAILURE_RATE: [
                "Analyze failure patterns for systemic issues",
                "Review message validation logic",
                "Check external service dependencies",
                "Consider adjusting retry policies"
            ],
            AlertType.PROCESSING_TIMEOUT: [
                "Investigate slow message handlers",
                "Check Redis connection performance",
                "Review system resource utilization",
                "Consider increasing timeout thresholds"
            ],
            AlertType.POISON_MESSAGE_SPIKE: [
                "Review poison message patterns",
                "Check message sources for issues",
                "Consider implementing additional filtering",
                "Investigate potential security incidents"
            ],
            AlertType.SYSTEM_DEGRADATION: [
                "Review system resource usage",
                "Check for performance bottlenecks",
                "Consider scaling DLQ processing capacity",
                "Investigate external dependencies"
            ]
        }
        
        return suggestion_map.get(alert_type, ["Investigate alert condition and take appropriate action"])
    
    async def _emit_alert_to_observability(self, alert: Alert) -> None:
        """Emit alert to observability system."""
        try:
            if self.observability_hooks:
                await self.observability_hooks.failure_detected(
                    failure_type=f"dlq_alert_{alert.alert_type.value}",
                    failure_description=alert.message,
                    affected_component=",".join(alert.affected_components),
                    severity=alert.severity.value,
                    error_details={
                        "alert_id": alert.alert_id,
                        "current_value": alert.current_value,
                        "threshold_value": alert.threshold_value,
                        "related_metrics": alert.related_metrics
                    },
                    detection_method="dlq_monitoring_system",
                    impact_assessment={
                        "dlq_system_affected": True,
                        "remediation_required": True,
                        "estimated_impact_severity": alert.severity.value
                    }
                )
            
        except Exception as e:
            logger.error(f"Error emitting alert to observability: {e}")
    
    async def _check_alert_resolutions(self) -> None:
        """Check if any active alerts can be resolved."""
        try:
            resolved_alerts = []
            
            for alert_id, alert in self._active_alerts.items():
                if await self._is_alert_resolved(alert):
                    alert.resolved_at = datetime.utcnow()
                    resolved_alerts.append(alert_id)
                    
                    # Emit resolution to observability
                    await self._emit_alert_resolution(alert)
                    
                    logger.info(
                        f"âœ… Alert resolved",
                        alert_id=alert_id,
                        alert_type=alert.alert_type.value,
                        duration_minutes=alert.duration_minutes
                    )
            
            # Remove resolved alerts from active list
            for alert_id in resolved_alerts:
                del self._active_alerts[alert_id]
                self._monitoring_metrics["alerts_resolved"] += 1
            
        except Exception as e:
            logger.error(f"Error checking alert resolutions: {e}")
    
    async def _is_alert_resolved(self, alert: Alert) -> bool:
        """Check if an alert condition is resolved."""
        try:
            current_value = self._get_metric_value_for_alert_type(alert.alert_type)
            if current_value is None:
                return False
            
            # For most alerts, resolution means current value is below threshold
            # Add some hysteresis to prevent flapping
            resolution_threshold = alert.threshold_value * 0.9  # 10% below threshold
            
            return current_value < resolution_threshold
            
        except Exception as e:
            logger.error(f"Error checking alert resolution: {e}")
            return False
    
    async def _emit_alert_resolution(self, alert: Alert) -> None:
        """Emit alert resolution to observability system."""
        try:
            if self.observability_hooks:
                await self.observability_hooks.recovery_initiated(
                    recovery_strategy="alert_condition_resolved",
                    trigger_failure=f"dlq_alert_{alert.alert_type.value}",
                    recovery_steps=["condition_monitoring", "threshold_check", "alert_resolution"],
                    estimated_recovery_time_ms=alert.duration_minutes * 60 * 1000,
                    rollback_checkpoint=f"alert_resolved_{alert.alert_id}"
                )
            
        except Exception as e:
            logger.error(f"Error emitting alert resolution: {e}")
    
    async def _process_alert_escalations(self) -> None:
        """Process alert escalations for long-running alerts."""
        try:
            escalation_threshold_minutes = 60  # Escalate after 1 hour
            
            for alert in self._active_alerts.values():
                if (alert.duration_minutes > escalation_threshold_minutes and 
                    alert.severity != AlertSeverity.CRITICAL):
                    
                    # Escalate alert severity
                    old_severity = alert.severity
                    alert.severity = AlertSeverity.CRITICAL
                    
                    logger.warning(
                        f"ðŸš¨ Alert escalated",
                        alert_id=alert.alert_id,
                        old_severity=old_severity.value,
                        new_severity=alert.severity.value,
                        duration_minutes=alert.duration_minutes
                    )
                    
                    # Re-emit escalated alert
                    await self._emit_alert_to_observability(alert)
            
        except Exception as e:
            logger.error(f"Error processing alert escalations: {e}")
    
    async def _cleanup_old_alerts(self) -> None:
        """Clean up old resolved alerts to prevent memory growth."""
        try:
            cutoff_time = datetime.utcnow() - timedelta(days=7)  # Keep 7 days of history
            
            original_count = len(self._alert_history)
            self._alert_history = [
                alert for alert in self._alert_history
                if alert.created_at > cutoff_time or alert.is_active
            ]
            
            cleaned_count = original_count - len(self._alert_history)
            if cleaned_count > 0:
                logger.info(f"ðŸ§¹ Cleaned up {cleaned_count} old alerts")
            
        except Exception as e:
            logger.error(f"Error cleaning up old alerts: {e}")
    
    async def _emit_observability_metrics(self) -> None:
        """Emit current metrics to observability system."""
        try:
            if self.observability_hooks:
                # Emit DLQ size metrics
                await self.observability_hooks.metric_recorded(
                    metric_name="dlq_size",
                    metric_value=float(self.current_metrics.dlq_size),
                    metric_type="gauge",
                    labels={"component": "dlq_monitor"}
                )
                
                # Emit success rate metric
                await self.observability_hooks.metric_recorded(
                    metric_name="dlq_success_rate",
                    metric_value=self.current_metrics.success_rate,
                    metric_type="gauge",
                    labels={"component": "dlq_monitor"}
                )
                
                # Emit system degradation score
                await self.observability_hooks.metric_recorded(
                    metric_name="dlq_system_degradation_score",
                    metric_value=self.current_metrics.system_degradation_score,
                    metric_type="gauge",
                    labels={"component": "dlq_monitor"}
                )
            
        except Exception as e:
            logger.error(f"Error emitting observability metrics: {e}")
    
    async def _update_monitoring_metrics(self, monitoring_time_ms: float) -> None:
        """Update monitoring performance metrics."""
        try:
            self._monitoring_metrics["monitoring_cycles_completed"] += 1
            
            # Update average monitoring time
            cycles = self._monitoring_metrics["monitoring_cycles_completed"]
            current_avg = self._monitoring_metrics["average_monitoring_time_ms"]
            self._monitoring_metrics["average_monitoring_time_ms"] = (
                (current_avg * (cycles - 1) + monitoring_time_ms) / cycles
            )
            
            self._monitoring_metrics["last_monitoring_time"] = datetime.utcnow().isoformat()
            
        except Exception as e:
            logger.error(f"Error updating monitoring metrics: {e}")
    
    async def get_monitoring_status(self) -> Dict[str, Any]:
        """Get comprehensive monitoring status and metrics."""
        
        return {
            "monitoring_status": {
                "running": self._running,
                "monitoring_interval_seconds": self.monitoring_interval_seconds,
                "alerting_enabled": self.enable_alerting,
                "trend_analysis_enabled": self.enable_trend_analysis
            },
            "current_metrics": self.current_metrics.to_dict(),
            "active_alerts": {
                alert_id: alert.to_dict()
                for alert_id, alert in self._active_alerts.items()
            },
            "alert_summary": {
                "active_count": len(self._active_alerts),
                "total_generated": len(self._alert_history),
                "alerts_by_severity": self._get_alerts_by_severity(),
                "alerts_by_type": self._get_alerts_by_type()
            },
            "monitoring_performance": self._monitoring_metrics.copy(),
            "system_health": {
                "overall_health": self._calculate_overall_health(),
                "degradation_score": self.current_metrics.system_degradation_score,
                "critical_issues": self._get_critical_issues()
            },
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def _get_alerts_by_severity(self) -> Dict[str, int]:
        """Get alert count by severity."""
        severity_counts = {severity.value: 0 for severity in AlertSeverity}
        
        for alert in self._active_alerts.values():
            severity_counts[alert.severity.value] += 1
        
        return severity_counts
    
    def _get_alerts_by_type(self) -> Dict[str, int]:
        """Get alert count by type."""
        type_counts = {alert_type.value: 0 for alert_type in AlertType}
        
        for alert in self._active_alerts.values():
            type_counts[alert.alert_type.value] += 1
        
        return type_counts
    
    def _calculate_overall_health(self) -> str:
        """Calculate overall system health status."""
        
        if len(self._active_alerts) == 0:
            return "healthy"
        
        # Check for critical alerts
        critical_alerts = [
            alert for alert in self._active_alerts.values()
            if alert.severity == AlertSeverity.CRITICAL
        ]
        
        if critical_alerts:
            return "critical"
        
        # Check for error alerts
        error_alerts = [
            alert for alert in self._active_alerts.values()
            if alert.severity == AlertSeverity.ERROR
        ]
        
        if error_alerts:
            return "degraded"
        
        # Only warning alerts
        return "warning"
    
    def _get_critical_issues(self) -> List[str]:
        """Get list of critical issues requiring immediate attention."""
        
        critical_issues = []
        
        # Check for critical alerts
        for alert in self._active_alerts.values():
            if alert.severity == AlertSeverity.CRITICAL:
                critical_issues.append(f"{alert.title}: {alert.message}")
        
        # Check system degradation
        if self.current_metrics.system_degradation_score > 0.8:
            critical_issues.append("System degradation score critically high")
        
        # Check DLQ size
        if self.current_metrics.dlq_size > 10000:
            critical_issues.append(f"DLQ size critically high: {self.current_metrics.dlq_size}")
        
        return critical_issues
    
    async def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
        """Acknowledge an active alert."""
        try:
            if alert_id in self._active_alerts:
                alert = self._active_alerts[alert_id]
                alert.acknowledged_at = datetime.utcnow()
                alert.acknowledged_by = acknowledged_by
                
                logger.info(
                    f"âœ… Alert acknowledged",
                    alert_id=alert_id,
                    acknowledged_by=acknowledged_by,
                    alert_type=alert.alert_type.value
                )
                
                return True
            else:
                logger.warning(f"Alert {alert_id} not found in active alerts")
                return False
                
        except Exception as e:
            logger.error(f"Error acknowledging alert {alert_id}: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """Perform comprehensive health check of monitoring system."""
        
        is_healthy = (
            self._running and
            self._monitor_task and not self._monitor_task.done() and
            self._monitoring_metrics["average_monitoring_time_ms"] < 1000 and  # <1s monitoring time
            len(self._active_alerts) < 50  # Reasonable alert count
        )
        
        return {
            "status": "healthy" if is_healthy else "degraded",
            "monitoring_running": self._running,
            "monitor_task_active": self._monitor_task and not self._monitor_task.done(),
            "alert_processor_active": self._alert_processor_task and not self._alert_processor_task.done(),
            "average_monitoring_time_ms": self._monitoring_metrics["average_monitoring_time_ms"],
            "active_alerts_count": len(self._active_alerts),
            "monitoring_cycles_completed": self._monitoring_metrics["monitoring_cycles_completed"],
            "timestamp": datetime.utcnow().isoformat()
        }