"""
Observability Streams Manager for LeanVibe Agent Hive 2.0 - VS 6.1

Redis Streams management for observability events with consumer groups:
- High-performance Redis Streams operations
- Consumer group coordination and load balancing
- Stream health monitoring and recovery
- Automatic scaling based on consumer group lag
- Dead letter queue handling for failed events
- Stream compaction and retention management
"""

import asyncio
import time
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Callable, NamedTuple
from dataclasses import dataclass
from enum import Enum
import structlog

import redis.asyncio as redis
from redis.exceptions import ResponseError

logger = structlog.get_logger()


class StreamHealth(Enum):
    """Stream health status indicators."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    UNAVAILABLE = "unavailable"


@dataclass
class ConsumerGroupInfo:
    """Information about a consumer group."""
    name: str
    consumers: int
    pending: int
    last_delivered_id: str
    lag: int = 0
    
    @property
    def is_lagging(self) -> bool:
        """Check if consumer group is lagging behind."""
        return self.lag > 1000 or self.pending > 100


@dataclass
class StreamInfo:
    """Information about a Redis stream."""
    name: str
    length: int
    radix_tree_keys: int
    radix_tree_nodes: int
    groups: int
    last_generated_id: str
    first_entry: Optional[Dict] = None
    last_entry: Optional[Dict] = None


class ConsumerGroupLagInfo(NamedTuple):
    """Consumer group lag information."""
    group_name: str
    pending_messages: int
    idle_time_ms: int
    consumers_count: int
    avg_processing_time_ms: float


class ObservabilityStreamsManager:
    """
    Manages Redis Streams for observability events with advanced features:
    - Consumer group management and scaling
    - Stream health monitoring
    - Automatic recovery and failover
    - Performance optimization
    """
    
    def __init__(
        self,
        redis_client: Optional[redis.Redis] = None,
        stream_name: str = "observability_events",
        max_stream_length: int = 100000,
        retention_hours: int = 72,
        health_check_interval_seconds: int = 30
    ):
        """Initialize streams manager."""
        self.redis_client = redis_client
        self.stream_name = stream_name
        self.max_stream_length = max_stream_length
        self.retention_hours = retention_hours
        self.health_check_interval = health_check_interval_seconds
        
        # Consumer group tracking
        self._consumer_groups: Set[str] = set()
        self._group_configs: Dict[str, Dict[str, Any]] = {}
        
        # Health monitoring
        self._health_status = StreamHealth.UNAVAILABLE
        self._last_health_check = None
        self._health_metrics: Dict[str, Any] = {}
        
        # Background task management
        self._monitoring_task: Optional[asyncio.Task] = None
        self._is_monitoring = False
        
        logger.info(
            "ðŸŒŠ ObservabilityStreamsManager initialized",
            stream_name=stream_name,
            max_length=max_stream_length,
            retention_hours=retention_hours
        )
    
    async def _ensure_redis_client(self) -> None:
        """Ensure Redis client is available."""
        if self.redis_client is None:
            from .redis import get_redis
            self.redis_client = await get_redis()
    
    async def initialize_stream(self) -> None:
        """Initialize the observability stream if it doesn't exist."""
        await self._ensure_redis_client()
        
        try:
            # Check if stream exists
            exists = await self.redis_client.exists(self.stream_name)
            
            if not exists:
                # Create stream with a dummy message
                await self.redis_client.xadd(
                    self.stream_name,
                    {"initialized": "true", "timestamp": datetime.utcnow().isoformat()},
                    maxlen=1
                )
                
                # Remove the dummy message
                stream_info = await self.redis_client.xinfo_stream(self.stream_name)
                first_id = stream_info.get("first-entry", [None])[0]
                if first_id:
                    await self.redis_client.xdel(self.stream_name, first_id)
                
                logger.info(f"âœ… Created observability stream: {self.stream_name}")
            else:
                logger.info(f"â„¹ï¸ Stream already exists: {self.stream_name}")
            
        except Exception as e:
            logger.error("âŒ Failed to initialize stream", error=str(e), exc_info=True)
            raise
    
    async def create_consumer_group(
        self,
        group_name: str,
        start_from: str = "0",
        consumer_config: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Create a consumer group for the observability stream.
        
        Args:
            group_name: Name of the consumer group
            start_from: Starting position ('0' for beginning, '$' for end, or specific ID)
            consumer_config: Configuration for the consumer group
            
        Returns:
            True if group was created, False if already exists
        """
        await self._ensure_redis_client()
        
        try:
            await self.redis_client.xgroup_create(
                self.stream_name,
                group_name,
                id=start_from,
                mkstream=True
            )
            
            self._consumer_groups.add(group_name)
            self._group_configs[group_name] = consumer_config or {}
            
            logger.info(
                f"âœ… Created consumer group: {group_name}",
                stream=self.stream_name,
                start_from=start_from
            )
            return True
            
        except ResponseError as e:
            if "BUSYGROUP" in str(e):
                logger.info(f"â„¹ï¸ Consumer group already exists: {group_name}")
                self._consumer_groups.add(group_name)
                self._group_configs[group_name] = consumer_config or {}
                return False
            else:
                logger.error("âŒ Failed to create consumer group", error=str(e))
                raise
    
    async def delete_consumer_group(self, group_name: str) -> bool:
        """Delete a consumer group."""
        await self._ensure_redis_client()
        
        try:
            result = await self.redis_client.xgroup_destroy(self.stream_name, group_name)
            
            if result:
                self._consumer_groups.discard(group_name)
                self._group_configs.pop(group_name, None)
                logger.info(f"âœ… Deleted consumer group: {group_name}")
            
            return bool(result)
            
        except Exception as e:
            logger.error("âŒ Failed to delete consumer group", error=str(e))
            return False
    
    async def get_stream_info(self) -> StreamInfo:
        """Get comprehensive stream information."""
        await self._ensure_redis_client()
        
        try:
            info = await self.redis_client.xinfo_stream(self.stream_name)
            
            return StreamInfo(
                name=self.stream_name,
                length=info.get("length", 0),
                radix_tree_keys=info.get("radix-tree-keys", 0),
                radix_tree_nodes=info.get("radix-tree-nodes", 0),
                groups=info.get("groups", 0),
                last_generated_id=info.get("last-generated-id", "0-0"),
                first_entry=info.get("first-entry"),
                last_entry=info.get("last-entry")
            )
            
        except Exception as e:
            logger.error("âŒ Failed to get stream info", error=str(e))
            raise
    
    async def get_consumer_groups_info(self) -> List[ConsumerGroupInfo]:
        """Get information about all consumer groups."""
        await self._ensure_redis_client()
        
        try:
            groups = await self.redis_client.xinfo_groups(self.stream_name)
            
            consumer_groups = []
            for group in groups:
                # Calculate lag (approximation)
                last_delivered = group[b"last-delivered-id"].decode()
                stream_info = await self.get_stream_info()
                
                # Simple lag calculation based on ID comparison
                lag = 0
                try:
                    if last_delivered != "0-0" and stream_info.last_generated_id != "0-0":
                        last_delivered_parts = last_delivered.split('-')
                        last_generated_parts = stream_info.last_generated_id.split('-')
                        
                        if len(last_delivered_parts) == 2 and len(last_generated_parts) == 2:
                            lag = int(last_generated_parts[0]) - int(last_delivered_parts[0])
                            lag = max(0, lag)  # Ensure non-negative
                except:
                    lag = 0
                
                consumer_groups.append(
                    ConsumerGroupInfo(
                        name=group[b"name"].decode(),
                        consumers=group[b"consumers"],
                        pending=group[b"pending"],
                        last_delivered_id=last_delivered,
                        lag=lag
                    )
                )
            
            return consumer_groups
            
        except Exception as e:
            logger.error("âŒ Failed to get consumer groups info", error=str(e))
            return []
    
    async def get_consumer_group_lag(self, group_name: str) -> ConsumerGroupLagInfo:
        """Get detailed lag information for a specific consumer group."""
        await self._ensure_redis_client()
        
        try:
            # Get group info
            groups_info = await self.get_consumer_groups_info()
            group_info = next((g for g in groups_info if g.name == group_name), None)
            
            if not group_info:
                raise ValueError(f"Consumer group not found: {group_name}")
            
            # Get pending messages details
            pending_info = await self.redis_client.xpending(self.stream_name, group_name)
            
            # Get consumers info
            consumers_info = await self.redis_client.xinfo_consumers(self.stream_name, group_name)
            
            # Calculate metrics
            pending_messages = pending_info[0] if pending_info else 0
            consumers_count = len(consumers_info)
            
            # Calculate average idle time
            total_idle_time = 0
            if consumers_info:
                for consumer in consumers_info:
                    total_idle_time += consumer[b"idle"]
                avg_idle_time = total_idle_time / len(consumers_info)
            else:
                avg_idle_time = 0
            
            # Approximate processing time (simplified)
            avg_processing_time = max(100, avg_idle_time / 10)  # Very rough estimate
            
            return ConsumerGroupLagInfo(
                group_name=group_name,
                pending_messages=pending_messages,
                idle_time_ms=int(avg_idle_time),
                consumers_count=consumers_count,
                avg_processing_time_ms=avg_processing_time
            )
            
        except Exception as e:
            logger.error("âŒ Failed to get consumer group lag", error=str(e))
            return ConsumerGroupLagInfo(group_name, 0, 0, 0, 0.0)
    
    async def trim_stream(self, max_len: Optional[int] = None) -> int:
        """
        Trim stream to manage memory usage.
        
        Args:
            max_len: Maximum length (uses instance default if None)
            
        Returns:
            Number of entries removed
        """
        await self._ensure_redis_client()
        
        max_len = max_len or self.max_stream_length
        
        try:
            # Get current stream length
            info = await self.get_stream_info()
            current_length = info.length
            
            if current_length <= max_len:
                return 0
            
            # Trim stream
            result = await self.redis_client.xtrim(
                self.stream_name,
                maxlen=max_len,
                approximate=True  # More efficient
            )
            
            logger.info(
                f"âœ‚ï¸ Trimmed stream",
                stream=self.stream_name,
                previous_length=current_length,
                new_max_length=max_len,
                entries_removed=result
            )
            
            return result
            
        except Exception as e:
            logger.error("âŒ Failed to trim stream", error=str(e))
            return 0
    
    async def purge_old_entries(self, older_than_hours: Optional[int] = None) -> int:
        """
        Purge entries older than specified time.
        
        Args:
            older_than_hours: Hours threshold (uses instance default if None)
            
        Returns:
            Number of entries removed
        """
        await self._ensure_redis_client()
        
        older_than_hours = older_than_hours or self.retention_hours
        cutoff_time = datetime.utcnow() - timedelta(hours=older_than_hours)
        cutoff_timestamp_ms = int(cutoff_time.timestamp() * 1000)
        
        try:
            # Find entries to delete
            entries_to_delete = []
            
            # Read entries in batches
            last_id = "0"
            batch_size = 100
            
            while True:
                entries = await self.redis_client.xrange(
                    self.stream_name,
                    min=last_id,
                    max="+",
                    count=batch_size
                )
                
                if not entries:
                    break
                
                for entry_id, fields in entries:
                    # Extract timestamp from entry ID
                    timestamp_part = entry_id.decode().split('-')[0]
                    entry_timestamp_ms = int(timestamp_part)
                    
                    if entry_timestamp_ms < cutoff_timestamp_ms:
                        entries_to_delete.append(entry_id.decode())
                    else:
                        # Since entries are ordered, we can stop here
                        break
                
                last_id = entries[-1][0].decode()
                
                # Break if we've found entries newer than cutoff
                if entries and int(entries[-1][0].decode().split('-')[0]) >= cutoff_timestamp_ms:
                    break
            
            # Delete old entries
            deleted_count = 0
            if entries_to_delete:
                deleted_count = await self.redis_client.xdel(self.stream_name, *entries_to_delete)
                
                logger.info(
                    f"ðŸ—‘ï¸ Purged old entries",
                    stream=self.stream_name,
                    entries_removed=deleted_count,
                    cutoff_hours=older_than_hours
                )
            
            return deleted_count
            
        except Exception as e:
            logger.error("âŒ Failed to purge old entries", error=str(e))
            return 0
    
    async def check_health(self) -> Dict[str, Any]:
        """
        Perform comprehensive health check of the streams system.
        
        Returns:
            Health status and metrics
        """
        await self._ensure_redis_client()
        
        health_metrics = {
            "timestamp": datetime.utcnow().isoformat(),
            "stream_accessible": False,
            "stream_info": None,
            "consumer_groups": [],
            "lagging_groups": [],
            "recommendations": []
        }
        
        try:
            # Check stream accessibility
            await self.redis_client.ping()
            health_metrics["stream_accessible"] = True
            
            # Get stream info
            stream_info = await self.get_stream_info()
            health_metrics["stream_info"] = {
                "length": stream_info.length,
                "groups": stream_info.groups,
                "last_id": stream_info.last_generated_id
            }
            
            # Check consumer groups
            groups_info = await self.get_consumer_groups_info()
            health_metrics["consumer_groups"] = [
                {
                    "name": group.name,
                    "consumers": group.consumers,
                    "pending": group.pending,
                    "lag": group.lag,
                    "is_lagging": group.is_lagging
                }
                for group in groups_info
            ]
            
            # Identify lagging groups
            lagging_groups = [group for group in groups_info if group.is_lagging]
            health_metrics["lagging_groups"] = [group.name for group in lagging_groups]
            
            # Generate recommendations
            recommendations = []
            
            if stream_info.length > self.max_stream_length * 0.9:
                recommendations.append("Stream length approaching limit - consider trimming")
            
            if lagging_groups:
                recommendations.append(f"Consumer groups lagging: {', '.join(g.name for g in lagging_groups)}")
            
            if stream_info.groups == 0:
                recommendations.append("No consumer groups configured")
            
            # Determine overall health status
            if not health_metrics["stream_accessible"]:
                self._health_status = StreamHealth.UNAVAILABLE
            elif lagging_groups or stream_info.length > self.max_stream_length:
                self._health_status = StreamHealth.DEGRADED
            elif stream_info.groups == 0:
                self._health_status = StreamHealth.DEGRADED
            else:
                self._health_status = StreamHealth.HEALTHY
            
            health_metrics["status"] = self._health_status.value
            health_metrics["recommendations"] = recommendations
            
            self._last_health_check = datetime.utcnow()
            self._health_metrics = health_metrics
            
        except Exception as e:
            logger.error("âŒ Health check failed", error=str(e))
            self._health_status = StreamHealth.UNAVAILABLE
            health_metrics["status"] = StreamHealth.UNAVAILABLE.value
            health_metrics["error"] = str(e)
        
        return health_metrics
    
    async def start_monitoring(self) -> None:
        """Start background monitoring of stream health."""
        if self._is_monitoring:
            logger.warning("Stream monitoring is already running")
            return
        
        self._is_monitoring = True
        self._monitoring_task = asyncio.create_task(self._monitoring_loop())
        
        logger.info("ðŸ” Started stream monitoring")
    
    async def stop_monitoring(self) -> None:
        """Stop background monitoring."""
        if not self._is_monitoring:
            return
        
        self._is_monitoring = False
        
        if self._monitoring_task:
            self._monitoring_task.cancel()
            try:
                await self._monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("ðŸ›‘ Stopped stream monitoring")
    
    async def _monitoring_loop(self) -> None:
        """Background monitoring loop."""
        try:
            while self._is_monitoring:
                # Perform health check
                await self.check_health()
                
                # Automatic maintenance
                if self._health_status in [StreamHealth.HEALTHY, StreamHealth.DEGRADED]:
                    # Trim stream if too long
                    stream_info = await self.get_stream_info()
                    if stream_info.length > self.max_stream_length:
                        await self.trim_stream()
                    
                    # Purge old entries
                    await self.purge_old_entries()
                
                # Wait for next check
                await asyncio.sleep(self.health_check_interval)
                
        except asyncio.CancelledError:
            logger.info("Stream monitoring loop cancelled")
        except Exception as e:
            logger.error("âŒ Stream monitoring loop failed", error=str(e), exc_info=True)
        finally:
            self._is_monitoring = False
    
    async def get_health_status(self) -> StreamHealth:
        """Get current health status."""
        return self._health_status
    
    async def get_health_metrics(self) -> Dict[str, Any]:
        """Get latest health metrics."""
        if not self._health_metrics or not self._last_health_check:
            return await self.check_health()
        
        # Return cached metrics if recent
        age = (datetime.utcnow() - self._last_health_check).total_seconds()
        if age < self.health_check_interval:
            return self._health_metrics
        
        # Refresh if stale
        return await self.check_health()
    
    async def setup_standard_consumer_groups(self) -> None:
        """Set up standard consumer groups for the observability system."""
        standard_groups = {
            "dashboard_group": {"start_from": "$", "description": "Real-time dashboard updates"},
            "analytics_group": {"start_from": "0", "description": "Historical analytics processing"},
            "performance_group": {"start_from": "$", "description": "Performance monitoring"},
            "alerting_group": {"start_from": "$", "description": "Alert processing"},
            "storage_group": {"start_from": "0", "description": "Long-term storage"}
        }
        
        for group_name, config in standard_groups.items():
            await self.create_consumer_group(
                group_name=group_name,
                start_from=config["start_from"],
                consumer_config=config
            )
        
        logger.info(f"âœ… Set up {len(standard_groups)} standard consumer groups")


# Global streams manager instance
_streams_manager: Optional[ObservabilityStreamsManager] = None


def get_streams_manager() -> Optional[ObservabilityStreamsManager]:
    """Get the global streams manager instance."""
    return _streams_manager


async def initialize_streams_manager(
    redis_client: Optional[redis.Redis] = None,
    **kwargs
) -> ObservabilityStreamsManager:
    """Initialize and set the global streams manager."""
    global _streams_manager
    
    _streams_manager = ObservabilityStreamsManager(redis_client=redis_client, **kwargs)
    
    # Initialize stream and set up consumer groups
    await _streams_manager.initialize_stream()
    await _streams_manager.setup_standard_consumer_groups()
    
    # Start monitoring
    await _streams_manager.start_monitoring()
    
    logger.info("âœ… Global streams manager initialized")
    return _streams_manager


async def shutdown_streams_manager() -> None:
    """Shutdown the global streams manager."""
    global _streams_manager
    
    if _streams_manager:
        await _streams_manager.stop_monitoring()
        _streams_manager = None
        logger.info("âœ… Streams manager shutdown complete")