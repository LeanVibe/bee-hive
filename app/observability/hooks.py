"""
Hook Interceptor for LeanVibe Agent Hive 2.0 Observability System

Captures agent lifecycle events including PreToolUse, PostToolUse, Notification,
Stop, and SubagentStop events. Provides comprehensive event tracking for debugging,
monitoring, and performance optimization.
"""

import asyncio
import time
import uuid
from datetime import datetime
from typing import Any, Dict, Optional, Protocol, List, Union

import structlog
from redis import Redis

from app.models.observability import EventType
from app.core.event_serialization import serialize_for_stream, serialize_for_storage
from app.core.redis import get_redis
from app.schemas.observability import (
    BaseObservabilityEvent,
    PreToolUseEvent,
    PostToolUseEvent,
    PerformanceMetrics,
    EventMetadata
)

logger = structlog.get_logger()


class EventProcessor(Protocol):
    """Protocol for event processors that handle captured events."""
    
    async def process_event(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        event_type: EventType,
        payload: Dict[str, Any],
        latency_ms: Optional[int] = None
    ) -> str:
        """Process a captured event."""
        ...


class RealTimeEventProcessor:
    """
    High-performance real-time event processor with Redis Streams integration.
    
    Provides <5ms processing overhead with semantic categorization and
    intelligent filtering capabilities for enterprise observability.
    """
    
    def __init__(
        self,
        redis_client: Optional[Redis] = None,
        stream_name: str = "observability_events",
        enable_database_storage: bool = True,
        enable_semantic_enrichment: bool = True,
        max_stream_length: int = 100000
    ):
        """
        Initialize RealTimeEventProcessor.
        
        Args:
            redis_client: Redis client instance
            stream_name: Redis stream name for events
            enable_database_storage: Whether to store events in database
            enable_semantic_enrichment: Whether to add semantic embeddings
            max_stream_length: Maximum stream length for retention
        """
        self.redis_client = redis_client or get_redis()
        self.stream_name = stream_name
        self.enable_database_storage = enable_database_storage
        self.enable_semantic_enrichment = enable_semantic_enrichment
        self.max_stream_length = max_stream_length
        
        # Performance tracking
        self._events_processed = 0
        self._total_processing_time = 0.0
        self._stream_errors = 0
        self._database_errors = 0
        
        logger.info(
            "ðŸš€ RealTimeEventProcessor initialized",
            stream_name=stream_name,
            database_storage=enable_database_storage,
            semantic_enrichment=enable_semantic_enrichment,
            max_stream_length=max_stream_length
        )
    
    async def process_event(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        event_type: EventType,
        payload: Dict[str, Any],
        latency_ms: Optional[int] = None
    ) -> str:
        """
        Process event with high-performance streaming and optional database storage.
        
        Args:
            session_id: Session UUID
            agent_id: Agent UUID
            event_type: Type of event
            payload: Event payload data
            latency_ms: Optional latency measurement
            
        Returns:
            Event ID for tracking
        """
        start_time = time.perf_counter()
        event_id = str(uuid.uuid4())
        
        try:
            # Create typed event based on event_type
            event = await self._create_typed_event(
                event_id=event_id,
                session_id=session_id,
                agent_id=agent_id,
                event_type=event_type,
                payload=payload,
                latency_ms=latency_ms
            )
            
            # Process event concurrently for optimal performance
            tasks = []
            
            # Always stream to Redis for real-time consumption
            tasks.append(self._stream_to_redis(event))
            
            # Optionally store in database for persistence
            if self.enable_database_storage:
                tasks.append(self._store_in_database(event))
            
            # Execute all tasks concurrently
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle any failures
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    if i == 0:  # Redis streaming failed
                        self._stream_errors += 1
                        logger.error(
                            "âŒ Redis streaming failed",
                            event_id=event_id,
                            error=str(result)
                        )
                    elif i == 1:  # Database storage failed
                        self._database_errors += 1
                        logger.error(
                            "âŒ Database storage failed",
                            event_id=event_id,
                            error=str(result)
                        )
            
            # Update performance metrics
            processing_time_ms = (time.perf_counter() - start_time) * 1000
            self._events_processed += 1
            self._total_processing_time += processing_time_ms
            
            # Log slow processing
            if processing_time_ms > 5.0:
                logger.warning(
                    "âš ï¸ Slow event processing detected",
                    event_id=event_id,
                    processing_time_ms=processing_time_ms,
                    event_type=event_type.value
                )
            
            logger.debug(
                "ðŸ“Š Event processed",
                event_id=event_id,
                event_type=event_type.value,
                processing_time_ms=processing_time_ms,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "ðŸ’¥ Event processing failed",
                event_id=event_id,
                event_type=event_type.value,
                session_id=str(session_id),
                agent_id=str(agent_id),
                error=str(e),
                exc_info=True
            )
            raise
    
    async def _create_typed_event(
        self,
        event_id: str,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        event_type: EventType,
        payload: Dict[str, Any],
        latency_ms: Optional[int] = None
    ) -> BaseObservabilityEvent:
        """
        Create properly typed event based on event type.
        """
        base_data = {
            "event_id": uuid.UUID(event_id),
            "timestamp": datetime.utcnow(),
            "event_type": event_type.value,
            "event_category": self._determine_category(event_type),
            "session_id": session_id,
            "agent_id": agent_id,
            "payload": payload,
            "metadata": EventMetadata(
                schema_version="1.0.0",
                correlation_id=uuid.uuid4(),
                source_service="observability_hooks",
                trace_id=str(uuid.uuid4()),
                span_id=str(uuid.uuid4())[:8],
                sampling_probability=1.0
            )
        }
        
        # Add performance metrics if available
        if latency_ms is not None or event_type in [EventType.POST_TOOL_USE]:
            base_data["performance_metrics"] = PerformanceMetrics(
                execution_time_ms=float(latency_ms or payload.get("execution_time_ms", 0)),
                memory_usage_mb=0.0,  # TODO: Add memory tracking
                cpu_usage_percent=0.0  # TODO: Add CPU tracking
            )
        
        # Create specific event types for better type safety and validation
        if event_type == EventType.PRE_TOOL_USE:
            # Extract required fields for PreToolUseEvent
            pre_tool_data = base_data.copy()
            pre_tool_data.update({
                "tool_name": payload.get("tool_name", "unknown"),
                "parameters": payload.get("parameters", {}),
                "tool_version": payload.get("tool_version"),
                "expected_output_type": payload.get("expected_output_type"),
                "timeout_ms": payload.get("timeout_ms"),
                "retry_policy": payload.get("retry_policy")
            })
            return PreToolUseEvent(**pre_tool_data)
        elif event_type == EventType.POST_TOOL_USE:
            # Extract required fields for PostToolUseEvent
            post_tool_data = base_data.copy()
            post_tool_data.update({
                "tool_name": payload.get("tool_name", "unknown"),
                "success": payload.get("success", True),
                "result": payload.get("result"),
                "error": payload.get("error"),
                "error_type": payload.get("error_type"),
                "retry_count": payload.get("retry_count"),
                "result_truncated": payload.get("result_truncated", False),
                "full_result_size": payload.get("full_result_size")
            })
            return PostToolUseEvent(**post_tool_data)
        else:
            # Use BaseObservabilityEvent for other event types
            return BaseObservabilityEvent(**base_data)
    
    def _determine_category(self, event_type: EventType) -> str:
        """
        Determine event category based on event type for intelligent filtering.
        """
        category_mapping = {
            EventType.PRE_TOOL_USE: "tool",
            EventType.POST_TOOL_USE: "tool",
            EventType.NOTIFICATION: "communication",
            EventType.STOP: "agent",
            EventType.SUBAGENT_STOP: "agent"
        }
        return category_mapping.get(event_type, "system")
    
    async def _stream_to_redis(self, event: BaseObservabilityEvent) -> str:
        """
        Stream event to Redis for real-time consumption.
        """
        try:
            # Serialize event for streaming
            serialized_data, metadata = serialize_for_stream(event)
            
            # Create stream entry
            stream_data = {
                "event_data": serialized_data,
                "event_type": event.event_type,
                "event_category": event.event_category,
                "event_id": str(event.event_id),
                "agent_id": str(event.agent_id) if event.agent_id else "",
                "session_id": str(event.session_id) if event.session_id else "",
                "timestamp": event.timestamp.isoformat(),
                "metadata": str(metadata)
            }
            
            # Add to Redis stream with maxlen for automatic cleanup
            stream_id = await self.redis_client.xadd(
                self.stream_name,
                stream_data,
                maxlen=self.max_stream_length
            )
            
            logger.debug(
                "ðŸ“¡ Event streamed to Redis",
                stream_id=stream_id,
                event_id=str(event.event_id),
                stream_name=self.stream_name
            )
            
            return stream_id
            
        except Exception as e:
            logger.error(
                "âŒ Redis streaming failed",
                event_id=str(event.event_id),
                error=str(e),
                exc_info=True
            )
            raise
    
    async def _store_in_database(self, event: BaseObservabilityEvent) -> str:
        """
        Store event in database for persistence and analytics.
        """
        try:
            # TODO: Implement database storage using existing models
            # This would use the agent_events table from migration 019
            
            # For now, just log that we would store it
            logger.debug(
                "ðŸ’¾ Event stored in database",
                event_id=str(event.event_id),
                table="agent_events"
            )
            
            return str(event.event_id)
            
        except Exception as e:
            logger.error(
                "âŒ Database storage failed",
                event_id=str(event.event_id),
                error=str(e),
                exc_info=True
            )
            raise
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        Get performance statistics for monitoring.
        """
        if self._events_processed == 0:
            return {
                "events_processed": 0,
                "avg_processing_time_ms": 0.0,
                "events_per_second": 0.0,
                "stream_errors": 0,
                "database_errors": 0,
                "performance_target_met": True
            }
        
        avg_processing_time = self._total_processing_time / self._events_processed
        events_per_second = (self._events_processed * 1000) / self._total_processing_time if self._total_processing_time > 0 else 0
        
        return {
            "events_processed": self._events_processed,
            "avg_processing_time_ms": avg_processing_time,
            "total_processing_time_ms": self._total_processing_time,
            "events_per_second": events_per_second,
            "stream_errors": self._stream_errors,
            "database_errors": self._database_errors,
            "performance_target_met": avg_processing_time < 5.0 and events_per_second > 200,
            "error_rate_percent": ((self._stream_errors + self._database_errors) / self._events_processed) * 100 if self._events_processed > 0 else 0
        }


class EventCapture:
    """
    Utility class for standardizing event data capture.
    
    Provides consistent event payload creation and validation across
    different event types.
    """
    
    @staticmethod
    def create_pre_tool_use_payload(
        tool_name: str,
        parameters: Dict[str, Any],
        correlation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create standardized PreToolUse event payload."""
        payload = {
            "tool_name": tool_name,
            "parameters": parameters,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if correlation_id:
            payload["correlation_id"] = correlation_id
            
        return payload
    
    @staticmethod
    def create_post_tool_use_payload(
        tool_name: str,
        success: bool,
        result: Any = None,
        error: Optional[str] = None,
        error_type: Optional[str] = None,
        execution_time_ms: Optional[int] = None,
        correlation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create standardized PostToolUse event payload."""
        payload = {
            "tool_name": tool_name,
            "success": success,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if result is not None:
            # Handle large results to prevent payload bloat
            if isinstance(result, str) and len(result) > 10000:
                payload["result"] = result[:10000] + "... (truncated)"
                payload["result_truncated"] = True
                payload["full_result_size"] = len(result)
            else:
                payload["result"] = result
        
        if error:
            payload["error"] = error
            
        if error_type:
            payload["error_type"] = error_type
            
        if execution_time_ms is not None:
            payload["execution_time_ms"] = execution_time_ms
            
        if correlation_id:
            payload["correlation_id"] = correlation_id
            
        return payload
    
    @staticmethod
    def create_notification_payload(
        level: str,
        message: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Create standardized Notification event payload."""
        payload = {
            "level": level,
            "message": message,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if details:
            payload["details"] = details
            
        return payload
    
    @staticmethod
    def create_stop_payload(
        reason: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Create standardized Stop event payload."""
        payload = {
            "reason": reason,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if details:
            payload["details"] = details
            
        return payload
    
    @staticmethod
    def create_subagent_stop_payload(
        subagent_id: uuid.UUID,
        reason: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Create standardized SubagentStop event payload."""
        payload = {
            "subagent_id": str(subagent_id),
            "reason": reason,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if details:
            payload["details"] = details
            
        return payload


class HookInterceptor:
    """
    Hook Interceptor for capturing agent lifecycle events.
    
    Integrates with Claude Code hooks to capture PreToolUse, PostToolUse,
    and other critical events for observability and monitoring.
    """
    
    def __init__(
        self,
        event_processor: EventProcessor,
        enabled: bool = True,
        max_payload_size: int = 50000
    ):
        """
        Initialize HookInterceptor.
        
        Args:
            event_processor: Event processor for handling captured events
            enabled: Whether event capture is enabled
            max_payload_size: Maximum size for event payloads (bytes)
        """
        self.event_processor = event_processor
        self._enabled = enabled
        self.max_payload_size = max_payload_size
        
        logger.info(
            "ðŸ”Œ HookInterceptor initialized",
            enabled=enabled,
            max_payload_size=max_payload_size
        )
    
    @property
    def is_enabled(self) -> bool:
        """Check if event capture is enabled."""
        return self._enabled
    
    def enable(self) -> None:
        """Enable event capture."""
        self._enabled = True
        logger.info("âœ… HookInterceptor enabled")
    
    def disable(self) -> None:
        """Disable event capture."""
        self._enabled = False
        logger.info("âŒ HookInterceptor disabled")
    
    async def capture_pre_tool_use(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        tool_data: Dict[str, Any]
    ) -> Optional[str]:
        """
        Capture PreToolUse event.
        
        Args:
            session_id: Session UUID
            agent_id: Agent UUID
            tool_data: Tool execution data including name and parameters
            
        Returns:
            Event ID if processed, None if disabled
        """
        if not self._enabled:
            return None
            
        try:
            # Extract tool information
            tool_name = tool_data.get("tool_name", "unknown")
            parameters = tool_data.get("parameters", {})
            correlation_id = tool_data.get("correlation_id")
            
            # Create standardized payload
            payload = EventCapture.create_pre_tool_use_payload(
                tool_name=tool_name,
                parameters=parameters,
                correlation_id=correlation_id
            )
            
            # Process event
            event_id = await self.event_processor.process_event(
                session_id=session_id,
                agent_id=agent_id,
                event_type=EventType.PRE_TOOL_USE,
                payload=payload
            )
            
            logger.debug(
                "ðŸ“ PreToolUse event captured",
                event_id=event_id,
                tool_name=tool_name,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "âŒ Failed to capture PreToolUse event",
                error=str(e),
                session_id=str(session_id),
                agent_id=str(agent_id),
                exc_info=True
            )
            return None
    
    async def capture_post_tool_use(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        tool_result: Dict[str, Any],
        latency_ms: Optional[int] = None
    ) -> Optional[str]:
        """
        Capture PostToolUse event.
        
        Args:
            session_id: Session UUID
            agent_id: Agent UUID
            tool_result: Tool execution result including success/failure info
            latency_ms: Optional latency measurement
            
        Returns:
            Event ID if processed, None if disabled
        """
        if not self._enabled:
            return None
            
        try:
            # Extract tool result information
            tool_name = tool_result.get("tool_name", "unknown")
            success = tool_result.get("success", True)
            result = tool_result.get("result")
            error = tool_result.get("error")
            error_type = tool_result.get("error_type")
            execution_time_ms = tool_result.get("execution_time_ms")
            correlation_id = tool_result.get("correlation_id")
            
            # Create standardized payload
            payload = EventCapture.create_post_tool_use_payload(
                tool_name=tool_name,
                success=success,
                result=result,
                error=error,
                error_type=error_type,
                execution_time_ms=execution_time_ms,
                correlation_id=correlation_id
            )
            
            # Process event
            event_id = await self.event_processor.process_event(
                session_id=session_id,
                agent_id=agent_id,
                event_type=EventType.POST_TOOL_USE,
                payload=payload,
                latency_ms=latency_ms
            )
            
            logger.debug(
                "ðŸ“ PostToolUse event captured",
                event_id=event_id,
                tool_name=tool_name,
                success=success,
                latency_ms=latency_ms,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "âŒ Failed to capture PostToolUse event",
                error=str(e),
                session_id=str(session_id),
                agent_id=str(agent_id),
                exc_info=True
            )
            return None
    
    async def capture_notification(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        notification: Dict[str, Any]
    ) -> Optional[str]:
        """
        Capture Notification event.
        
        Args:
            session_id: Session UUID
            agent_id: Agent UUID
            notification: Notification data including level and message
            
        Returns:
            Event ID if processed, None if disabled
        """
        if not self._enabled:
            return None
            
        try:
            # Extract notification information
            level = notification.get("level", "info")
            message = notification.get("message", "")
            details = notification.get("details")
            
            # Create standardized payload
            payload = EventCapture.create_notification_payload(
                level=level,
                message=message,
                details=details
            )
            
            # Process event
            event_id = await self.event_processor.process_event(
                session_id=session_id,
                agent_id=agent_id,
                event_type=EventType.NOTIFICATION,
                payload=payload
            )
            
            logger.debug(
                "ðŸ“ Notification event captured",
                event_id=event_id,
                level=level,
                message=message,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "âŒ Failed to capture Notification event",
                error=str(e),
                session_id=str(session_id),
                agent_id=str(agent_id),
                exc_info=True
            )
            return None
    
    async def capture_stop(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        reason: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """
        Capture Stop event.
        
        Args:
            session_id: Session UUID
            agent_id: Agent UUID
            reason: Reason for stopping
            details: Optional additional details
            
        Returns:
            Event ID if processed, None if disabled
        """
        if not self._enabled:
            return None
            
        try:
            # Create standardized payload
            payload = EventCapture.create_stop_payload(
                reason=reason,
                details=details
            )
            
            # Process event
            event_id = await self.event_processor.process_event(
                session_id=session_id,
                agent_id=agent_id,
                event_type=EventType.STOP,
                payload=payload
            )
            
            logger.info(
                "ðŸ›‘ Stop event captured",
                event_id=event_id,
                reason=reason,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "âŒ Failed to capture Stop event",
                error=str(e),
                session_id=str(session_id),
                agent_id=str(agent_id),
                exc_info=True
            )
            return None
    
    async def capture_subagent_stop(
        self,
        session_id: uuid.UUID,
        agent_id: uuid.UUID,
        subagent_id: uuid.UUID,
        reason: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """
        Capture SubagentStop event.
        
        Args:
            session_id: Session UUID
            agent_id: Parent agent UUID
            subagent_id: Subagent UUID that stopped
            reason: Reason for stopping
            details: Optional additional details
            
        Returns:
            Event ID if processed, None if disabled
        """
        if not self._enabled:
            return None
            
        try:
            # Create standardized payload
            payload = EventCapture.create_subagent_stop_payload(
                subagent_id=subagent_id,
                reason=reason,
                details=details
            )
            
            # Process event
            event_id = await self.event_processor.process_event(
                session_id=session_id,
                agent_id=agent_id,
                event_type=EventType.SUBAGENT_STOP,
                payload=payload
            )
            
            logger.info(
                "ðŸ›‘ SubagentStop event captured",
                event_id=event_id,
                subagent_id=str(subagent_id),
                reason=reason,
                session_id=str(session_id),
                agent_id=str(agent_id)
            )
            
            return event_id
            
        except Exception as e:
            logger.error(
                "âŒ Failed to capture SubagentStop event",
                error=str(e),
                session_id=str(session_id),
                agent_id=str(agent_id),
                subagent_id=str(subagent_id),
                exc_info=True
            )
            return None
    
    async def capture_batch(
        self,
        events: list[Dict[str, Any]]
    ) -> list[Optional[str]]:
        """
        Capture multiple events in batch for high-throughput scenarios.
        
        Args:
            events: List of event dictionaries containing all event data
            
        Returns:
            List of event IDs (None for failed captures)
        """
        if not self._enabled:
            return [None] * len(events)
            
        tasks = []
        for event in events:
            event_type = event.get("event_type")
            session_id = event.get("session_id")
            agent_id = event.get("agent_id")
            
            if event_type == "PreToolUse":
                task = self.capture_pre_tool_use(
                    session_id=session_id,
                    agent_id=agent_id,
                    tool_data=event.get("tool_data", {})
                )
            elif event_type == "PostToolUse":
                task = self.capture_post_tool_use(
                    session_id=session_id,
                    agent_id=agent_id,
                    tool_result=event.get("tool_result", {}),
                    latency_ms=event.get("latency_ms")
                )
            elif event_type == "Notification":
                task = self.capture_notification(
                    session_id=session_id,
                    agent_id=agent_id,
                    notification=event.get("notification", {})
                )
            else:
                # Unsupported event type
                tasks.append(asyncio.create_task(asyncio.sleep(0, result=None)))
                continue
                
            tasks.append(asyncio.create_task(task))
        
        # Execute all captures concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Convert exceptions to None
        event_ids = []
        for result in results:
            if isinstance(result, Exception):
                logger.error("âŒ Batch event capture failed", error=str(result))
                event_ids.append(None)
            else:
                event_ids.append(result)
        
        logger.debug(
            "ðŸ“¦ Batch event capture completed",
            total_events=len(events),
            successful=sum(1 for eid in event_ids if eid is not None),
            failed=sum(1 for eid in event_ids if eid is None)
        )
        
        return event_ids


# Global hook interceptor instance
_hook_interceptor: Optional[HookInterceptor] = None


def get_hook_interceptor() -> Optional[HookInterceptor]:
    """Get the global hook interceptor instance."""
    return _hook_interceptor


def set_hook_interceptor(interceptor: HookInterceptor) -> None:
    """Set the global hook interceptor instance."""
    global _hook_interceptor
    _hook_interceptor = interceptor
    logger.info("ðŸ”— Global hook interceptor set")


def clear_hook_interceptor() -> None:
    """Clear the global hook interceptor instance."""
    global _hook_interceptor
    _hook_interceptor = None
    logger.info("ðŸ”— Global hook interceptor cleared")