#!/usr/bin/env python3
"""
Engine Performance Benchmarking and Validation Script

Validates that consolidated engines meet all performance targets:
- TaskExecutionEngine: <100ms assignment, 1000+ concurrent tasks
- WorkflowEngine: <2s compilation, real-time dependency resolution  
- DataProcessingEngine: <50ms search, 60-80% compression
- SecurityEngine: <5ms decisions
- CommunicationEngine: <10ms routing, 10,000+ msg/sec
"""

import asyncio
import time
import statistics
from pathlib import Path
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from app.core.engines.base_engine import EngineConfig, EngineRequest
from app.core.engines.task_execution_engine import TaskExecutionEngine
from app.core.engines.workflow_engine import WorkflowEngine
from app.core.engines.data_processing_engine import DataProcessingEngine
from app.core.engines.security_engine import SecurityEngine
from app.core.engines.communication_engine import CommunicationEngine
from app.core.engines.monitoring_engine import MonitoringEngine
from app.core.engines.integration_engine import IntegrationEngine
from app.core.engines.optimization_engine import OptimizationEngine


class EngineBenchmarks:
    """Comprehensive engine performance benchmarks."""
    
    def __init__(self):
        self.results = {}
        self.engines = {}
    
    async def setup_engines(self):
        """Initialize all engines for testing."""
        print("üîß Setting up engines for benchmarking...")
        
        engine_configs = {
            "task_execution": EngineConfig(
                engine_id="task_execution_benchmark",
                name="Task Execution Engine",
                max_concurrent_requests=2000,
                plugins_enabled=True
            ),
            "workflow": EngineConfig(
                engine_id="workflow_benchmark", 
                name="Workflow Engine",
                max_concurrent_requests=1000
            ),
            "data_processing": EngineConfig(
                engine_id="data_processing_benchmark",
                name="Data Processing Engine",
                max_concurrent_requests=1000
            ),
            "security": EngineConfig(
                engine_id="security_benchmark",
                name="Security Engine", 
                max_concurrent_requests=5000  # High volume for auth
            ),
            "communication": EngineConfig(
                engine_id="communication_benchmark",
                name="Communication Engine",
                max_concurrent_requests=10000  # Very high throughput
            )
        }
        
        self.engines = {
            "task_execution": TaskExecutionEngine(engine_configs["task_execution"]),
            "workflow": WorkflowEngine(engine_configs["workflow"]),
            "data_processing": DataProcessingEngine(engine_configs["data_processing"]),
            "security": SecurityEngine(engine_configs["security"]),
            "communication": CommunicationEngine(engine_configs["communication"])
        }
        
        # Initialize all engines
        for name, engine in self.engines.items():
            await engine.initialize()
            print(f"  ‚úÖ {name.replace('_', ' ').title()} Engine initialized")
    
    async def benchmark_task_execution_engine(self):
        """Benchmark TaskExecutionEngine performance."""
        print("\nüìä Benchmarking TaskExecutionEngine...")
        engine = self.engines["task_execution"]
        
        # Test 1: Task Assignment Latency (<100ms target)
        print("  Testing task assignment latency...")
        latencies = []
        
        for i in range(1000):  # 1000 assignments for statistical significance
            start_time = time.time()
            
            request = EngineRequest(
                request_type="execute_task",
                payload={
                    "task_type": "function",
                    "task_data": {"index": i},
                    "execution_mode": "async"
                }
            )
            
            response = await engine.process(request)
            latency_ms = (time.time() - start_time) * 1000
            latencies.append(latency_ms)
            
            if not response.success:
                print(f"    ‚ùå Task assignment failed: {response.error}")
        
        avg_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th percentile
        
        print(f"    Average latency: {avg_latency:.2f}ms")
        print(f"    P95 latency: {p95_latency:.2f}ms") 
        print(f"    P99 latency: {p99_latency:.2f}ms")
        print(f"    Target <100ms: {'‚úÖ' if avg_latency < 100 else '‚ùå'}")
        
        # Test 2: Concurrent Task Capacity (1000+ target)
        print("  Testing concurrent task capacity...")
        concurrent_tasks = 1500  # Test above target
        
        start_time = time.time()
        requests = [
            EngineRequest(
                request_type="execute_task", 
                payload={
                    "task_type": "function",
                    "execution_mode": "async"
                }
            )
            for _ in range(concurrent_tasks)
        ]
        
        responses = await asyncio.gather(*[
            engine.process(request) for request in requests
        ])
        
        total_time = time.time() - start_time
        successful = sum(1 for r in responses if r.success)
        throughput = successful / total_time
        
        print(f"    Processed: {successful}/{concurrent_tasks} tasks")
        print(f"    Time: {total_time:.2f}s")
        print(f"    Throughput: {throughput:.0f} tasks/sec")
        print(f"    Target 1000+ concurrent: {'‚úÖ' if successful >= 1000 else '‚ùå'}")
        
        self.results["task_execution"] = {
            "avg_latency_ms": avg_latency,
            "p95_latency_ms": p95_latency,
            "p99_latency_ms": p99_latency,
            "concurrent_capacity": successful,
            "throughput_tasks_per_sec": throughput,
            "latency_target_met": avg_latency < 100,
            "capacity_target_met": successful >= 1000
        }
    
    async def benchmark_workflow_engine(self):
        """Benchmark WorkflowEngine performance.""" 
        print("\nüìä Benchmarking WorkflowEngine...")
        engine = self.engines["workflow"]
        
        # Test workflow compilation time (<2s target)
        print("  Testing workflow compilation time...")
        compilation_times = []
        
        for i in range(100):  # 100 workflows for average
            start_time = time.time()
            
            request = EngineRequest(
                request_type="compile_workflow",
                payload={
                    "workflow_definition": {
                        "tasks": [f"task_{j}" for j in range(10)],  # 10 task workflow
                        "dependencies": {"task_1": ["task_0"], "task_2": ["task_1"]}
                    }
                }
            )
            
            response = await engine.process(request)
            compilation_time = (time.time() - start_time) * 1000  # ms
            compilation_times.append(compilation_time)
        
        avg_compilation = statistics.mean(compilation_times)
        p95_compilation = statistics.quantiles(compilation_times, n=20)[18]
        
        print(f"    Average compilation: {avg_compilation:.0f}ms")
        print(f"    P95 compilation: {p95_compilation:.0f}ms")
        print(f"    Target <2000ms: {'‚úÖ' if avg_compilation < 2000 else '‚ùå'}")
        
        self.results["workflow"] = {
            "avg_compilation_ms": avg_compilation,
            "p95_compilation_ms": p95_compilation,
            "compilation_target_met": avg_compilation < 2000
        }
    
    async def benchmark_data_processing_engine(self):
        """Benchmark DataProcessingEngine performance."""
        print("\nüìä Benchmarking DataProcessingEngine...")
        engine = self.engines["data_processing"]
        
        # Test semantic search latency (<50ms target)
        print("  Testing semantic search latency...")
        search_times = []
        
        for i in range(500):  # 500 searches
            start_time = time.time()
            
            request = EngineRequest(
                request_type="semantic_search",
                payload={
                    "query": f"test query {i}",
                    "limit": 10
                }
            )
            
            response = await engine.process(request)
            search_time = (time.time() - start_time) * 1000  # ms
            search_times.append(search_time)
        
        avg_search = statistics.mean(search_times)
        p95_search = statistics.quantiles(search_times, n=20)[18]
        
        print(f"    Average search: {avg_search:.1f}ms")
        print(f"    P95 search: {p95_search:.1f}ms")
        print(f"    Target <50ms: {'‚úÖ' if avg_search < 50 else '‚ùå'}")
        
        self.results["data_processing"] = {
            "avg_search_ms": avg_search,
            "p95_search_ms": p95_search,
            "search_target_met": avg_search < 50
        }
    
    async def benchmark_security_engine(self):
        """Benchmark SecurityEngine performance."""
        print("\nüìä Benchmarking SecurityEngine...")
        engine = self.engines["security"]
        
        # Test authorization decision latency (<5ms target)
        print("  Testing authorization decision latency...")
        auth_times = []
        
        for i in range(1000):  # 1000 auth decisions
            start_time = time.time()
            
            request = EngineRequest(
                request_type="authorize",
                payload={
                    "user_id": f"user_{i % 100}",
                    "resource": f"resource_{i % 50}",
                    "action": "read"
                }
            )
            
            response = await engine.process(request)
            auth_time = (time.time() - start_time) * 1000  # ms
            auth_times.append(auth_time)
        
        avg_auth = statistics.mean(auth_times)
        p95_auth = statistics.quantiles(auth_times, n=20)[18]
        
        print(f"    Average authorization: {avg_auth:.2f}ms")
        print(f"    P95 authorization: {p95_auth:.2f}ms")
        print(f"    Target <5ms: {'‚úÖ' if avg_auth < 5 else '‚ùå'}")
        
        self.results["security"] = {
            "avg_auth_ms": avg_auth,
            "p95_auth_ms": p95_auth,
            "auth_target_met": avg_auth < 5
        }
    
    async def benchmark_communication_engine(self):
        """Benchmark CommunicationEngine performance."""
        print("\nüìä Benchmarking CommunicationEngine...")
        engine = self.engines["communication"]
        
        # Test message routing latency (<10ms target)
        print("  Testing message routing latency...")
        routing_times = []
        
        for i in range(1000):  # 1000 messages
            start_time = time.time()
            
            request = EngineRequest(
                request_type="route_message",
                payload={
                    "from_agent": f"agent_{i % 10}",
                    "to_agent": f"agent_{(i + 1) % 10}",
                    "message": {"type": "test", "data": f"message_{i}"}
                }
            )
            
            response = await engine.process(request)
            routing_time = (time.time() - start_time) * 1000  # ms
            routing_times.append(routing_time)
        
        avg_routing = statistics.mean(routing_times)
        p95_routing = statistics.quantiles(routing_times, n=20)[18]
        
        print(f"    Average routing: {avg_routing:.2f}ms")
        print(f"    P95 routing: {p95_routing:.2f}ms")
        print(f"    Target <10ms: {'‚úÖ' if avg_routing < 10 else '‚ùå'}")
        
        # Test message throughput (10,000+ msg/sec target)
        print("  Testing message throughput...")
        message_count = 50000  # 50k messages for throughput test
        
        start_time = time.time()
        requests = [
            EngineRequest(
                request_type="route_message",
                payload={
                    "from_agent": "throughput_test",
                    "to_agent": "sink",
                    "message": {"data": i}
                }
            )
            for i in range(message_count)
        ]
        
        responses = await asyncio.gather(*[
            engine.process(request) for request in requests
        ])
        
        total_time = time.time() - start_time
        successful = sum(1 for r in responses if r.success)
        throughput = successful / total_time
        
        print(f"    Processed: {successful}/{message_count} messages")
        print(f"    Time: {total_time:.2f}s")
        print(f"    Throughput: {throughput:.0f} msg/sec")
        print(f"    Target 10,000+ msg/sec: {'‚úÖ' if throughput >= 10000 else '‚ùå'}")
        
        self.results["communication"] = {
            "avg_routing_ms": avg_routing,
            "p95_routing_ms": p95_routing,
            "throughput_msg_per_sec": throughput,
            "routing_target_met": avg_routing < 10,
            "throughput_target_met": throughput >= 10000
        }
    
    async def generate_performance_report(self):
        """Generate comprehensive performance report."""
        print("\n" + "="*80)
        print("üéØ ENGINE CONSOLIDATION PERFORMANCE REPORT")
        print("="*80)
        
        all_targets_met = True
        
        # TaskExecutionEngine Summary
        te_results = self.results.get("task_execution", {})
        print(f"\nüìã TaskExecutionEngine:")
        print(f"  Assignment Latency: {te_results.get('avg_latency_ms', 0):.2f}ms (target: <100ms) {'‚úÖ' if te_results.get('latency_target_met') else '‚ùå'}")
        print(f"  Concurrent Capacity: {te_results.get('concurrent_capacity', 0)} tasks (target: 1000+) {'‚úÖ' if te_results.get('capacity_target_met') else '‚ùå'}")
        print(f"  Throughput: {te_results.get('throughput_tasks_per_sec', 0):.0f} tasks/sec")
        
        if not te_results.get('latency_target_met') or not te_results.get('capacity_target_met'):
            all_targets_met = False
        
        # WorkflowEngine Summary
        wf_results = self.results.get("workflow", {})
        print(f"\n‚ö° WorkflowEngine:")
        print(f"  Compilation Time: {wf_results.get('avg_compilation_ms', 0):.0f}ms (target: <2000ms) {'‚úÖ' if wf_results.get('compilation_target_met') else '‚ùå'}")
        
        if not wf_results.get('compilation_target_met'):
            all_targets_met = False
        
        # DataProcessingEngine Summary
        dp_results = self.results.get("data_processing", {})
        print(f"\nüîç DataProcessingEngine:")
        print(f"  Search Latency: {dp_results.get('avg_search_ms', 0):.1f}ms (target: <50ms) {'‚úÖ' if dp_results.get('search_target_met') else '‚ùå'}")
        
        if not dp_results.get('search_target_met'):
            all_targets_met = False
        
        # SecurityEngine Summary
        sec_results = self.results.get("security", {})
        print(f"\nüîí SecurityEngine:")
        print(f"  Authorization Time: {sec_results.get('avg_auth_ms', 0):.2f}ms (target: <5ms) {'‚úÖ' if sec_results.get('auth_target_met') else '‚ùå'}")
        
        if not sec_results.get('auth_target_met'):
            all_targets_met = False
        
        # CommunicationEngine Summary
        comm_results = self.results.get("communication", {})
        print(f"\nüí¨ CommunicationEngine:")
        print(f"  Routing Latency: {comm_results.get('avg_routing_ms', 0):.2f}ms (target: <10ms) {'‚úÖ' if comm_results.get('routing_target_met') else '‚ùå'}")
        print(f"  Message Throughput: {comm_results.get('throughput_msg_per_sec', 0):.0f} msg/sec (target: 10,000+) {'‚úÖ' if comm_results.get('throughput_target_met') else '‚ùå'}")
        
        if not comm_results.get('routing_target_met') or not comm_results.get('throughput_target_met'):
            all_targets_met = False
        
        # Overall Assessment
        print(f"\nüèÜ OVERALL PERFORMANCE ASSESSMENT:")
        if all_targets_met:
            print("  ‚úÖ ALL PERFORMANCE TARGETS MET")
            print("  ‚úÖ 5x PERFORMANCE IMPROVEMENT ACHIEVED")
            print("  ‚úÖ READY FOR PRODUCTION DEPLOYMENT")
        else:
            print("  ‚ö†Ô∏è  Some performance targets not met")
            print("  ‚ö†Ô∏è  Additional optimization required")
        
        # Performance Improvement Summary
        print(f"\nüìà PERFORMANCE IMPROVEMENTS:")
        print(f"  ‚Ä¢ Task Assignment: 500ms ‚Üí {te_results.get('avg_latency_ms', 0):.2f}ms = {500/max(te_results.get('avg_latency_ms', 1), 0.01):.0f}x faster")
        print(f"  ‚Ä¢ Workflow Compilation: 10s ‚Üí {wf_results.get('avg_compilation_ms', 1000)/1000:.1f}s = {10/(wf_results.get('avg_compilation_ms', 1000)/1000):.0f}x faster") 
        print(f"  ‚Ä¢ Search Operations: 200ms ‚Üí {dp_results.get('avg_search_ms', 0):.1f}ms = {200/max(dp_results.get('avg_search_ms', 1), 0.1):.0f}x faster")
        print(f"  ‚Ä¢ Authorization: 20ms ‚Üí {sec_results.get('avg_auth_ms', 0):.2f}ms = {20/max(sec_results.get('avg_auth_ms', 1), 0.01):.0f}x faster")
        print(f"  ‚Ä¢ Message Routing: 50ms ‚Üí {comm_results.get('avg_routing_ms', 0):.2f}ms = {50/max(comm_results.get('avg_routing_ms', 1), 0.01):.0f}x faster")
        
        return all_targets_met
    
    async def cleanup_engines(self):
        """Cleanup all engines."""
        print("\nüßπ Cleaning up engines...")
        for name, engine in self.engines.items():
            await engine.shutdown()
            print(f"  ‚úÖ {name.replace('_', ' ').title()} Engine shut down")


async def main():
    """
    Run comprehensive engine benchmarks.
    
    REFACTORED: This function eliminates the duplicated async main() pattern
    by using the shared async_main_wrapper utility.
    """
    print("üöÄ Engine Consolidation Performance Benchmarks")
    print("="*80)
    
    benchmarks = EngineBenchmarks()
    
    try:
        await benchmarks.setup_engines()
        
        # Run all benchmarks
        await benchmarks.benchmark_task_execution_engine()
        await benchmarks.benchmark_workflow_engine()
        await benchmarks.benchmark_data_processing_engine()
        await benchmarks.benchmark_security_engine()
        await benchmarks.benchmark_communication_engine()
        
        # Generate final report
        all_targets_met = await benchmarks.generate_performance_report()
        
        # Exit with appropriate code
        return 0 if all_targets_met else 1
        
    except Exception as e:
        print(f"\n‚ùå Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        await benchmarks.cleanup_engines()


# REFACTORED: Import shared patterns to eliminate async main() duplication
sys.path.append(str(Path(__file__).parent.parent))
from app.common.utilities.shared_patterns import async_main_wrapper


if __name__ == "__main__":
    # REFACTORED: Use shared async_main_wrapper instead of duplicated pattern
    # This replaces: 
    #   import sys
    #   exit_code = asyncio.run(main())
    #   sys.exit(exit_code)
    async_main_wrapper(main, "engine_benchmarks")