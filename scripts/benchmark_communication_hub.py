"""
CommunicationHub Performance Benchmarking Script

This script validates that the CommunicationHub meets all performance targets:
- <10ms message routing latency
- 10,000+ messages/second throughput 
- Memory usage <100MB under load
- Error rate <0.1%
- Circuit breaker functionality
- Protocol adapter efficiency

Usage:
    python scripts/benchmark_communication_hub.py
    python scripts/benchmark_communication_hub.py --redis-host localhost --redis-port 6379
"""

import argparse
import asyncio
import gc
import psutil
import statistics
import time
from typing import List, Dict, Any
import json
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.communication_hub import (
    create_communication_hub, CommunicationHub, CommunicationConfig,
    UnifiedMessage, MessageType, Priority, DeliveryGuarantee,
    ProtocolType, ConnectionConfig, create_message
)


class CommunicationHubBenchmark:
    """Comprehensive benchmarking suite for CommunicationHub."""
    
    def __init__(self, redis_host: str = "localhost", redis_port: int = 6379,
                 websocket_host: str = "localhost", websocket_port: int = 8765):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.websocket_host = websocket_host
        self.websocket_port = websocket_port
        
        self.hub: CommunicationHub = None
        self.results: Dict[str, Any] = {}
        self.process = psutil.Process()
        
    async def setup_hub(self) -> bool:
        """Setup CommunicationHub for benchmarking."""
        try:
            print("Setting up CommunicationHub...")
            
            self.hub = create_communication_hub(
                redis_host=self.redis_host,
                redis_port=self.redis_port,
                websocket_host=self.websocket_host,
                websocket_port=self.websocket_port
            )
            
            success = await self.hub.initialize()
            if success:
                print("‚úÖ CommunicationHub initialized successfully")
                return True
            else:
                print("‚ùå CommunicationHub initialization failed")
                return False
                
        except Exception as e:
            print(f"‚ùå Setup failed: {e}")
            return False
    
    async def teardown_hub(self):
        """Cleanup CommunicationHub."""
        if self.hub:
            await self.hub.shutdown()
            self.hub = None
            print("üßπ CommunicationHub shutdown completed")
    
    async def benchmark_routing_latency(self, num_messages: int = 1000) -> Dict[str, float]:
        """Benchmark message routing latency."""
        print(f"\nüìä Benchmarking routing latency ({num_messages} messages)...")
        
        latencies = []
        successful_messages = 0
        
        for i in range(num_messages):
            message = create_message(
                source="benchmark_agent",
                destination="target_agent",
                message_type=MessageType.TASK_REQUEST,
                payload={"benchmark_id": i, "timestamp": time.time()}
            )
            
            start_time = time.time()
            result = await self.hub.send_message(message)
            end_time = time.time()
            
            if result.success:
                routing_latency = (end_time - start_time) * 1000  # Convert to ms
                latencies.append(routing_latency)
                successful_messages += 1
            
            # Progress indicator
            if (i + 1) % 100 == 0:
                print(f"  Progress: {i + 1}/{num_messages}")
        
        if not latencies:
            return {"error": "No successful messages"}
        
        # Calculate statistics
        avg_latency = statistics.mean(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        median_latency = statistics.median(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max_latency
        p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max_latency
        
        results = {
            "total_messages": num_messages,
            "successful_messages": successful_messages,
            "success_rate": successful_messages / num_messages,
            "avg_latency_ms": avg_latency,
            "min_latency_ms": min_latency,
            "max_latency_ms": max_latency,
            "median_latency_ms": median_latency,
            "p95_latency_ms": p95_latency,
            "p99_latency_ms": p99_latency
        }
        
        # Print results
        print(f"  Average latency: {avg_latency:.2f}ms")
        print(f"  Median latency: {median_latency:.2f}ms")
        print(f"  95th percentile: {p95_latency:.2f}ms")
        print(f"  99th percentile: {p99_latency:.2f}ms")
        print(f"  Min latency: {min_latency:.2f}ms")
        print(f"  Max latency: {max_latency:.2f}ms")
        print(f"  Success rate: {results['success_rate']:.1%}")
        
        # Validate performance targets
        if avg_latency < 10.0:
            print(f"  ‚úÖ Average latency target met (<10ms)")
        else:
            print(f"  ‚ùå Average latency target missed (>10ms)")
        
        if p95_latency < 10.0:
            print(f"  ‚úÖ 95th percentile latency target met (<10ms)")
        else:
            print(f"  ‚ùå 95th percentile latency target missed (>10ms)")
        
        return results
    
    async def benchmark_throughput(self, target_duration: float = 1.0, 
                                 concurrent_batches: int = 10) -> Dict[str, float]:
        """Benchmark message throughput."""
        print(f"\nüöÄ Benchmarking throughput (target: >10,000 msg/sec)...")
        
        async def send_batch(batch_size: int, batch_id: int) -> List[bool]:
            """Send a batch of messages."""
            results = []
            for i in range(batch_size):
                message = create_message(
                    source=f"batch_{batch_id}",
                    destination="throughput_target",
                    message_type=MessageType.AGENT_HEARTBEAT,
                    payload={"batch_id": batch_id, "msg_id": i}
                )
                
                result = await self.hub.send_message(message)
                results.append(result.success)
            
            return results
        
        # Calculate batch size to achieve target load
        messages_per_batch = 1000
        total_messages = concurrent_batches * messages_per_batch
        
        print(f"  Sending {total_messages} messages in {concurrent_batches} concurrent batches...")
        
        # Measure throughput
        start_time = time.time()
        
        # Create concurrent batch tasks
        batch_tasks = [
            send_batch(messages_per_batch, batch_id) 
            for batch_id in range(concurrent_batches)
        ]
        
        # Execute all batches concurrently
        batch_results = await asyncio.gather(*batch_tasks)
        
        end_time = time.time()
        
        # Calculate results
        duration = end_time - start_time
        total_successful = sum(sum(batch) for batch in batch_results)
        throughput = total_successful / duration
        
        results = {
            "total_messages": total_messages,
            "successful_messages": total_successful,
            "duration_seconds": duration,
            "throughput_msg_per_sec": throughput,
            "concurrent_batches": concurrent_batches,
            "messages_per_batch": messages_per_batch,
            "success_rate": total_successful / total_messages
        }
        
        # Print results
        print(f"  Total messages: {total_messages}")
        print(f"  Successful: {total_successful}")
        print(f"  Duration: {duration:.3f}s")
        print(f"  Throughput: {throughput:.0f} msg/sec")
        print(f"  Success rate: {results['success_rate']:.1%}")
        
        # Validate performance targets
        if throughput > 10000:
            print(f"  ‚úÖ Throughput target met (>10,000 msg/sec)")
        else:
            print(f"  ‚ùå Throughput target missed (<10,000 msg/sec)")
        
        return results
    
    async def benchmark_memory_usage(self, num_messages: int = 50000) -> Dict[str, float]:
        """Benchmark memory usage under load."""
        print(f"\nüíæ Benchmarking memory usage ({num_messages} messages)...")
        
        # Measure baseline memory
        gc.collect()
        baseline_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"  Baseline memory: {baseline_memory:.1f}MB")
        
        # Send messages and monitor memory
        memory_samples = []
        batch_size = 1000
        
        for batch in range(0, num_messages, batch_size):
            # Send batch of messages
            batch_end = min(batch + batch_size, num_messages)
            
            for i in range(batch, batch_end):
                message = create_message(
                    source="memory_test_agent",
                    destination="target_agent",
                    message_type=MessageType.TASK_REQUEST,
                    payload={
                        "data": "x" * 100,  # Add some payload data
                        "sequence": i
                    }
                )
                
                await self.hub.send_message(message)
            
            # Sample memory usage
            current_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            memory_samples.append(current_memory)
            
            if (batch + batch_size) % 10000 == 0:
                print(f"  Progress: {batch + batch_size}/{num_messages}, Memory: {current_memory:.1f}MB")
        
        # Calculate memory statistics
        peak_memory = max(memory_samples)
        avg_memory = statistics.mean(memory_samples)
        memory_increase = peak_memory - baseline_memory
        
        # Force garbage collection and measure final memory
        gc.collect()
        await asyncio.sleep(0.1)  # Allow cleanup
        final_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        results = {
            "baseline_memory_mb": baseline_memory,
            "peak_memory_mb": peak_memory,
            "avg_memory_mb": avg_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": memory_increase,
            "messages_processed": num_messages
        }
        
        # Print results
        print(f"  Peak memory: {peak_memory:.1f}MB")
        print(f"  Average memory: {avg_memory:.1f}MB")
        print(f"  Memory increase: {memory_increase:.1f}MB")
        print(f"  Final memory: {final_memory:.1f}MB")
        
        # Validate memory targets
        if peak_memory < 100:
            print(f"  ‚úÖ Memory usage target met (<100MB)")
        else:
            print(f"  ‚ùå Memory usage target exceeded (>100MB)")
        
        return results
    
    async def benchmark_error_resilience(self, num_messages: int = 1000,
                                       error_rate: float = 0.05) -> Dict[str, float]:
        """Benchmark error handling and resilience."""
        print(f"\nüõ°Ô∏è Benchmarking error resilience ({error_rate:.1%} simulated error rate)...")
        
        # Note: This would require mock adapters or actual Redis/WebSocket errors
        # For now, we'll test the hub's behavior with real connections
        
        successful_messages = 0
        failed_messages = 0
        latencies = []
        
        for i in range(num_messages):
            message = create_message(
                source="resilience_test_agent",
                destination="target_agent",
                message_type=MessageType.TASK_REQUEST,
                payload={"test_id": i}
            )
            
            start_time = time.time()
            result = await self.hub.send_message(message)
            end_time = time.time()
            
            if result.success:
                successful_messages += 1
                latency = (end_time - start_time) * 1000
                latencies.append(latency)
            else:
                failed_messages += 1
            
            if (i + 1) % 200 == 0:
                print(f"  Progress: {i + 1}/{num_messages}")
        
        # Calculate results
        actual_error_rate = failed_messages / num_messages
        avg_latency = statistics.mean(latencies) if latencies else 0
        
        results = {
            "total_messages": num_messages,
            "successful_messages": successful_messages,
            "failed_messages": failed_messages,
            "error_rate": actual_error_rate,
            "avg_latency_ms": avg_latency,
            "target_error_rate": 0.001  # 0.1% target
        }
        
        # Print results
        print(f"  Successful: {successful_messages}")
        print(f"  Failed: {failed_messages}")
        print(f"  Error rate: {actual_error_rate:.2%}")
        print(f"  Average latency: {avg_latency:.2f}ms")
        
        # Validate error rate targets
        if actual_error_rate < 0.001:
            print(f"  ‚úÖ Error rate target met (<0.1%)")
        else:
            print(f"  ‚ùå Error rate target missed (>{actual_error_rate:.2%})")
        
        return results
    
    async def benchmark_protocol_adapters(self) -> Dict[str, Dict[str, Any]]:
        """Benchmark individual protocol adapter performance."""
        print(f"\nüîå Benchmarking protocol adapters...")
        
        adapter_results = {}
        
        # Get adapter metrics
        try:
            adapter_metrics = await self.hub.adapter_registry.get_all_metrics()
            
            for protocol, metrics in adapter_metrics.items():
                adapter_results[protocol.value] = {
                    "messages_sent": metrics.messages_sent,
                    "messages_received": metrics.messages_received,
                    "messages_failed": metrics.messages_failed,
                    "average_latency_ms": metrics.average_latency_ms,
                    "connection_count": metrics.connection_count,
                    "active_subscriptions": metrics.active_subscriptions,
                    "uptime_seconds": metrics.uptime_seconds,
                    "error_rate": metrics.error_rate
                }
                
                print(f"  {protocol.value}:")
                print(f"    Messages sent: {metrics.messages_sent}")
                print(f"    Avg latency: {metrics.average_latency_ms:.2f}ms")
                print(f"    Error rate: {metrics.error_rate:.2%}")
                print(f"    Connections: {metrics.connection_count}")
        
        except Exception as e:
            print(f"  ‚ùå Error getting adapter metrics: {e}")
            adapter_results["error"] = str(e)
        
        return adapter_results
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Run all benchmarks and return comprehensive results."""
        print("üî• Starting CommunicationHub Comprehensive Benchmark")
        print("=" * 60)
        
        # Setup
        if not await self.setup_hub():
            return {"error": "Failed to setup CommunicationHub"}
        
        try:
            # Run all benchmarks
            benchmark_results = {}
            
            # 1. Routing Latency
            benchmark_results["routing_latency"] = await self.benchmark_routing_latency(1000)
            
            # 2. Throughput
            benchmark_results["throughput"] = await self.benchmark_throughput(1.0, 20)
            
            # 3. Memory Usage
            benchmark_results["memory_usage"] = await self.benchmark_memory_usage(10000)
            
            # 4. Error Resilience
            benchmark_results["error_resilience"] = await self.benchmark_error_resilience(1000)
            
            # 5. Protocol Adapters
            benchmark_results["protocol_adapters"] = await self.benchmark_protocol_adapters()
            
            # 6. Overall Health
            health_status = await self.hub.get_health_status()
            benchmark_results["health_status"] = health_status
            
            # 7. Detailed Metrics
            detailed_metrics = await self.hub.get_detailed_metrics()
            benchmark_results["detailed_metrics"] = {
                "total_messages_sent": detailed_metrics.total_messages_sent,
                "total_messages_received": detailed_metrics.total_messages_received,
                "total_messages_failed": detailed_metrics.total_messages_failed,
                "average_routing_latency_ms": detailed_metrics.average_routing_latency_ms,
                "messages_per_second": detailed_metrics.messages_per_second,
                "active_connections": detailed_metrics.active_connections,
                "active_subscriptions": detailed_metrics.active_subscriptions,
                "uptime_seconds": detailed_metrics.uptime_seconds
            }
            
            # Summary
            self._print_benchmark_summary(benchmark_results)
            
            return benchmark_results
            
        except Exception as e:
            print(f"‚ùå Benchmark failed: {e}")
            return {"error": str(e)}
        
        finally:
            await self.teardown_hub()
    
    def _print_benchmark_summary(self, results: Dict[str, Any]):
        """Print a summary of benchmark results."""
        print("\n" + "=" * 60)
        print("üìä BENCHMARK SUMMARY")
        print("=" * 60)
        
        # Performance targets check
        targets_met = 0
        total_targets = 5
        
        if "routing_latency" in results:
            latency_data = results["routing_latency"]
            if isinstance(latency_data, dict) and "avg_latency_ms" in latency_data:
                if latency_data["avg_latency_ms"] < 10.0:
                    print("‚úÖ Routing Latency: <10ms target MET")
                    targets_met += 1
                else:
                    print(f"‚ùå Routing Latency: {latency_data['avg_latency_ms']:.2f}ms (target: <10ms)")
        
        if "throughput" in results:
            throughput_data = results["throughput"]
            if isinstance(throughput_data, dict) and "throughput_msg_per_sec" in throughput_data:
                if throughput_data["throughput_msg_per_sec"] > 10000:
                    print("‚úÖ Throughput: >10,000 msg/sec target MET")
                    targets_met += 1
                else:
                    print(f"‚ùå Throughput: {throughput_data['throughput_msg_per_sec']:.0f} msg/sec (target: >10,000)")
        
        if "memory_usage" in results:
            memory_data = results["memory_usage"]
            if isinstance(memory_data, dict) and "peak_memory_mb" in memory_data:
                if memory_data["peak_memory_mb"] < 100:
                    print("‚úÖ Memory Usage: <100MB target MET")
                    targets_met += 1
                else:
                    print(f"‚ùå Memory Usage: {memory_data['peak_memory_mb']:.1f}MB (target: <100MB)")
        
        if "error_resilience" in results:
            error_data = results["error_resilience"]
            if isinstance(error_data, dict) and "error_rate" in error_data:
                if error_data["error_rate"] < 0.001:
                    print("‚úÖ Error Rate: <0.1% target MET")
                    targets_met += 1
                else:
                    print(f"‚ùå Error Rate: {error_data['error_rate']:.2%} (target: <0.1%)")
        
        # Overall hub health
        if "health_status" in results:
            health = results["health_status"]
            if isinstance(health, dict) and health.get("hub_status") == "healthy":
                print("‚úÖ Hub Health: HEALTHY")
                targets_met += 1
            else:
                print("‚ùå Hub Health: UNHEALTHY")
        
        print("-" * 60)
        print(f"OVERALL SCORE: {targets_met}/{total_targets} targets met ({targets_met/total_targets:.1%})")
        
        if targets_met == total_targets:
            print("üéâ ALL PERFORMANCE TARGETS MET! üéâ")
        elif targets_met >= total_targets * 0.8:
            print("üü° Most performance targets met")
        else:
            print("üî¥ Performance improvements needed")
        
        print("=" * 60)


async def main():
    """Main benchmark execution."""
    parser = argparse.ArgumentParser(description="CommunicationHub Performance Benchmark")
    parser.add_argument("--redis-host", default="localhost", help="Redis host")
    parser.add_argument("--redis-port", type=int, default=6379, help="Redis port")
    parser.add_argument("--websocket-host", default="localhost", help="WebSocket host")
    parser.add_argument("--websocket-port", type=int, default=8765, help="WebSocket port")
    parser.add_argument("--output", help="Output file for results (JSON)")
    
    args = parser.parse_args()
    
    # Create and run benchmark
    benchmark = CommunicationHubBenchmark(
        redis_host=args.redis_host,
        redis_port=args.redis_port,
        websocket_host=args.websocket_host,
        websocket_port=args.websocket_port
    )
    
    results = await benchmark.run_comprehensive_benchmark()
    
    # Save results if output file specified
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nüíæ Results saved to {args.output}")
    
    return results


if __name__ == "__main__":
    asyncio.run(main())