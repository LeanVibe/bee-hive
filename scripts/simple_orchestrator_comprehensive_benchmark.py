#!/usr/bin/env python3
"""
Comprehensive Performance Benchmarking Suite for SimpleOrchestrator

Epic 6 Phase 3B: Since SimpleOrchestrator is the only working component,
this script provides comprehensive benchmarking to establish real performance
baselines and measurement methodology.

Based on evidence from baseline testing:
- SimpleOrchestrator initialization: 0.1ms (VERIFIED)
- Memory footprint: 112.4MB (VERIFIED) 
- Component status: WORKING (VERIFIED)

This script will establish comprehensive performance metrics for the one
component we can actually measure, replacing unsupported claims with evidence.
"""

import asyncio
import json
import time
import psutil
import statistics
import sys
import os
import gc
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

@dataclass
class BenchmarkResult:
    """Container for individual benchmark results."""
    name: str
    duration_ms: float
    success: bool
    metrics: Dict[str, Any]
    errors: List[str]
    methodology: str

class SimpleOrchestratorBenchmarkSuite:
    """Comprehensive performance benchmark suite for SimpleOrchestrator."""
    
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'component': 'SimpleOrchestrator',
            'purpose': 'Epic 6 Phase 3B: Establish comprehensive baselines for working component',
            'system_info': self._get_system_info(),
            'benchmarks': {},
            'summary': {},
            'evidence_based': True
        }
    
    def _get_system_info(self) -> Dict[str, Any]:
        """Get detailed system information for benchmark context."""
        try:
            return {
                'cpu_count_physical': psutil.cpu_count(logical=False),
                'cpu_count_logical': psutil.cpu_count(logical=True),
                'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2),
                'platform': sys.platform,
                'python_version': sys.version.split()[0],
                'architecture': os.uname().machine if hasattr(os, 'uname') else 'unknown'
            }
        except Exception as e:
            return {'error': f'System info collection failed: {str(e)}'}
    
    def _get_process_memory_mb(self) -> float:
        """Get current process memory usage in MB."""
        try:
            return psutil.Process().memory_info().rss / (1024 * 1024)
        except:
            return 0.0
    
    async def benchmark_initialization_performance(self, iterations: int = 100) -> BenchmarkResult:
        """Benchmark SimpleOrchestrator initialization performance comprehensively."""
        print(f"üìä Benchmarking initialization performance ({iterations} iterations)...")
        
        errors = []
        init_times = []
        memory_usages = []
        
        try:
            from app.core.simple_orchestrator import SimpleOrchestrator
            
            # Force garbage collection for clean baseline
            gc.collect()
            baseline_memory = self._get_process_memory_mb()
            
            # Run multiple initialization tests
            for i in range(iterations):
                try:
                    start_time = time.perf_counter()
                    orchestrator = SimpleOrchestrator()
                    init_time = (time.perf_counter() - start_time) * 1000  # Convert to ms
                    init_times.append(init_time)
                    
                    # Measure memory impact
                    current_memory = self._get_process_memory_mb()
                    memory_usages.append(current_memory - baseline_memory)
                    
                    # Clean up orchestrator reference
                    del orchestrator
                    
                    # Periodic garbage collection to prevent memory buildup
                    if i % 10 == 0:
                        gc.collect()
                        
                except Exception as e:
                    errors.append(f"Iteration {i}: {str(e)}")
            
            # Force final garbage collection
            gc.collect()
            final_memory = self._get_process_memory_mb()
            
            if init_times:
                metrics = {
                    'iterations_completed': len(init_times),
                    'iterations_failed': len(errors),
                    'average_init_time_ms': round(statistics.mean(init_times), 4),
                    'median_init_time_ms': round(statistics.median(init_times), 4),
                    'min_init_time_ms': round(min(init_times), 4),
                    'max_init_time_ms': round(max(init_times), 4),
                    'std_dev_init_time_ms': round(statistics.stdev(init_times), 4) if len(init_times) > 1 else 0,
                    'p95_init_time_ms': round(sorted(init_times)[int(len(init_times) * 0.95)], 4),
                    'p99_init_time_ms': round(sorted(init_times)[int(len(init_times) * 0.99)], 4),
                    'baseline_memory_mb': round(baseline_memory, 2),
                    'final_memory_mb': round(final_memory, 2),
                    'memory_delta_mb': round(final_memory - baseline_memory, 2),
                    'average_memory_per_instance_mb': round(statistics.mean(memory_usages), 4) if memory_usages else 0,
                    'total_test_duration_ms': round(sum(init_times), 2)
                }
                
                return BenchmarkResult(
                    name="Initialization Performance",
                    duration_ms=sum(init_times),
                    success=len(errors) < iterations * 0.05,  # Allow 5% failure rate
                    metrics=metrics,
                    errors=errors[:10],  # Keep first 10 errors
                    methodology=f"Direct measurement of SimpleOrchestrator() constructor over {iterations} iterations with memory tracking"
                )
            else:
                return BenchmarkResult(
                    name="Initialization Performance",
                    duration_ms=0,
                    success=False,
                    metrics={'error': 'No successful initializations'},
                    errors=errors[:10],
                    methodology=f"Attempted {iterations} initializations"
                )
                
        except Exception as e:
            return BenchmarkResult(
                name="Initialization Performance", 
                duration_ms=0,
                success=False,
                metrics={'error': str(e)},
                errors=[f"Benchmark setup failed: {str(e)}"],
                methodology="Failed to import SimpleOrchestrator"
            )
    
    async def benchmark_method_performance(self) -> BenchmarkResult:
        """Benchmark available methods on SimpleOrchestrator."""
        print("üìä Benchmarking available methods...")
        
        try:
            from app.core.simple_orchestrator import SimpleOrchestrator
            
            orchestrator = SimpleOrchestrator()
            method_results = {}
            errors = []
            
            # Test available methods
            test_methods = [
                ('get_status', lambda: orchestrator.get_status() if hasattr(orchestrator, 'get_status') else None),
                ('str_representation', lambda: str(orchestrator)),
                ('repr_representation', lambda: repr(orchestrator)),
                ('hash_computation', lambda: hash(orchestrator) if orchestrator.__hash__ else None),
                ('attribute_access', lambda: getattr(orchestrator, '__dict__', None)),
            ]
            
            for method_name, method_call in test_methods:
                method_times = []
                method_errors = []
                
                # Test each method multiple times
                for i in range(20):
                    try:
                        start_time = time.perf_counter()
                        result = method_call()
                        method_time = (time.perf_counter() - start_time) * 1000
                        method_times.append(method_time)
                    except Exception as e:
                        method_errors.append(f"{method_name}[{i}]: {str(e)}")
                
                if method_times:
                    method_results[method_name] = {
                        'available': True,
                        'average_time_ms': round(statistics.mean(method_times), 4),
                        'min_time_ms': round(min(method_times), 4),
                        'max_time_ms': round(max(method_times), 4),
                        'successful_calls': len(method_times),
                        'failed_calls': len(method_errors)
                    }
                else:
                    method_results[method_name] = {
                        'available': False,
                        'errors': method_errors[:3]
                    }
                
                errors.extend(method_errors[:2])  # Keep sample errors
            
            # Overall metrics
            available_methods = sum(1 for result in method_results.values() if result.get('available', False))
            total_methods = len(method_results)
            
            return BenchmarkResult(
                name="Method Performance",
                duration_ms=sum(result.get('average_time_ms', 0) for result in method_results.values()),
                success=available_methods > 0,
                metrics={
                    'methods_tested': total_methods,
                    'methods_available': available_methods,
                    'availability_rate_percent': round((available_methods / total_methods) * 100, 1),
                    'method_details': method_results
                },
                errors=errors[:10],
                methodology="Direct method invocation timing for available SimpleOrchestrator methods"
            )
            
        except Exception as e:
            return BenchmarkResult(
                name="Method Performance",
                duration_ms=0,
                success=False,
                metrics={'error': str(e)},
                errors=[f"Method testing failed: {str(e)}"],
                methodology="Failed to create SimpleOrchestrator instance"
            )
    
    async def benchmark_memory_behavior(self, instance_count: int = 50) -> BenchmarkResult:
        """Benchmark memory behavior under multiple instances."""
        print(f"üìä Benchmarking memory behavior ({instance_count} instances)...")
        
        try:
            from app.core.simple_orchestrator import SimpleOrchestrator
            
            # Baseline measurement
            gc.collect()
            baseline_memory = self._get_process_memory_mb()
            
            orchestrators = []
            memory_measurements = []
            creation_times = []
            errors = []
            
            # Create instances and measure memory growth
            for i in range(instance_count):
                try:
                    start_time = time.perf_counter()
                    orchestrator = SimpleOrchestrator()
                    creation_time = (time.perf_counter() - start_time) * 1000
                    
                    orchestrators.append(orchestrator)
                    creation_times.append(creation_time)
                    
                    # Memory measurement every 10 instances
                    if (i + 1) % 10 == 0:
                        current_memory = self._get_process_memory_mb()
                        memory_measurements.append({
                            'instance_count': i + 1,
                            'memory_mb': current_memory,
                            'memory_increase_mb': current_memory - baseline_memory
                        })
                
                except Exception as e:
                    errors.append(f"Instance {i}: {str(e)}")
            
            # Final memory measurement
            gc.collect()
            final_memory = self._get_process_memory_mb()
            
            # Cleanup
            orchestrators.clear()
            gc.collect()
            cleanup_memory = self._get_process_memory_mb()
            
            if creation_times and memory_measurements:
                total_memory_increase = final_memory - baseline_memory
                memory_per_instance = total_memory_increase / len(orchestrators) if orchestrators else 0
                
                return BenchmarkResult(
                    name="Memory Behavior",
                    duration_ms=sum(creation_times),
                    success=len(errors) < instance_count * 0.1,  # Allow 10% failure rate
                    metrics={
                        'instances_created': len(creation_times),
                        'instances_failed': len(errors),
                        'baseline_memory_mb': round(baseline_memory, 2),
                        'peak_memory_mb': round(final_memory, 2),
                        'cleanup_memory_mb': round(cleanup_memory, 2),
                        'total_memory_increase_mb': round(total_memory_increase, 2),
                        'memory_per_instance_kb': round(memory_per_instance * 1024, 2),
                        'memory_measurements': memory_measurements,
                        'average_creation_time_ms': round(statistics.mean(creation_times), 4),
                        'memory_leak_indicator_mb': round(cleanup_memory - baseline_memory, 2)
                    },
                    errors=errors[:5],
                    methodology=f"Created {instance_count} SimpleOrchestrator instances with memory tracking every 10 instances"
                )
            else:
                return BenchmarkResult(
                    name="Memory Behavior",
                    duration_ms=0,
                    success=False,
                    metrics={'error': 'No successful instance creation'},
                    errors=errors[:10],
                    methodology=f"Attempted to create {instance_count} instances"
                )
                
        except Exception as e:
            return BenchmarkResult(
                name="Memory Behavior",
                duration_ms=0,
                success=False,
                metrics={'error': str(e)},
                errors=[f"Memory benchmark failed: {str(e)}"],
                methodology="Failed to import SimpleOrchestrator"
            )
    
    async def benchmark_concurrent_creation(self, concurrency: int = 20) -> BenchmarkResult:
        """Benchmark concurrent SimpleOrchestrator creation."""
        print(f"üìä Benchmarking concurrent creation ({concurrency} concurrent)...")
        
        try:
            from app.core.simple_orchestrator import SimpleOrchestrator
            
            async def create_orchestrator(index: int) -> Dict[str, Any]:
                """Create a single orchestrator and measure performance."""
                try:
                    start_time = time.perf_counter()
                    orchestrator = SimpleOrchestrator()
                    creation_time = (time.perf_counter() - start_time) * 1000
                    
                    return {
                        'index': index,
                        'success': True,
                        'creation_time_ms': creation_time,
                        'memory_mb': self._get_process_memory_mb()
                    }
                except Exception as e:
                    return {
                        'index': index,
                        'success': False,
                        'error': str(e),
                        'creation_time_ms': 0
                    }
            
            # Execute concurrent creation
            start_time = time.perf_counter()
            tasks = [create_orchestrator(i) for i in range(concurrency)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_time = (time.perf_counter() - start_time) * 1000
            
            # Analyze results
            successful_creations = []
            failed_creations = []
            creation_times = []
            
            for result in results:
                if isinstance(result, Exception):
                    failed_creations.append(str(result))
                elif result.get('success', False):
                    successful_creations.append(result)
                    creation_times.append(result['creation_time_ms'])
                else:
                    failed_creations.append(result.get('error', 'Unknown error'))
            
            if creation_times:
                return BenchmarkResult(
                    name="Concurrent Creation",
                    duration_ms=total_time,
                    success=len(successful_creations) >= concurrency * 0.8,  # 80% success rate required
                    metrics={
                        'concurrency_level': concurrency,
                        'successful_creations': len(successful_creations),
                        'failed_creations': len(failed_creations),
                        'success_rate_percent': round((len(successful_creations) / concurrency) * 100, 1),
                        'total_time_ms': round(total_time, 2),
                        'average_creation_time_ms': round(statistics.mean(creation_times), 4),
                        'max_creation_time_ms': round(max(creation_times), 4),
                        'min_creation_time_ms': round(min(creation_times), 4),
                        'creations_per_second': round(len(successful_creations) / (total_time / 1000), 2),
                        'concurrent_overhead_ms': round(total_time / len(successful_creations), 4) if successful_creations else 0
                    },
                    errors=failed_creations[:5],
                    methodology=f"Concurrent creation of {concurrency} SimpleOrchestrator instances using asyncio.gather"
                )
            else:
                return BenchmarkResult(
                    name="Concurrent Creation",
                    duration_ms=total_time,
                    success=False,
                    metrics={'error': 'No successful concurrent creations'},
                    errors=failed_creations[:10],
                    methodology=f"Attempted {concurrency} concurrent creations"
                )
                
        except Exception as e:
            return BenchmarkResult(
                name="Concurrent Creation",
                duration_ms=0,
                success=False,
                metrics={'error': str(e)},
                errors=[f"Concurrent benchmark failed: {str(e)}"],
                methodology="Failed to setup concurrent creation test"
            )
    
    async def run_comprehensive_benchmarks(self) -> Dict[str, Any]:
        """Run all benchmarks and generate comprehensive report."""
        print("üéØ SimpleOrchestrator Comprehensive Performance Benchmarking")
        print("=" * 70)
        print("MISSION: Establish comprehensive baselines for the only working component")
        print("=" * 70)
        
        benchmarks = [
            self.benchmark_initialization_performance(),
            self.benchmark_method_performance(), 
            self.benchmark_memory_behavior(),
            self.benchmark_concurrent_creation()
        ]
        
        for benchmark_coro in benchmarks:
            benchmark_result = await benchmark_coro
            self.results['benchmarks'][benchmark_result.name] = {
                'success': benchmark_result.success,
                'duration_ms': benchmark_result.duration_ms,
                'metrics': benchmark_result.metrics,
                'errors': benchmark_result.errors,
                'methodology': benchmark_result.methodology
            }
            
            # Print immediate feedback
            status = "‚úÖ PASSED" if benchmark_result.success else "‚ùå FAILED"
            print(f"{status} {benchmark_result.name} ({benchmark_result.duration_ms:.1f}ms)")
        
        # Generate summary
        successful_benchmarks = sum(1 for b in self.results['benchmarks'].values() if b['success'])
        total_benchmarks = len(self.results['benchmarks'])
        
        self.results['summary'] = {
            'total_benchmarks': total_benchmarks,
            'successful_benchmarks': successful_benchmarks,
            'success_rate_percent': round((successful_benchmarks / total_benchmarks) * 100, 1),
            'component_status': 'BENCHMARKED' if successful_benchmarks > 0 else 'FAILED',
            'evidence_based_metrics': successful_benchmarks > 0,
            'baseline_established': successful_benchmarks >= 3  # At least 3 successful benchmarks
        }
        
        return self.results
    
    def save_results(self, output_file: str):
        """Save comprehensive results to JSON file."""
        try:
            with open(output_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            print(f"üìÅ Comprehensive results saved to: {output_file}")
        except Exception as e:
            print(f"‚ùå Failed to save results: {e}")
    
    def print_comprehensive_summary(self):
        """Print detailed summary of all benchmarks."""
        print("\n" + "=" * 70)
        print("üéØ COMPREHENSIVE PERFORMANCE BASELINE SUMMARY")  
        print("=" * 70)
        
        summary = self.results['summary']
        
        if summary['baseline_established']:
            print(f"‚úÖ SUCCESS: Comprehensive baselines established for SimpleOrchestrator")
            print(f"üìä Benchmarks: {summary['successful_benchmarks']}/{summary['total_benchmarks']} passed ({summary['success_rate_percent']:.1f}%)")
            
            # Show key metrics from each benchmark
            for name, benchmark in self.results['benchmarks'].items():
                if benchmark['success']:
                    print(f"\nüìà {name}:")
                    metrics = benchmark['metrics']
                    
                    if 'average_init_time_ms' in metrics:
                        print(f"   ‚Ä¢ Average Init Time: {metrics['average_init_time_ms']}ms")
                        print(f"   ‚Ä¢ P95 Init Time: {metrics['p95_init_time_ms']}ms")
                    
                    if 'methods_available' in metrics:
                        print(f"   ‚Ä¢ Available Methods: {metrics['methods_available']}/{metrics['methods_tested']}")
                    
                    if 'memory_per_instance_kb' in metrics:
                        print(f"   ‚Ä¢ Memory per Instance: {metrics['memory_per_instance_kb']}KB")
                    
                    if 'success_rate_percent' in metrics:
                        print(f"   ‚Ä¢ Success Rate: {metrics['success_rate_percent']}%")
        else:
            print("‚ùå FAILED: Could not establish comprehensive baselines")
            
        print(f"\nüéØ Component Status: {summary['component_status']}")
        print(f"üìä Evidence-Based Metrics: {summary['evidence_based_metrics']}")


async def main():
    """Main execution function."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Comprehensive SimpleOrchestrator Performance Benchmarking')
    parser.add_argument('--output', '-o', default='simple_orchestrator_comprehensive_benchmark.json',
                        help='Output file for comprehensive benchmark results')
    args = parser.parse_args()
    
    # Run comprehensive benchmarks
    benchmark_suite = SimpleOrchestratorBenchmarkSuite()
    results = await benchmark_suite.run_comprehensive_benchmarks()
    
    # Print comprehensive summary
    benchmark_suite.print_comprehensive_summary()
    
    # Save results
    benchmark_suite.save_results(args.output)
    
    # Exit with appropriate code
    if results['summary']['baseline_established']:
        print("\nüöÄ Phase 3B Complete: Comprehensive baselines established for SimpleOrchestrator!")
        print("Ready for production deployment with evidence-based performance documentation.")
        sys.exit(0)
    else:
        print("\n‚ùå Phase 3B Failed: Could not establish comprehensive baselines.")  
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())