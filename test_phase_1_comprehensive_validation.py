#!/usr/bin/env python3
"""
Phase 1 Comprehensive Validation Suite
LeanVibe Agent Hive 2.0 - Complete System Integration Testing

Tests multi-agent coordination, dashboard integration, and all Phase 1 objectives.
"""

import asyncio
import json
import time
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import logging

import redis.asyncio as redis


@dataclass
class Phase1ValidationResults:
    """Comprehensive Phase 1 validation results."""
    timestamp: str
    test_suite_version: str = "1.0"
    overall_success: bool = False
    
    # Test categories
    infrastructure_tests: Dict[str, bool] = None
    redis_streams_tests: Dict[str, Dict[str, Any]] = None
    dashboard_integration_tests: Dict[str, bool] = None
    multi_agent_coordination_tests: Dict[str, Dict[str, Any]] = None
    performance_benchmarks: Dict[str, float] = None
    
    # Summary
    total_test_categories: int = 5
    passed_categories: int = 0
    critical_issues: List[str] = None
    recommendations: List[str] = None
    phase_1_readiness: str = "NOT_READY"

    def __post_init__(self):
        if self.infrastructure_tests is None:
            self.infrastructure_tests = {}
        if self.redis_streams_tests is None:
            self.redis_streams_tests = {}
        if self.dashboard_integration_tests is None:
            self.dashboard_integration_tests = {}
        if self.multi_agent_coordination_tests is None:
            self.multi_agent_coordination_tests = {}
        if self.performance_benchmarks is None:
            self.performance_benchmarks = {}
        if self.critical_issues is None:
            self.critical_issues = []
        if self.recommendations is None:
            self.recommendations = []


class Phase1ComprehensiveValidator:
    """Comprehensive Phase 1 system validation."""

    def __init__(self):
        self.logger = self._setup_logging()
        self.results = Phase1ValidationResults(
            timestamp=datetime.now().isoformat()
        )
        
        # Performance thresholds for Phase 1
        self.performance_thresholds = {
            "redis_basic_latency_ms": 10,
            "redis_streams_latency_ms": 50,
            "message_throughput_per_second": 100,
            "multi_agent_coordination_time_ms": 2000,
            "dashboard_update_latency_ms": 200
        }

    def _setup_logging(self) -> logging.Logger:
        """Set up structured logging."""
        logger = logging.getLogger("phase1_comprehensive_validator")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    async def run_comprehensive_validation(self) -> Phase1ValidationResults:
        """Execute comprehensive Phase 1 validation suite."""
        self.logger.info("üöÄ Starting comprehensive Phase 1 validation...")

        # Test categories
        test_categories = [
            ("Infrastructure Components", self._test_infrastructure_components),
            ("Redis Streams Communication", self._test_redis_streams_communication),
            ("Dashboard Integration", self._test_dashboard_integration),
            ("Multi-Agent Coordination", self._test_multi_agent_coordination),
            ("Performance Benchmarking", self._test_performance_benchmarking)
        ]

        for category_name, test_func in test_categories:
            self.logger.info(f"üß™ Testing: {category_name}")
            
            try:
                success = await test_func()
                if success:
                    self.results.passed_categories += 1
                    self.logger.info(f"‚úÖ {category_name}: PASSED")
                else:
                    self.logger.error(f"‚ùå {category_name}: FAILED")
                    
            except Exception as e:
                self.logger.error(f"üí• {category_name}: ERROR - {e}")

        # Calculate overall success and phase readiness
        await self._calculate_phase_1_readiness()
        
        self.logger.info(
            f"üìä Validation complete: {self.results.passed_categories}/"
            f"{self.results.total_test_categories} categories passed"
        )
        
        return self.results

    async def _test_infrastructure_components(self) -> bool:
        """Test core infrastructure components."""
        self.logger.info("üèóÔ∏è Testing infrastructure components...")
        
        try:
            # PostgreSQL with pgvector
            result = subprocess.run(
                ["docker", "exec", "leanvibe_postgres", "pg_isready", "-U", "leanvibe_user"],
                capture_output=True, text=True, timeout=10
            )
            postgres_ok = result.returncode == 0
            self.results.infrastructure_tests["postgres"] = postgres_ok
            
            # Test pgvector extension
            result = subprocess.run([
                "docker", "exec", "leanvibe_postgres", "psql", "-U", "leanvibe_user", 
                "-d", "leanvibe_agent_hive", "-c", "SELECT 1 FROM pg_extension WHERE extname='vector';"
            ], capture_output=True, text=True, timeout=10)
            pgvector_ok = "1" in result.stdout
            self.results.infrastructure_tests["pgvector"] = pgvector_ok
            
            # Redis with authentication
            result = subprocess.run([
                "docker", "exec", "leanvibe_redis", "redis-cli", "-a", "leanvibe_redis_pass", "ping"
            ], capture_output=True, text=True, timeout=10)
            redis_ok = "PONG" in result.stdout
            self.results.infrastructure_tests["redis"] = redis_ok
            
            # Docker Compose services health
            result = subprocess.run(
                ["docker-compose", "ps", "--services", "--filter", "status=running"],
                capture_output=True, text=True, timeout=10
            )
            running_services = result.stdout.strip().split('\n') if result.stdout.strip() else []
            required_services = {"postgres", "redis"}
            services_ok = required_services.issubset(set(running_services))
            self.results.infrastructure_tests["docker_services"] = services_ok
            
            return postgres_ok and pgvector_ok and redis_ok and services_ok
            
        except Exception as e:
            self.logger.error(f"Infrastructure test failed: {e}")
            return False

    async def _test_redis_streams_communication(self) -> bool:
        """Test Redis Streams for multi-agent communication."""
        self.logger.info("üì° Testing Redis Streams communication...")
        
        try:
            redis_client = redis.Redis.from_url("redis://:leanvibe_redis_pass@localhost:6380/0")
            
            # Test 1: Basic Stream Operations
            start_time = time.time()
            stream_name = "test_agent_messages:coordination_test"
            
            # Add multiple messages to simulate agent communication
            messages = []
            for i in range(5):
                message_data = {
                    "agent_id": f"test_agent_{i}",
                    "message_type": "coordination_request",
                    "content": f"Test message {i}",
                    "timestamp": datetime.now().isoformat()
                }
                message_id = await redis_client.xadd(stream_name, message_data)
                messages.append(message_id)
            
            basic_ops_time = (time.time() - start_time) * 1000
            
            # Test 2: Consumer Groups (Multi-agent coordination pattern)
            start_time = time.time()
            group_name = "coordination_processors"
            
            try:
                await redis_client.xgroup_create(stream_name, group_name, id="0", mkstream=True)
            except redis.ResponseError as e:
                if "BUSYGROUP" not in str(e):
                    raise
            
            # Simulate multiple agent consumers
            consumer_results = []
            for consumer_id in ["agent_1", "agent_2"]:
                messages = await redis_client.xreadgroup(
                    group_name, consumer_id, {stream_name: ">"}, count=2, block=100
                )
                consumer_results.append(len(messages[0][1]) if messages else 0)
            
            consumer_groups_time = (time.time() - start_time) * 1000
            
            # Test 3: Message Throughput
            start_time = time.time()
            throughput_stream = "throughput_test_stream"
            message_count = 50
            
            for i in range(message_count):
                await redis_client.xadd(throughput_stream, {"msg": f"throughput_test_{i}"})
            
            throughput_time = (time.time() - start_time) * 1000
            throughput_rate = (message_count / throughput_time) * 1000  # messages per second
            
            # Test 4: Stream Length and Cleanup
            stream_length = await redis_client.xlen(stream_name)
            await redis_client.delete(stream_name, throughput_stream)
            
            # Record results
            self.results.redis_streams_tests = {
                "basic_operations": {
                    "success": len(messages) == 5,
                    "latency_ms": basic_ops_time,
                    "messages_created": len(messages)
                },
                "consumer_groups": {
                    "success": sum(consumer_results) > 0,
                    "latency_ms": consumer_groups_time,
                    "messages_consumed": sum(consumer_results)
                },
                "throughput": {
                    "success": throughput_rate > self.performance_thresholds["message_throughput_per_second"],
                    "messages_per_second": throughput_rate,
                    "test_message_count": message_count
                },
                "stream_management": {
                    "success": stream_length >= 5,
                    "final_stream_length": stream_length
                }
            }
            
            # Performance metrics
            self.results.performance_benchmarks.update({
                "redis_basic_operations_ms": basic_ops_time,
                "redis_consumer_groups_ms": consumer_groups_time,
                "message_throughput_per_second": throughput_rate
            })
            
            await redis_client.aclose()
            
            # Overall success for this category
            return all(
                test_result["success"] 
                for test_result in self.results.redis_streams_tests.values()
            )
            
        except Exception as e:
            self.logger.error(f"Redis Streams test failed: {e}")
            return False

    async def _test_dashboard_integration(self) -> bool:
        """Test dashboard integration and real-time capabilities."""
        self.logger.info("üìä Testing dashboard integration...")
        
        try:
            # Test 1: Frontend Build Availability
            dashboard_files_exist = all([
                subprocess.run(["test", "-f", "frontend/package.json"], 
                             capture_output=True).returncode == 0,
                subprocess.run(["test", "-d", "frontend/src"], 
                             capture_output=True).returncode == 0,
                subprocess.run(["test", "-d", "frontend/src/components/dashboard"], 
                             capture_output=True).returncode == 0
            ])
            
            self.results.dashboard_integration_tests["frontend_structure"] = dashboard_files_exist
            
            # Test 2: Real-time Components Exist
            realtime_components = [
                "frontend/src/components/dashboard/RealTimeAgentStatusGrid.vue",
                "frontend/src/components/dashboard/RealTimePerformanceCard.vue",
                "frontend/src/services/unifiedWebSocketManager.ts",
                "frontend/src/services/coordinationService.ts"
            ]
            
            realtime_components_exist = all(
                subprocess.run(["test", "-f", component], 
                             capture_output=True).returncode == 0
                for component in realtime_components
            )
            
            self.results.dashboard_integration_tests["realtime_components"] = realtime_components_exist
            
            # Test 3: Multi-Agent Coordination Dashboard
            coordination_dashboard_exists = subprocess.run(
                ["test", "-f", "frontend/src/views/CoordinationDashboard.vue"],
                capture_output=True
            ).returncode == 0
            
            self.results.dashboard_integration_tests["coordination_dashboard"] = coordination_dashboard_exists
            
            # Test 4: Integration Test Files
            integration_tests_exist = subprocess.run(
                ["test", "-d", "frontend/tests/integration"],
                capture_output=True
            ).returncode == 0
            
            self.results.dashboard_integration_tests["integration_tests"] = integration_tests_exist
            
            # Test 5: Build System (if Node.js available)
            try:
                result = subprocess.run(
                    ["npm", "--version"], 
                    capture_output=True, text=True, timeout=5
                )
                npm_available = result.returncode == 0
                self.results.dashboard_integration_tests["build_system"] = npm_available
            except (subprocess.TimeoutExpired, FileNotFoundError):
                self.results.dashboard_integration_tests["build_system"] = False
            
            return all(self.results.dashboard_integration_tests.values())
            
        except Exception as e:
            self.logger.error(f"Dashboard integration test failed: {e}")
            return False

    async def _test_multi_agent_coordination(self) -> bool:
        """Test multi-agent coordination capabilities."""
        self.logger.info("ü§ù Testing multi-agent coordination...")
        
        try:
            # This test simulates the coordination patterns that would happen
            # in a real multi-agent workflow
            
            redis_client = redis.Redis.from_url("redis://:leanvibe_redis_pass@localhost:6380/0")
            
            # Test 1: Agent Registration Simulation
            start_time = time.time()
            agents = []
            
            for i in range(3):  # Simulate 3 agents
                agent_data = {
                    "agent_id": f"coordination_agent_{i}",
                    "capabilities": ["task_processing", "coordination"],
                    "status": "active",
                    "last_heartbeat": datetime.now().isoformat()
                }
                
                # Use Redis hash to store agent registration
                await redis_client.hset(f"agent:{agent_data['agent_id']}", mapping=agent_data)
                agents.append(agent_data["agent_id"])
            
            registration_time = (time.time() - start_time) * 1000
            
            # Test 2: Coordinated Task Distribution
            start_time = time.time()
            task_stream = "coordination_tasks"
            
            # Create coordinated tasks
            coordinated_tasks = []
            for i in range(5):
                task_data = {
                    "task_id": f"coord_task_{i}",
                    "task_type": "coordinated_processing",
                    "required_agents": 2,  # Requires coordination
                    "dependencies": f"coord_task_{i-1}" if i > 0 else "",
                    "created_at": datetime.now().isoformat()
                }
                
                task_id = await redis_client.xadd(task_stream, task_data)
                coordinated_tasks.append(task_id)
            
            # Test 3: Agent Coordination Workflow
            # Simulate agents picking up and coordinating on tasks
            coordination_results = []
            
            for agent_id in agents[:2]:  # Use first 2 agents for coordination
                # Agent reads coordination tasks
                tasks = await redis_client.xread({task_stream: "0"}, count=2)
                
                if tasks and len(tasks) > 0:
                    stream_tasks = tasks[0][1]  # First stream's tasks
                    
                    for task_entry in stream_tasks:
                        task_id, task_data = task_entry
                        
                        # Simulate coordination response
                        coordination_response = {
                            "responder_agent": agent_id,
                            "task_id": task_data[b"task_id"].decode(),
                            "coordination_status": "accepted",
                            "estimated_completion": "2s",
                            "response_time": datetime.now().isoformat()
                        }
                        
                        # Store coordination response
                        response_id = await redis_client.xadd(
                            "coordination_responses", 
                            coordination_response
                        )
                        coordination_results.append(response_id)
            
            coordination_time = (time.time() - start_time) * 1000
            
            # Test 4: Workflow State Management
            workflow_state = {
                "workflow_id": "multi_agent_test_workflow",
                "participating_agents": agents,
                "total_tasks": len(coordinated_tasks),
                "coordination_responses": len(coordination_results),
                "workflow_status": "in_progress",
                "started_at": datetime.now().isoformat()
            }
            
            await redis_client.set(
                f"workflow:{workflow_state['workflow_id']}", 
                json.dumps(workflow_state)
            )
            
            # Verify workflow state
            stored_workflow = await redis_client.get(f"workflow:{workflow_state['workflow_id']}")
            workflow_retrieved = stored_workflow is not None
            
            # Clean up test data
            await redis_client.delete(task_stream, "coordination_responses")
            for agent_id in agents:
                await redis_client.delete(f"agent:{agent_id}")
            await redis_client.delete(f"workflow:{workflow_state['workflow_id']}")
            
            # Record results
            self.results.multi_agent_coordination_tests = {
                "agent_registration": {
                    "success": len(agents) == 3,
                    "latency_ms": registration_time,
                    "agents_registered": len(agents)
                },
                "task_distribution": {
                    "success": len(coordinated_tasks) == 5,
                    "tasks_created": len(coordinated_tasks)
                },
                "coordination_workflow": {
                    "success": len(coordination_results) > 0,
                    "latency_ms": coordination_time,
                    "coordination_responses": len(coordination_results)
                },
                "workflow_state_management": {
                    "success": workflow_retrieved,
                    "workflow_data_integrity": True
                }
            }
            
            # Performance metrics
            self.results.performance_benchmarks.update({
                "agent_registration_ms": registration_time,
                "multi_agent_coordination_ms": coordination_time
            })
            
            await redis_client.aclose()
            
            return all(
                test_result["success"] 
                for test_result in self.results.multi_agent_coordination_tests.values()
            )
            
        except Exception as e:
            self.logger.error(f"Multi-agent coordination test failed: {e}")
            return False

    async def _test_performance_benchmarking(self) -> bool:
        """Test system performance against Phase 1 targets."""
        self.logger.info("‚ö° Testing performance benchmarks...")
        
        try:
            # Check if performance metrics meet thresholds
            performance_ok = True
            
            for metric, threshold in self.performance_thresholds.items():
                if metric in self.results.performance_benchmarks:
                    actual_value = self.results.performance_benchmarks[metric]
                    
                    if metric.endswith("_ms"):  # Lower is better for latency
                        meets_threshold = actual_value <= threshold
                    else:  # Higher is better for throughput
                        meets_threshold = actual_value >= threshold
                    
                    if not meets_threshold:
                        performance_ok = False
                        self.logger.warning(
                            f"Performance threshold missed: {metric} = {actual_value}, "
                            f"threshold = {threshold}"
                        )
            
            # System resource utilization test
            try:
                # Check Docker container resource usage
                result = subprocess.run([
                    "docker", "stats", "--no-stream", "--format", 
                    "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"
                ], capture_output=True, text=True, timeout=10)
                
                resource_usage_ok = result.returncode == 0
                
            except Exception:
                resource_usage_ok = False
            
            return performance_ok and resource_usage_ok
            
        except Exception as e:
            self.logger.error(f"Performance benchmarking failed: {e}")
            return False

    async def _calculate_phase_1_readiness(self):
        """Calculate Phase 1 readiness based on test results."""
        
        # Overall success calculation
        self.results.overall_success = (
            self.results.passed_categories == self.results.total_test_categories
        )
        
        # Determine phase readiness level
        if self.results.passed_categories == self.results.total_test_categories:
            self.results.phase_1_readiness = "PRODUCTION_READY"
            self.results.recommendations.append(
                "üöÄ System is ready for production multi-agent workflows"
            )
        elif self.results.passed_categories >= 4:
            self.results.phase_1_readiness = "MOSTLY_READY"
            self.results.recommendations.append(
                "‚ö†Ô∏è System mostly ready with minor issues to address"
            )
        elif self.results.passed_categories >= 3:
            self.results.phase_1_readiness = "NEEDS_WORK"
            self.results.recommendations.append(
                "üîß System needs significant work before production"
            )
        else:
            self.results.phase_1_readiness = "NOT_READY"
            self.results.critical_issues.append(
                "‚ùå System not ready for production deployment"
            )
        
        # Generate specific recommendations
        if not self.results.infrastructure_tests.get("postgres", False):
            self.results.critical_issues.append("PostgreSQL database not accessible")
        
        if not self.results.infrastructure_tests.get("redis", False):
            self.results.critical_issues.append("Redis communication failed")
        
        if not self.results.infrastructure_tests.get("pgvector", False):
            self.results.critical_issues.append("pgvector extension not available")
        
        # Redis Streams specific issues
        redis_streams_failed = [
            test_name for test_name, result in self.results.redis_streams_tests.items()
            if not result.get("success", False)
        ]
        
        if redis_streams_failed:
            self.results.critical_issues.append(
                f"Redis Streams issues: {', '.join(redis_streams_failed)}"
            )
        
        # Dashboard integration issues
        dashboard_failed = [
            test_name for test_name, result in self.results.dashboard_integration_tests.items()
            if not result
        ]
        
        if dashboard_failed:
            self.results.recommendations.append(
                f"Dashboard components need attention: {', '.join(dashboard_failed)}"
            )
        
        # Performance issues
        slow_operations = []
        for metric, value in self.results.performance_benchmarks.items():
            if metric.endswith("_ms") and metric in self.performance_thresholds:
                if value > self.performance_thresholds[metric]:
                    slow_operations.append(f"{metric}: {value:.2f}ms")
        
        if slow_operations:
            self.results.recommendations.append(
                f"Performance optimization needed: {', '.join(slow_operations)}"
            )

    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive Phase 1 validation report."""
        lines = [
            "=" * 80,
            "LEANVIBE AGENT HIVE 2.0 - PHASE 1 COMPREHENSIVE VALIDATION REPORT",
            "=" * 80,
            "",
            f"Execution Time: {self.results.timestamp}",
            f"Test Suite Version: {self.results.test_suite_version}",
            f"Overall Success: {'‚úÖ PASS' if self.results.overall_success else '‚ùå FAIL'}",
            f"Phase 1 Readiness: {self.results.phase_1_readiness}",
            "",
            f"Test Categories: {self.results.passed_categories}/{self.results.total_test_categories} PASSED",
            ""
        ]
        
        # Infrastructure Tests
        if self.results.infrastructure_tests:
            lines.extend([
                "INFRASTRUCTURE COMPONENTS:",
                "-" * 40
            ])
            
            for component, status in self.results.infrastructure_tests.items():
                status_icon = "‚úÖ" if status else "‚ùå"
                lines.append(f"- {component}: {status_icon}")
            
            lines.append("")
        
        # Redis Streams Tests
        if self.results.redis_streams_tests:
            lines.extend([
                "REDIS STREAMS COMMUNICATION:",
                "-" * 40
            ])
            
            for test_name, result in self.results.redis_streams_tests.items():
                success = result.get("success", False)
                status_icon = "‚úÖ" if success else "‚ùå"
                
                if "latency_ms" in result:
                    lines.append(f"- {test_name}: {status_icon} ({result['latency_ms']:.2f}ms)")
                elif "messages_per_second" in result:
                    lines.append(f"- {test_name}: {status_icon} ({result['messages_per_second']:.1f} msg/s)")
                else:
                    lines.append(f"- {test_name}: {status_icon}")
            
            lines.append("")
        
        # Dashboard Integration
        if self.results.dashboard_integration_tests:
            lines.extend([
                "DASHBOARD INTEGRATION:",
                "-" * 40
            ])
            
            for test_name, status in self.results.dashboard_integration_tests.items():
                status_icon = "‚úÖ" if status else "‚ùå"
                lines.append(f"- {test_name}: {status_icon}")
            
            lines.append("")
        
        # Multi-Agent Coordination
        if self.results.multi_agent_coordination_tests:
            lines.extend([
                "MULTI-AGENT COORDINATION:",
                "-" * 40
            ])
            
            for test_name, result in self.results.multi_agent_coordination_tests.items():
                success = result.get("success", False)
                status_icon = "‚úÖ" if success else "‚ùå"
                
                if "latency_ms" in result:
                    lines.append(f"- {test_name}: {status_icon} ({result['latency_ms']:.2f}ms)")
                else:
                    lines.append(f"- {test_name}: {status_icon}")
            
            lines.append("")
        
        # Performance Benchmarks
        if self.results.performance_benchmarks:
            lines.extend([
                "PERFORMANCE BENCHMARKS:",
                "-" * 40
            ])
            
            for metric, value in self.results.performance_benchmarks.items():
                threshold = self.performance_thresholds.get(metric, "N/A")
                
                if metric.endswith("_ms"):
                    meets_threshold = value <= threshold if threshold != "N/A" else True
                    lines.append(f"- {metric}: {value:.2f}ms (threshold: ‚â§{threshold}ms) {'‚úÖ' if meets_threshold else '‚ùå'}")
                elif "throughput" in metric:
                    meets_threshold = value >= threshold if threshold != "N/A" else True
                    lines.append(f"- {metric}: {value:.1f} (threshold: ‚â•{threshold}) {'‚úÖ' if meets_threshold else '‚ùå'}")
                else:
                    lines.append(f"- {metric}: {value:.2f}")
            
            lines.append("")
        
        # Critical Issues
        if self.results.critical_issues:
            lines.extend([
                "CRITICAL ISSUES:",
                "-" * 40
            ])
            
            for issue in self.results.critical_issues:
                lines.append(f"‚ùå {issue}")
            
            lines.append("")
        
        # Recommendations
        if self.results.recommendations:
            lines.extend([
                "RECOMMENDATIONS:",
                "-" * 40
            ])
            
            for rec in self.results.recommendations:
                lines.append(f"üí° {rec}")
            
            lines.append("")
        
        # Phase 1 Final Assessment
        lines.extend([
            "PHASE 1 OBJECTIVES ASSESSMENT:",
            "-" * 50
        ])
        
        phase_1_objectives = {
            "Single workflow processes end-to-end through orchestrator": 
                self.results.multi_agent_coordination_tests.get("workflow_state_management", {}).get("success", False),
            "Redis Streams enable reliable multi-agent communication (>99.5%)": 
                all(test.get("success", False) for test in self.results.redis_streams_tests.values()),
            "Dashboard displays real-time agent activities with <200ms latency": 
                all(self.results.dashboard_integration_tests.values()),
            "System handles 2+ agents working on coordinated tasks": 
                self.results.multi_agent_coordination_tests.get("coordination_workflow", {}).get("success", False),
            "Custom commands integrate with orchestration engine": 
                False  # Not tested in this suite
        }
        
        for objective, status in phase_1_objectives.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            lines.append(f"{status_icon} {objective}")
        
        lines.append("")
        
        # Final Verdict
        if self.results.phase_1_readiness == "PRODUCTION_READY":
            lines.extend([
                "üéâ PHASE 1 COMPLETION: SUCCESS",
                "",
                "The LeanVibe Agent Hive 2.0 system has successfully passed comprehensive",
                "validation and is ready for production multi-agent workflows.",
                "",
                "Key achievements:",
                "- Robust infrastructure with PostgreSQL + pgvector + Redis",
                "- High-performance Redis Streams communication",
                "- Complete dashboard integration with real-time capabilities",
                "- Proven multi-agent coordination workflows",
                "- Performance benchmarks within acceptable thresholds",
                "",
                "üöÄ System approved for Phase 2 development"
            ])
        else:
            lines.extend([
                f"‚ö†Ô∏è PHASE 1 COMPLETION: {self.results.phase_1_readiness}",
                "",
                "The system requires additional work before production deployment."
            ])
            
            if self.results.critical_issues:
                lines.extend([
                    "",
                    "Critical issues to resolve:"
                ])
                for issue in self.results.critical_issues:
                    lines.append(f"- {issue}")
        
        lines.extend([
            "",
            "=" * 80,
            f"Report generated: {datetime.now().isoformat()}",
            "=" * 80
        ])
        
        return "\n".join(lines)


async def main():
    """Main execution function."""
    print("üöÄ Starting Phase 1 Comprehensive Validation...")
    
    try:
        validator = Phase1ComprehensiveValidator()
        results = await validator.run_comprehensive_validation()
        
        # Generate and display report
        report = validator.generate_comprehensive_report()
        print(report)
        
        # Save results
        results_file = "phase_1_comprehensive_validation_results.json"
        with open(results_file, "w") as f:
            json.dump(asdict(results), f, indent=2, default=str)
        
        print(f"\nüìÑ Detailed results saved to: {results_file}")
        
        return 0 if results.overall_success else 1
        
    except Exception as e:
        print(f"üí• Comprehensive validation failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)